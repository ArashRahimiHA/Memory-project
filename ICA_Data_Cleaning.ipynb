{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e83a1b",
   "metadata": {},
   "source": [
    "# MNE Data Analysis Pipeline with Metadata\n",
    "\n",
    "This notebook provides tools for:\n",
    "- Loading FIF files and their corresponding CSV metadata\n",
    "- Concatenating multiple sessions from a subject\n",
    "- Filtering and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede8642",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "462a41f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels marked as bad:\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats as scipy_stats\n",
    "import glob\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# For interactive plotting\n",
    "matplotlib.use(\"Qt5Agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b8bec1",
   "metadata": {},
   "source": [
    "## 2. MetaData Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068db076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_session_metadata(csv_path):\n",
    "    \"\"\"\n",
    "    Load metadata from a single CSV file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : str\n",
    "        Path to CSV metadata file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    metadata : pd.DataFrame\n",
    "        Trial metadata with columns: trial_number, set_size, match, correct, \n",
    "        response, response_time, probe_letter\n",
    "    \"\"\"\n",
    "    metadata = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convert to appropriate types\n",
    "    numeric_cols = ['trial_number', 'set_size', 'match', 'correct', 'response', 'response_time']\n",
    "    for col in numeric_cols:\n",
    "        if col in metadata.columns:\n",
    "            metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "    \n",
    "    print(f\"Loaded metadata: {len(metadata)} trials\")\n",
    "    print(f\"  Columns: {list(metadata.columns)}\")\n",
    "    \n",
    "    # Show summary\n",
    "    if 'correct' in metadata.columns:\n",
    "        acc = metadata['correct'].mean() * 100\n",
    "        print(f\"  Accuracy: {acc:.1f}%\")\n",
    "    if 'response_time' in metadata.columns:\n",
    "        rt = metadata['response_time'].mean()\n",
    "        print(f\"  Mean RT: {rt:.3f}s\")\n",
    "    if 'set_size' in metadata.columns:\n",
    "        sizes = sorted(metadata['set_size'].dropna().unique())\n",
    "        print(f\"  Set sizes: {sizes}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def load_subject_metadata(subject_dir, subject_id=None, pattern='*metadata*.csv'):\n",
    "    \"\"\"\n",
    "    Load and concatenate metadata from all sessions for a subject.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_dir : str\n",
    "        Directory containing subject's session files\n",
    "    subject_id : str, optional\n",
    "        Subject identifier to add as a column\n",
    "    pattern : str\n",
    "        Glob pattern to match metadata CSV files (default: '*metadata*.csv')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    metadata_all : pd.DataFrame\n",
    "        Concatenated metadata from all sessions with added 'session' column\n",
    "    \"\"\"\n",
    "    csv_files = sorted(glob.glob(os.path.join(subject_dir, pattern)))\n",
    "    \n",
    "    if len(csv_files) == 0:\n",
    "        raise ValueError(f\"No metadata CSV files found in {subject_dir}\")\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} metadata files:\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  {os.path.basename(f)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    metadata_list = []\n",
    "    \n",
    "    for session_idx, csv_file in enumerate(csv_files, start=1):\n",
    "        print(f\"\\nLoading session {session_idx}: {os.path.basename(csv_file)}\")\n",
    "        \n",
    "        metadata = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Add session identifier\n",
    "        metadata['session'] = session_idx\n",
    "        metadata['session_file'] = os.path.basename(csv_file)\n",
    "        \n",
    "        if subject_id is not None:\n",
    "            metadata['subject'] = subject_id\n",
    "        \n",
    "        # Convert types\n",
    "        numeric_cols = ['trial_number', 'set_size', 'match', 'correct', 'response', 'response_time']\n",
    "        for col in numeric_cols:\n",
    "            if col in metadata.columns:\n",
    "                metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "        \n",
    "        print(f\"  Trials: {len(metadata)}\")\n",
    "        if 'correct' in metadata.columns:\n",
    "            acc = metadata['correct'].mean() * 100\n",
    "            print(f\"  Accuracy: {acc:.1f}%\")\n",
    "        \n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    # Concatenate all sessions\n",
    "    metadata_all = pd.concat(metadata_list, ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"\\nCombined metadata:\")\n",
    "    print(f\"  Total trials: {len(metadata_all)}\")\n",
    "    print(f\"  Sessions: {metadata_all['session'].nunique()}\")\n",
    "    \n",
    "    if 'correct' in metadata_all.columns:\n",
    "        overall_acc = metadata_all['correct'].mean() * 100\n",
    "        print(f\"  Overall accuracy: {overall_acc:.1f}%\")\n",
    "        \n",
    "        # Per-session accuracy\n",
    "        print(f\"\\n  Per-session accuracy:\")\n",
    "        for session in sorted(metadata_all['session'].unique()):\n",
    "            session_data = metadata_all[metadata_all['session'] == session]\n",
    "            acc = session_data['correct'].mean() * 100\n",
    "            print(f\"    Session {session}: {acc:.1f}%\")\n",
    "    \n",
    "    if 'response_time' in metadata_all.columns:\n",
    "        overall_rt = metadata_all['response_time'].mean()\n",
    "        print(f\"\\n  Overall mean RT: {overall_rt:.3f}s\")\n",
    "    \n",
    "    return metadata_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b617107",
   "metadata": {},
   "source": [
    "## 3. Multi-Session Concatenation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7a49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concatenate_subject(\n",
    "    subject_dir: str,\n",
    "    use_common_channels: bool = True,\n",
    "    preload: bool = True,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[mne.io.Raw, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and concatenate all FIF files and metadata for a subject.\n",
    "    \n",
    "    Automatically finds all .fif and .csv files in the directory.\n",
    "    Matches them by sorting alphabetically.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_dir : str\n",
    "        Directory containing subject's session files\n",
    "    use_common_channels : bool\n",
    "        If True, only keep channels common to all files\n",
    "    preload : bool\n",
    "        Whether to load data into memory\n",
    "    verbose : bool\n",
    "        Verbose output\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    raw_concat : mne.io.Raw\n",
    "        Concatenated Raw object from all sessions\n",
    "    metadata_all : pd.DataFrame\n",
    "        Concatenated metadata from all sessions with 'session' column\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> raw, metadata = load_and_concatenate_subject('data/Subject_01/')\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"LOADING SUBJECT DATA\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDirectory: {subject_dir}\")\n",
    "    \n",
    "    # Get all files in directory\n",
    "    all_files = os.listdir(subject_dir)\n",
    "    \n",
    "    # Find FIF files\n",
    "    fif_files = sorted([f for f in all_files if f.endswith('.fif')])\n",
    "    \n",
    "    # Find CSV files (look for files with .csv extension)\n",
    "    csv_files = sorted([f for f in all_files if f.endswith('.csv')])\n",
    "    \n",
    "    # Verify we found files\n",
    "    if len(fif_files) == 0:\n",
    "        raise ValueError(f\"No .fif files found in {subject_dir}\")\n",
    "    \n",
    "    if len(csv_files) == 0:\n",
    "        raise ValueError(f\"No .csv files found in {subject_dir}\")\n",
    "    \n",
    "    print(f\"\\n[1/2] Loading FIF files...\")\n",
    "    print(f\"Found {len(fif_files)} FIF files:\")\n",
    "    for f in fif_files:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    # Load FIF files\n",
    "    raw_list = []\n",
    "    \n",
    "    for fif_file in fif_files:\n",
    "        full_path = os.path.join(subject_dir, fif_file)\n",
    "        raw = mne.io.read_raw_fif(full_path, preload=False, verbose=verbose)\n",
    "        raw_list.append(raw)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Loaded: {fif_file} ({len(raw.ch_names)} channels)\")\n",
    "    \n",
    "    # Find and use common channels\n",
    "    if use_common_channels and len(raw_list) > 1:\n",
    "        common_channels = set(raw_list[0].ch_names)\n",
    "        for raw in raw_list[1:]:\n",
    "            common_channels &= set(raw.ch_names)\n",
    "        \n",
    "        common_channels = sorted(list(common_channels))\n",
    "        print(f\"\\nUsing {len(common_channels)} common channels\")\n",
    "        \n",
    "        # Pick common channels from all files\n",
    "        for i, raw in enumerate(raw_list):\n",
    "            raw_list[i] = raw.copy().pick_channels(common_channels, ordered=True)\n",
    "    \n",
    "    # Concatenate FIF files\n",
    "    print(\"\\nConcatenating FIF files...\")\n",
    "    raw_concat = mne.concatenate_raws(raw_list, preload=preload, verbose=verbose)\n",
    "    \n",
    "    print(f\"✓ Neural data concatenated:\")\n",
    "    print(f\"  Duration: {raw_concat.times[-1]:.2f}s\")\n",
    "    print(f\"  Channels: {len(raw_concat.ch_names)}\")\n",
    "    print(f\"  Sampling rate: {raw_concat.info['sfreq']} Hz\")\n",
    "    \n",
    "    # Load metadata\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[2/2] Loading metadata...\")\n",
    "    print(f\"Found {len(csv_files)} CSV files:\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    metadata_list = []\n",
    "    \n",
    "    for session_idx, csv_file in enumerate(csv_files, start=1):\n",
    "        full_path = os.path.join(subject_dir, csv_file)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n  Loading session {session_idx}: {csv_file}\")\n",
    "        \n",
    "        metadata = pd.read_csv(full_path)\n",
    "        \n",
    "        # Add session identifier\n",
    "        metadata['session'] = session_idx\n",
    "        metadata['session_file'] = csv_file\n",
    "        \n",
    "        # Convert to numeric types\n",
    "        numeric_cols = ['trial_number', 'set_size', 'match', 'correct', \n",
    "                       'response', 'response_time']\n",
    "        for col in numeric_cols:\n",
    "            if col in metadata.columns:\n",
    "                metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"    Trials: {len(metadata)}\")\n",
    "            if 'correct' in metadata.columns:\n",
    "                acc = metadata['correct'].mean() * 100\n",
    "                print(f\"    Accuracy: {acc:.1f}%\")\n",
    "        \n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    # Concatenate all metadata\n",
    "    metadata_all = pd.concat(metadata_list, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n✓ Metadata concatenated:\")\n",
    "    print(f\"  Total trials: {len(metadata_all)}\")\n",
    "    print(f\"  Sessions: {metadata_all['session'].nunique()}\")\n",
    "    \n",
    "    if 'correct' in metadata_all.columns:\n",
    "        overall_acc = metadata_all['correct'].mean() * 100\n",
    "        print(f\"  Overall accuracy: {overall_acc:.1f}%\")\n",
    "    \n",
    "    if 'response_time' in metadata_all.columns:\n",
    "        overall_rt = metadata_all['response_time'].mean()\n",
    "        print(f\"  Overall mean RT: {overall_rt:.3f}s\")\n",
    "    \n",
    "    # Verify alignment\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    n_sessions_fif = len(fif_files)\n",
    "    n_sessions_meta = metadata_all['session'].nunique()\n",
    "    \n",
    "    if n_sessions_fif == n_sessions_meta:\n",
    "        print(f\"✓ Session count matches: {n_sessions_fif} sessions\")\n",
    "    else:\n",
    "        print(f\"⚠ WARNING: Session count mismatch!\")\n",
    "        print(f\"  FIF files: {n_sessions_fif}\")\n",
    "        print(f\"  CSV files: {n_sessions_meta}\")\n",
    "    \n",
    "    print(\"\\n✓ Loading complete!\")\n",
    "    \n",
    "    return raw_concat, metadata_all\n",
    "\n",
    "\n",
    "def load_and_concatenate_subject_paired(\n",
    "    subject_dir: str,\n",
    "    fif_prefix: Optional[str] = None,\n",
    "    csv_suffix: Optional[str] = None,\n",
    "    use_common_channels: bool = True,\n",
    "    preload: bool = True,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[mne.io.Raw, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and concatenate with smart FIF-CSV pairing.\n",
    "    \n",
    "    Pairs files based on shared naming (e.g., Session_01_raw.fif with Session_01_metadata.csv).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_dir : str\n",
    "        Directory containing subject's session files\n",
    "    fif_prefix : str, optional\n",
    "        Only load FIF files starting with this prefix\n",
    "    csv_suffix : str, optional\n",
    "        Only load CSV files with this suffix (e.g., 'metadata')\n",
    "    use_common_channels : bool\n",
    "        If True, only keep channels common to all files\n",
    "    preload : bool\n",
    "        Whether to load data into memory\n",
    "    verbose : bool\n",
    "        Verbose output\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    raw_concat : mne.io.Raw\n",
    "        Concatenated Raw object\n",
    "    metadata_all : pd.DataFrame\n",
    "        Concatenated metadata\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Load only files with specific naming\n",
    "    >>> raw, meta = load_and_concatenate_subject_paired(\n",
    "    ...     'data/Subject_01/',\n",
    "    ...     csv_suffix='metadata'\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"LOADING SUBJECT DATA (PAIRED MODE)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDirectory: {subject_dir}\")\n",
    "    \n",
    "    # Get all files\n",
    "    all_files = os.listdir(subject_dir)\n",
    "    \n",
    "    # Find FIF files\n",
    "    fif_files = [f for f in all_files if f.endswith('.fif')]\n",
    "    if fif_prefix:\n",
    "        fif_files = [f for f in fif_files if f.startswith(fif_prefix)]\n",
    "    fif_files = sorted(fif_files)\n",
    "    \n",
    "    # Find CSV files\n",
    "    csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "    if csv_suffix:\n",
    "        csv_files = [f for f in csv_files if csv_suffix in f.lower()]\n",
    "    csv_files = sorted(csv_files)\n",
    "    \n",
    "    # Verify\n",
    "    if len(fif_files) == 0:\n",
    "        raise ValueError(f\"No FIF files found in {subject_dir}\")\n",
    "    if len(csv_files) == 0:\n",
    "        raise ValueError(f\"No CSV files found in {subject_dir}\")\n",
    "    \n",
    "    print(f\"\\n[1/2] Loading {len(fif_files)} FIF files...\")\n",
    "    for f in fif_files:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    # Try to pair FIF with CSV files\n",
    "    paired_files = []\n",
    "    \n",
    "    for fif_file in fif_files:\n",
    "        # Extract base name (remove extension and common suffixes)\n",
    "        base_name = fif_file.replace('.fif', '').replace('_raw', '').replace('_eeg', '')\n",
    "        \n",
    "        # Look for matching CSV\n",
    "        matching_csv = None\n",
    "        for csv_file in csv_files:\n",
    "            csv_base = csv_file.replace('.csv', '').replace('_metadata', '')\n",
    "            if base_name in csv_base or csv_base in base_name:\n",
    "                matching_csv = csv_file\n",
    "                break\n",
    "        \n",
    "        if matching_csv:\n",
    "            paired_files.append((fif_file, matching_csv))\n",
    "            if verbose:\n",
    "                print(f\"  Paired: {fif_file} ↔ {matching_csv}\")\n",
    "        else:\n",
    "            # No match found, still use this FIF but warn\n",
    "            paired_files.append((fif_file, None))\n",
    "            print(f\"  ⚠ No matching CSV for: {fif_file}\")\n",
    "    \n",
    "    # Load FIF files\n",
    "    raw_list = []\n",
    "    for fif_file, _ in paired_files:\n",
    "        full_path = os.path.join(subject_dir, fif_file)\n",
    "        raw = mne.io.read_raw_fif(full_path, preload=False, verbose=verbose)\n",
    "        raw_list.append(raw)\n",
    "    \n",
    "    # Common channels\n",
    "    if use_common_channels and len(raw_list) > 1:\n",
    "        common_channels = set(raw_list[0].ch_names)\n",
    "        for raw in raw_list[1:]:\n",
    "            common_channels &= set(raw.ch_names)\n",
    "        common_channels = sorted(list(common_channels))\n",
    "        print(f\"\\nUsing {len(common_channels)} common channels\")\n",
    "        \n",
    "        for i, raw in enumerate(raw_list):\n",
    "            raw_list[i] = raw.copy().pick_channels(common_channels, ordered=True)\n",
    "    \n",
    "    # Concatenate\n",
    "    raw_concat = mne.concatenate_raws(raw_list, preload=preload, verbose=verbose)\n",
    "    print(f\"\\n✓ Neural data: {raw_concat.times[-1]:.2f}s, {len(raw_concat.ch_names)} channels\")\n",
    "    \n",
    "    # Load metadata\n",
    "    print(\"\\n[2/2] Loading metadata...\")\n",
    "    metadata_list = []\n",
    "    \n",
    "    for session_idx, (fif_file, csv_file) in enumerate(paired_files, start=1):\n",
    "        if csv_file is None:\n",
    "            print(f\"  ⚠ Skipping session {session_idx} (no CSV)\")\n",
    "            continue\n",
    "        \n",
    "        full_path = os.path.join(subject_dir, csv_file)\n",
    "        metadata = pd.read_csv(full_path)\n",
    "        metadata['session'] = session_idx\n",
    "        metadata['session_file'] = csv_file\n",
    "        metadata['fif_file'] = fif_file\n",
    "        \n",
    "        # Convert types\n",
    "        numeric_cols = ['trial_number', 'set_size', 'match', 'correct', \n",
    "                       'response', 'response_time']\n",
    "        for col in numeric_cols:\n",
    "            if col in metadata.columns:\n",
    "                metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "        \n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    if len(metadata_list) == 0:\n",
    "        raise ValueError(\"No metadata files could be loaded\")\n",
    "    \n",
    "    metadata_all = pd.concat(metadata_list, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n✓ Metadata: {len(metadata_all)} trials from {len(metadata_list)} sessions\")\n",
    "    \n",
    "    return raw_concat, metadata_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69d823",
   "metadata": {},
   "source": [
    "## 5. ICA summary pdf output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574754ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICA_summary_pdf(data, ica):\n",
    "    # Calculate explained variance properly\n",
    "    # Method 1: Use the correct approach\n",
    "    try:\n",
    "        sources = ica.get_sources(data)\n",
    "        # Calculate variance for each component\n",
    "        explained_var_array = np.var(sources.get_data(), axis=1)\n",
    "        # Normalize to get proportion\n",
    "        explained_var_array = explained_var_array / np.sum(explained_var_array)\n",
    "    except:\n",
    "        # Fallback\n",
    "        explained_var_array = np.ones(ica.n_components_) / ica.n_components_\n",
    "\n",
    "    print(f\"Explained variance shape: {explained_var_array.shape}\")\n",
    "    print(f\"Number of components: {ica.n_components_}\")\n",
    "    print(f\"Explained variance values:\\n{explained_var_array}\")\n",
    "    print(f\"Total variance: {np.sum(explained_var_array)*100:.2f}%\")\n",
    "\n",
    "    # Create PDF with all component properties\n",
    "    pdf_filename = 'ica_components_report.pdf'\n",
    "\n",
    "    with PdfPages(pdf_filename) as pdf:\n",
    "        n_components = ica.n_components_\n",
    "        \n",
    "        for comp_idx in range(n_components):\n",
    "            # Create figure with subplots for each component\n",
    "            fig = plt.figure(figsize=(11.69, 8.27))  # A4 landscape\n",
    "            \n",
    "            # Add title with component number and explained variance\n",
    "            var_text = f'{explained_var_array[comp_idx]*100:.2f}%'\n",
    "            fig.suptitle(f'ICA{comp_idx:03d} - Explained Variance: {var_text}', \n",
    "                        fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # 1. Topography\n",
    "            ax1 = plt.subplot(2, 3, 1)\n",
    "            ica.plot_components(picks=comp_idx, axes=ax1, show=False, colorbar=True)\n",
    "            \n",
    "            # Get source data for this component\n",
    "            sources = ica.get_sources(raw)\n",
    "            sfreq = raw.info['sfreq']\n",
    "            \n",
    "            # 2. Time course (using first 10 seconds of data)\n",
    "            ax2 = plt.subplot(2, 3, 2)\n",
    "            n_samples = min(int(10 * sfreq), sources.n_times)\n",
    "            times = sources.times[:n_samples]\n",
    "            data = sources.get_data(picks=comp_idx)[:, :n_samples]\n",
    "            ax2.plot(times, data.T, 'k', linewidth=0.5)\n",
    "            ax2.set_xlabel('Time (s)')\n",
    "            ax2.set_ylabel('AU')\n",
    "            ax2.set_title('Time Course (first 10s)')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # 3. Power Spectrum (fixed)\n",
    "            ax3 = plt.subplot(2, 3, 3)\n",
    "            from mne.time_frequency import psd_array_welch\n",
    "            \n",
    "            # Use all available data for PSD\n",
    "            data_full = sources.get_data(picks=comp_idx)\n",
    "            \n",
    "            # Adjust n_fft based on available data\n",
    "            n_fft = min(2048, data_full.shape[1])\n",
    "            n_per_seg = min(n_fft, data_full.shape[1])\n",
    "            \n",
    "            psds, freqs = psd_array_welch(\n",
    "                data_full, \n",
    "                sfreq=sfreq, \n",
    "                fmin=0.5, \n",
    "                fmax=50, \n",
    "                n_fft=n_fft,\n",
    "                n_per_seg=n_per_seg\n",
    "            )\n",
    "            \n",
    "            ax3.semilogy(freqs, psds.T, 'k', linewidth=1)\n",
    "            ax3.set_xlabel('Frequency (Hz)')\n",
    "            ax3.set_ylabel('Power Spectral Density (µV²/Hz)')\n",
    "            ax3.set_title('Power Spectrum')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            ax3.set_xlim([0, 50])\n",
    "            \n",
    "            # 4. Component properties (alternative visualization)\n",
    "            ax4 = plt.subplot(2, 3, 4)\n",
    "            # Plot first 2 seconds in more detail\n",
    "            n_samples_detail = min(int(2 * sfreq), sources.n_times)\n",
    "            times_detail = sources.times[:n_samples_detail]\n",
    "            data_detail = sources.get_data(picks=comp_idx)[:, :n_samples_detail]\n",
    "            ax4.plot(times_detail, data_detail.T, 'b', linewidth=0.8)\n",
    "            ax4.set_xlabel('Time (s)')\n",
    "            ax4.set_ylabel('AU')\n",
    "            ax4.set_title('Time Course Detail (first 2s)')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            # 5. Variance bar (showing this component in context)\n",
    "            ax5 = plt.subplot(2, 3, 5)\n",
    "            colors = ['red' if i == comp_idx else 'gray' for i in range(n_components)]\n",
    "            ax5.bar(range(n_components), explained_var_array * 100, color=colors, alpha=0.6)\n",
    "            ax5.set_xlabel('Component')\n",
    "            ax5.set_ylabel('Explained Variance (%)')\n",
    "            ax5.set_title('Variance Explained by All Components')\n",
    "            ax5.axhline(y=5, color='r', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "            ax5.legend()\n",
    "            ax5.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # 6. Properties statistics\n",
    "            ax6 = plt.subplot(2, 3, 6)\n",
    "            ax6.axis('off')\n",
    "            \n",
    "            # Calculate some statistics\n",
    "            data_stats = sources.get_data(picks=comp_idx).flatten()\n",
    "            stats_text = f\"\"\"Component Statistics:\n",
    "            \n",
    "    Mean: {np.mean(data_stats):.3f}\n",
    "    Std: {np.std(data_stats):.3f}\n",
    "    Min: {np.min(data_stats):.3f}\n",
    "    Max: {np.max(data_stats):.3f}\n",
    "    Kurtosis: {np.mean((data_stats - np.mean(data_stats))**4) / (np.std(data_stats)**4):.3f}\n",
    "\n",
    "    Explained Variance: {explained_var_array[comp_idx]*100:.2f}%\n",
    "    Rank by Variance: #{np.where(np.argsort(explained_var_array)[::-1] == comp_idx)[0][0] + 1}\n",
    "            \"\"\"\n",
    "            \n",
    "            ax6.text(0.1, 0.9, stats_text, \n",
    "                    transform=ax6.transAxes,\n",
    "                    fontsize=10,\n",
    "                    verticalalignment='top',\n",
    "                    fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            \n",
    "            print(f\"Processed component {comp_idx+1}/{n_components}\")\n",
    "        \n",
    "        # Add summary page at the end\n",
    "        fig_summary = plt.figure(figsize=(11.69, 8.27))\n",
    "        fig_summary.suptitle('ICA Components Summary', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Variance explained table\n",
    "        ax_table = plt.subplot(1, 2, 1)\n",
    "        ax_table.axis('tight')\n",
    "        ax_table.axis('off')\n",
    "        \n",
    "        # Sort by variance for the table\n",
    "        sorted_indices = np.argsort(explained_var_array)[::-1]\n",
    "        table_data = [[f'ICA{i:03d}', f'{explained_var_array[i]*100:.2f}%', f'#{rank+1}'] \n",
    "                    for rank, i in enumerate(sorted_indices)]\n",
    "        \n",
    "        table = ax_table.table(cellText=table_data, \n",
    "                            colLabels=['Component', 'Variance (%)', 'Rank'],\n",
    "                            cellLoc='center',\n",
    "                            loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(8)\n",
    "        table.scale(1, 1.5)\n",
    "        ax_table.set_title('Components Ranked by Variance', fontsize=12, pad=20)\n",
    "        \n",
    "        # Overall variance plot\n",
    "        ax_var = plt.subplot(1, 2, 2)\n",
    "        ax_var.bar(range(n_components), explained_var_array * 100, color='steelblue', alpha=0.7)\n",
    "        ax_var.set_xlabel('Component Index')\n",
    "        ax_var.set_ylabel('Explained Variance (%)')\n",
    "        ax_var.set_title('Variance Explained by Each Component')\n",
    "        ax_var.axhline(y=5, color='r', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "        ax_var.legend()\n",
    "        ax_var.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add text summary\n",
    "        total_var = np.sum(explained_var_array) * 100\n",
    "        ax_var.text(0.02, 0.98, f'Total variance: {total_var:.2f}%',\n",
    "                    transform=ax_var.transAxes, \n",
    "                    verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig_summary, bbox_inches='tight')\n",
    "        plt.close(fig_summary)\n",
    "\n",
    "    print(f'\\nPDF report saved to: {pdf_filename}')\n",
    "\n",
    "    # Print summary to console\n",
    "    print('\\nExplained Variance Summary (sorted):')\n",
    "    print('-' * 50)\n",
    "    sorted_indices = np.argsort(explained_var_array)[::-1]\n",
    "    for rank, i in enumerate(sorted_indices):\n",
    "        print(f'#{rank+1:2d} - ICA{i:03d}: {explained_var_array[i]*100:6.2f}%')\n",
    "    print('-' * 50)\n",
    "    print(f'Total: {np.sum(explained_var_array)*100:6.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa1eb2e",
   "metadata": {},
   "source": [
    "## 6. Usage Example: Load Subject Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f74659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING SUBJECT DATA\n",
      "================================================================================\n",
      "\n",
      "Directory: Data_converted_MetaData\\Subject_08\n",
      "\n",
      "[1/2] Loading FIF files...\n",
      "Found 5 FIF files:\n",
      "  Data_Subject_08_Session_01.h5_seeg_raw.fif\n",
      "  Data_Subject_08_Session_02.h5_seeg_raw.fif\n",
      "  Data_Subject_08_Session_03.h5_seeg_raw.fif\n",
      "  Data_Subject_08_Session_04.h5_seeg_raw.fif\n",
      "  Data_Subject_08_Session_05.h5_seeg_raw.fif\n",
      "Opening raw data file Data_converted_MetaData\\Subject_08\\Data_Subject_08_Session_01.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 79999 =      0.000 ...   399.995 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\RSKALA\\AppData\\Local\\Temp\\ipykernel_14432\\740267351.py:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  subject_dir = 'Data_converted_MetaData\\Subject_08'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded: Data_Subject_08_Session_01.h5_seeg_raw.fif (20 channels)\n",
      "Opening raw data file Data_converted_MetaData\\Subject_08\\Data_Subject_08_Session_02.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 79999 =      0.000 ...   399.995 secs\n",
      "Ready.\n",
      "  Loaded: Data_Subject_08_Session_02.h5_seeg_raw.fif (20 channels)\n",
      "Opening raw data file Data_converted_MetaData\\Subject_08\\Data_Subject_08_Session_03.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 79999 =      0.000 ...   399.995 secs\n",
      "Ready.\n",
      "  Loaded: Data_Subject_08_Session_03.h5_seeg_raw.fif (20 channels)\n",
      "Opening raw data file Data_converted_MetaData\\Subject_08\\Data_Subject_08_Session_04.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 79999 =      0.000 ...   399.995 secs\n",
      "Ready.\n",
      "  Loaded: Data_Subject_08_Session_04.h5_seeg_raw.fif (20 channels)\n",
      "Opening raw data file Data_converted_MetaData\\Subject_08\\Data_Subject_08_Session_05.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 78399 =      0.000 ...   391.995 secs\n",
      "Ready.\n",
      "  Loaded: Data_Subject_08_Session_05.h5_seeg_raw.fif (20 channels)\n",
      "\n",
      "Using 20 common channels\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "\n",
      "Concatenating FIF files...\n",
      "✓ Neural data concatenated:\n",
      "  Duration: 1991.99s\n",
      "  Channels: 20\n",
      "  Sampling rate: 200.0 Hz\n",
      "\n",
      "================================================================================\n",
      "[2/2] Loading metadata...\n",
      "Found 5 CSV files:\n",
      "  Data_Subject_08_Session_01.h5_seeg_raw.csv\n",
      "  Data_Subject_08_Session_02.h5_seeg_raw.csv\n",
      "  Data_Subject_08_Session_03.h5_seeg_raw.csv\n",
      "  Data_Subject_08_Session_04.h5_seeg_raw.csv\n",
      "  Data_Subject_08_Session_05.h5_seeg_raw.csv\n",
      "\n",
      "  Loading session 1: Data_Subject_08_Session_01.h5_seeg_raw.csv\n",
      "    Trials: 50\n",
      "    Accuracy: 88.0%\n",
      "\n",
      "  Loading session 2: Data_Subject_08_Session_02.h5_seeg_raw.csv\n",
      "    Trials: 50\n",
      "    Accuracy: 94.0%\n",
      "\n",
      "  Loading session 3: Data_Subject_08_Session_03.h5_seeg_raw.csv\n",
      "    Trials: 50\n",
      "    Accuracy: 86.0%\n",
      "\n",
      "  Loading session 4: Data_Subject_08_Session_04.h5_seeg_raw.csv\n",
      "    Trials: 50\n",
      "    Accuracy: 88.0%\n",
      "\n",
      "  Loading session 5: Data_Subject_08_Session_05.h5_seeg_raw.csv\n",
      "    Trials: 49\n",
      "    Accuracy: 93.9%\n",
      "\n",
      "✓ Metadata concatenated:\n",
      "  Total trials: 249\n",
      "  Sessions: 5\n",
      "  Overall accuracy: 90.0%\n",
      "  Overall mean RT: 1.538s\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION\n",
      "================================================================================\n",
      "✓ Session count matches: 5 sessions\n",
      "\n",
      "✓ Loading complete!\n",
      "\n",
      "================================================================================\n",
      "SUCCESS!\n",
      "================================================================================\n",
      "Loaded 20 channels, 1992.0s\n",
      "Loaded 249 trials\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Example 1: Simple loading (all .fif and .csv files)\n",
    "    subject_dir = 'Data_converted_MetaData\\Subject_08'\n",
    "    \n",
    "    try:\n",
    "        raw, metadata = load_and_concatenate_subject(\n",
    "            subject_dir=subject_dir,\n",
    "            use_common_channels=True,\n",
    "            preload=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUCCESS!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Loaded {len(raw.ch_names)} channels, {raw.times[-1]:.1f}s\")\n",
    "        print(f\"Loaded {len(metadata)} trials\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nDirectory not found: {e}\")\n",
    "        print(\"Please update subject_dir to point to your data\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "raw_cropped = raw.copy().crop(tmax = 1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdff9cc",
   "metadata": {},
   "source": [
    "## 7. Band-pass Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c59b212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 5 contiguous segments\n",
      "Setting up band-pass filter from 0.1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.10\n",
      "- Lower transition bandwidth: 0.10 Hz (-6 dB cutoff frequency: 0.05 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 6601 samples (33.005 s)\n",
      "\n",
      "Filtering raw data in 5 contiguous segments\n",
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 661 samples (3.305 s)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_filtered = raw_cropped.copy().filter(l_freq=0.1, h_freq=40.0, verbose=True)\n",
    "raw_filtered_ica = raw_cropped.copy().filter(l_freq=1, h_freq = 40, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75fc6d",
   "metadata": {},
   "source": [
    "## 8. ICA Decleration & Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1452013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 19 channels (please be patient, this may take a while)\n",
      "Selecting by number: 19 components\n",
      "Fitting ICA took 13.0s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table mne-repr-table\">\n",
       "    <tr>\n",
       "        <th>Method</th>\n",
       "        <td>picard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit parameters</th>\n",
       "        <td>max_iter=500<br /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit</th>\n",
       "        <td>69 iterations on raw data (320001 samples)</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>ICA components</th>\n",
       "        <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Available PCA components</th>\n",
       "        <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Channel types</th>\n",
       "        <td>eeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ICA components marked for exclusion</th>\n",
       "        <td>&mdash;</td>\n",
       "    </tr>\n",
       "    \n",
       "</table>"
      ],
      "text/plain": [
       "<ICA | raw data decomposition, method: picard (fit in 69 iterations on 320001 samples), 19 ICA components (19 PCA components available), channel types: eeg, no sources marked for exclusion>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 19  # Should normally be higher, like 0.999!!\n",
    "method = 'picard'\n",
    "max_iter = 500  # Should normally be higher, like 500 or even 1000!!\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "ica = mne.preprocessing.ICA(n_components=n_components,\n",
    "                            method=method,\n",
    "                            max_iter=max_iter,\n",
    "                            random_state=random_state)\n",
    "ica.fit(raw_filtered_ica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b1a86",
   "metadata": {},
   "source": [
    "## 9. Visualisation & Saving as PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a1d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MNEFigure size 975x967 with 19 Axes>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ICA_summary_pdf(raw_filtered_ica, ica)\n",
    "ica.plot_components(inst = raw_filtered_ica)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e35962",
   "metadata": {},
   "source": [
    "## 10. Excluding components & visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfa7e2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (19 components)\n",
      "    Zeroing out 3 ICA components\n",
      "    Projecting back using 19 PCA components\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (19 components)\n",
      "    Zeroing out 4 ICA components\n",
      "    Projecting back using 19 PCA components\n",
      "Channels marked as bad:\n",
      "none\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 3 Axes>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ica.exclude = [0, 1, 2]\n",
    "reconst_raw_filtered_ica = raw_filtered_ica.copy()\n",
    "ica.apply(reconst_raw_filtered_ica)\n",
    "reconst_raw_filtered_ica.plot()\n",
    "ica.plot_overlay(raw_filtered_ica, exclude=[0,1,2,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56692965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
