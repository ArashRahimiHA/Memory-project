{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e83a1b",
   "metadata": {},
   "source": [
    "# MNE Data Cleaning Pipeline with Metadata\n",
    "\n",
    "This notebook provides tools for:\n",
    "- Loading FIF files and their corresponding CSV metadata\n",
    "- Concatenating multiple sessions from a subject\n",
    "- Filtering and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede8642",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "462a41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats as scipy_stats\n",
    "import glob\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# For interactive plotting\n",
    "matplotlib.use(\"Qt5Agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b8bec1",
   "metadata": {},
   "source": [
    "## 2. MetaData Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068db076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_session_metadata(csv_path):\n",
    "    \"\"\"\n",
    "    Load metadata from a single CSV file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : str\n",
    "        Path to CSV metadata file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    metadata : pd.DataFrame\n",
    "        Trial metadata with columns: trial_number, set_size, match, correct, \n",
    "        response, response_time, probe_letter\n",
    "    \"\"\"\n",
    "    metadata = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convert to appropriate types\n",
    "    numeric_cols = ['trial_number', 'set_size', 'match', 'correct', 'response', 'response_time']\n",
    "    for col in numeric_cols:\n",
    "        if col in metadata.columns:\n",
    "            metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "    \n",
    "    print(f\"Loaded metadata: {len(metadata)} trials\")\n",
    "    print(f\"  Columns: {list(metadata.columns)}\")\n",
    "    \n",
    "    # Show summary\n",
    "    if 'correct' in metadata.columns:\n",
    "        acc = metadata['correct'].mean() * 100\n",
    "        print(f\"  Accuracy: {acc:.1f}%\")\n",
    "    if 'response_time' in metadata.columns:\n",
    "        rt = metadata['response_time'].mean()\n",
    "        print(f\"  Mean RT: {rt:.3f}s\")\n",
    "    if 'set_size' in metadata.columns:\n",
    "        sizes = sorted(metadata['set_size'].dropna().unique())\n",
    "        print(f\"  Set sizes: {sizes}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def load_subject_metadata(subject_dir, subject_id=None, pattern='*metadata*.csv'):\n",
    "    \"\"\"\n",
    "    Load and concatenate metadata from all sessions for a subject.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_dir : str\n",
    "        Directory containing subject's session files\n",
    "    subject_id : str, optional\n",
    "        Subject identifier to add as a column\n",
    "    pattern : str\n",
    "        Glob pattern to match metadata CSV files (default: '*metadata*.csv')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    metadata_all : pd.DataFrame\n",
    "        Concatenated metadata from all sessions with added 'session' column\n",
    "    \"\"\"\n",
    "    csv_files = sorted(glob.glob(os.path.join(subject_dir, pattern)))\n",
    "    \n",
    "    if len(csv_files) == 0:\n",
    "        raise ValueError(f\"No metadata CSV files found in {subject_dir}\")\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} metadata files:\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  {os.path.basename(f)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    metadata_list = []\n",
    "    \n",
    "    for session_idx, csv_file in enumerate(csv_files, start=1):\n",
    "        print(f\"\\nLoading session {session_idx}: {os.path.basename(csv_file)}\")\n",
    "        \n",
    "        metadata = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Add session identifier\n",
    "        metadata['session'] = session_idx\n",
    "        metadata['session_file'] = os.path.basename(csv_file)\n",
    "        \n",
    "        if subject_id is not None:\n",
    "            metadata['subject'] = subject_id\n",
    "        \n",
    "        # Convert types\n",
    "        numeric_cols = ['trial_number', 'set_size', 'match', 'correct', 'response', 'response_time']\n",
    "        for col in numeric_cols:\n",
    "            if col in metadata.columns:\n",
    "                metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "        \n",
    "        print(f\"  Trials: {len(metadata)}\")\n",
    "        if 'correct' in metadata.columns:\n",
    "            acc = metadata['correct'].mean() * 100\n",
    "            print(f\"  Accuracy: {acc:.1f}%\")\n",
    "        \n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    # Concatenate all sessions\n",
    "    metadata_all = pd.concat(metadata_list, ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"\\nCombined metadata:\")\n",
    "    print(f\"  Total trials: {len(metadata_all)}\")\n",
    "    print(f\"  Sessions: {metadata_all['session'].nunique()}\")\n",
    "    \n",
    "    if 'correct' in metadata_all.columns:\n",
    "        overall_acc = metadata_all['correct'].mean() * 100\n",
    "        print(f\"  Overall accuracy: {overall_acc:.1f}%\")\n",
    "        \n",
    "        # Per-session accuracy\n",
    "        print(f\"\\n  Per-session accuracy:\")\n",
    "        for session in sorted(metadata_all['session'].unique()):\n",
    "            session_data = metadata_all[metadata_all['session'] == session]\n",
    "            acc = session_data['correct'].mean() * 100\n",
    "            print(f\"    Session {session}: {acc:.1f}%\")\n",
    "    \n",
    "    if 'response_time' in metadata_all.columns:\n",
    "        overall_rt = metadata_all['response_time'].mean()\n",
    "        print(f\"\\n  Overall mean RT: {overall_rt:.3f}s\")\n",
    "    \n",
    "    return metadata_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b617107",
   "metadata": {},
   "source": [
    "## 3. Multi-Session Concatenation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7a49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concatenate_subject(\n",
    "    subject_dir: str,\n",
    "    use_common_channels: bool = True,\n",
    "    preload: bool = True,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[mne.io.Raw, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and concatenate all FIF files and metadata for a subject.\n",
    "    \n",
    "    Automatically finds all .fif and .csv files in the directory.\n",
    "    Matches them by sorting alphabetically.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_dir : str\n",
    "        Directory containing subject's session files\n",
    "    use_common_channels : bool\n",
    "        If True, only keep channels common to all files\n",
    "    preload : bool\n",
    "        Whether to load data into memory\n",
    "    verbose : bool\n",
    "        Verbose output\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    raw_concat : mne.io.Raw\n",
    "        Concatenated Raw object from all sessions\n",
    "    metadata_all : pd.DataFrame\n",
    "        Concatenated metadata from all sessions with 'session' column\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> raw, metadata = load_and_concatenate_subject('data/Subject_01/')\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"LOADING SUBJECT DATA\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDirectory: {subject_dir}\")\n",
    "    \n",
    "    # Get all files in directory\n",
    "    all_files = os.listdir(subject_dir)\n",
    "    \n",
    "    # Find FIF files\n",
    "    fif_files = sorted([f for f in all_files if f.endswith('.fif')])\n",
    "    \n",
    "    # Find CSV files (look for files with .csv extension)\n",
    "    csv_files = sorted([f for f in all_files if f.endswith('.csv')])\n",
    "    \n",
    "    # Verify we found files\n",
    "    if len(fif_files) == 0:\n",
    "        raise ValueError(f\"No .fif files found in {subject_dir}\")\n",
    "    \n",
    "    if len(csv_files) == 0:\n",
    "        raise ValueError(f\"No .csv files found in {subject_dir}\")\n",
    "    \n",
    "    print(f\"\\n[1/2] Loading FIF files...\")\n",
    "    print(f\"Found {len(fif_files)} FIF files:\")\n",
    "    for f in fif_files:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    # Load FIF files\n",
    "    raw_list = []\n",
    "    \n",
    "    for fif_file in fif_files:\n",
    "        full_path = os.path.join(subject_dir, fif_file)\n",
    "        raw = mne.io.read_raw_fif(full_path, preload=False, verbose=verbose)\n",
    "        raw_list.append(raw)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Loaded: {fif_file} ({len(raw.ch_names)} channels)\")\n",
    "    \n",
    "    # Find and use common channels\n",
    "    if use_common_channels and len(raw_list) > 1:\n",
    "        common_channels = set(raw_list[0].ch_names)\n",
    "        for raw in raw_list[1:]:\n",
    "            common_channels &= set(raw.ch_names)\n",
    "        \n",
    "        common_channels = sorted(list(common_channels))\n",
    "        print(f\"\\nUsing {len(common_channels)} common channels\")\n",
    "        \n",
    "        # Pick common channels from all files\n",
    "        for i, raw in enumerate(raw_list):\n",
    "            raw_list[i] = raw.copy().pick_channels(common_channels, ordered=True)\n",
    "    \n",
    "    # Concatenate FIF files\n",
    "    print(\"\\nConcatenating FIF files...\")\n",
    "    raw_concat = mne.concatenate_raws(raw_list, preload=preload, verbose=verbose)\n",
    "    \n",
    "    print(f\"✓ Neural data concatenated:\")\n",
    "    print(f\"  Duration: {raw_concat.times[-1]:.2f}s\")\n",
    "    print(f\"  Channels: {len(raw_concat.ch_names)}\")\n",
    "    print(f\"  Sampling rate: {raw_concat.info['sfreq']} Hz\")\n",
    "    \n",
    "    # Load metadata\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[2/2] Loading metadata...\")\n",
    "    print(f\"Found {len(csv_files)} CSV files:\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    metadata_list = []\n",
    "    \n",
    "    for session_idx, csv_file in enumerate(csv_files, start=1):\n",
    "        full_path = os.path.join(subject_dir, csv_file)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n  Loading session {session_idx}: {csv_file}\")\n",
    "        \n",
    "        metadata = pd.read_csv(full_path)\n",
    "        \n",
    "        # Add session identifier\n",
    "        metadata['session'] = session_idx\n",
    "        metadata['session_file'] = csv_file\n",
    "        \n",
    "        # Convert to numeric types\n",
    "        numeric_cols = ['trial_number', 'set_size', 'match', 'correct', \n",
    "                       'response', 'response_time']\n",
    "        for col in numeric_cols:\n",
    "            if col in metadata.columns:\n",
    "                metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"    Trials: {len(metadata)}\")\n",
    "            if 'correct' in metadata.columns:\n",
    "                acc = metadata['correct'].mean() * 100\n",
    "                print(f\"    Accuracy: {acc:.1f}%\")\n",
    "        \n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    # Concatenate all metadata\n",
    "    metadata_all = pd.concat(metadata_list, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n✓ Metadata concatenated:\")\n",
    "    print(f\"  Total trials: {len(metadata_all)}\")\n",
    "    print(f\"  Sessions: {metadata_all['session'].nunique()}\")\n",
    "    \n",
    "    if 'correct' in metadata_all.columns:\n",
    "        overall_acc = metadata_all['correct'].mean() * 100\n",
    "        print(f\"  Overall accuracy: {overall_acc:.1f}%\")\n",
    "    \n",
    "    if 'response_time' in metadata_all.columns:\n",
    "        overall_rt = metadata_all['response_time'].mean()\n",
    "        print(f\"  Overall mean RT: {overall_rt:.3f}s\")\n",
    "    \n",
    "    # Verify alignment\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    n_sessions_fif = len(fif_files)\n",
    "    n_sessions_meta = metadata_all['session'].nunique()\n",
    "    \n",
    "    if n_sessions_fif == n_sessions_meta:\n",
    "        print(f\"✓ Session count matches: {n_sessions_fif} sessions\")\n",
    "    else:\n",
    "        print(f\"⚠ WARNING: Session count mismatch!\")\n",
    "        print(f\"  FIF files: {n_sessions_fif}\")\n",
    "        print(f\"  CSV files: {n_sessions_meta}\")\n",
    "    \n",
    "    print(\"\\n✓ Loading complete!\")\n",
    "    \n",
    "    return raw_concat, metadata_all\n",
    "\n",
    "\n",
    "def load_and_concatenate_subject_paired(\n",
    "    subject_dir: str,\n",
    "    fif_prefix: Optional[str] = None,\n",
    "    csv_suffix: Optional[str] = None,\n",
    "    use_common_channels: bool = True,\n",
    "    preload: bool = True,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[mne.io.Raw, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and concatenate with smart FIF-CSV pairing.\n",
    "    \n",
    "    Pairs files based on shared naming (e.g., Session_01_raw.fif with Session_01_metadata.csv).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_dir : str\n",
    "        Directory containing subject's session files\n",
    "    fif_prefix : str, optional\n",
    "        Only load FIF files starting with this prefix\n",
    "    csv_suffix : str, optional\n",
    "        Only load CSV files with this suffix (e.g., 'metadata')\n",
    "    use_common_channels : bool\n",
    "        If True, only keep channels common to all files\n",
    "    preload : bool\n",
    "        Whether to load data into memory\n",
    "    verbose : bool\n",
    "        Verbose output\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    raw_concat : mne.io.Raw\n",
    "        Concatenated Raw object\n",
    "    metadata_all : pd.DataFrame\n",
    "        Concatenated metadata\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Load only files with specific naming\n",
    "    >>> raw, meta = load_and_concatenate_subject_paired(\n",
    "    ...     'data/Subject_01/',\n",
    "    ...     csv_suffix='metadata'\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"LOADING SUBJECT DATA (PAIRED MODE)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDirectory: {subject_dir}\")\n",
    "    \n",
    "    # Get all files\n",
    "    all_files = os.listdir(subject_dir)\n",
    "    \n",
    "    # Find FIF files\n",
    "    fif_files = [f for f in all_files if f.endswith('.fif')]\n",
    "    if fif_prefix:\n",
    "        fif_files = [f for f in fif_files if f.startswith(fif_prefix)]\n",
    "    fif_files = sorted(fif_files)\n",
    "    \n",
    "    # Find CSV files\n",
    "    csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "    if csv_suffix:\n",
    "        csv_files = [f for f in csv_files if csv_suffix in f.lower()]\n",
    "    csv_files = sorted(csv_files)\n",
    "    \n",
    "    # Verify\n",
    "    if len(fif_files) == 0:\n",
    "        raise ValueError(f\"No FIF files found in {subject_dir}\")\n",
    "    if len(csv_files) == 0:\n",
    "        raise ValueError(f\"No CSV files found in {subject_dir}\")\n",
    "    \n",
    "    print(f\"\\n[1/2] Loading {len(fif_files)} FIF files...\")\n",
    "    for f in fif_files:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    # Try to pair FIF with CSV files\n",
    "    paired_files = []\n",
    "    \n",
    "    for fif_file in fif_files:\n",
    "        # Extract base name (remove extension and common suffixes)\n",
    "        base_name = fif_file.replace('.fif', '').replace('_raw', '').replace('_eeg', '')\n",
    "        \n",
    "        # Look for matching CSV\n",
    "        matching_csv = None\n",
    "        for csv_file in csv_files:\n",
    "            csv_base = csv_file.replace('.csv', '').replace('_metadata', '')\n",
    "            if base_name in csv_base or csv_base in base_name:\n",
    "                matching_csv = csv_file\n",
    "                break\n",
    "        \n",
    "        if matching_csv:\n",
    "            paired_files.append((fif_file, matching_csv))\n",
    "            if verbose:\n",
    "                print(f\"  Paired: {fif_file} ↔ {matching_csv}\")\n",
    "        else:\n",
    "            # No match found, still use this FIF but warn\n",
    "            paired_files.append((fif_file, None))\n",
    "            print(f\"  ⚠ No matching CSV for: {fif_file}\")\n",
    "    \n",
    "    # Load FIF files\n",
    "    raw_list = []\n",
    "    for fif_file, _ in paired_files:\n",
    "        full_path = os.path.join(subject_dir, fif_file)\n",
    "        raw = mne.io.read_raw_fif(full_path, preload=False, verbose=verbose)\n",
    "        raw_list.append(raw)\n",
    "    \n",
    "    # Common channels\n",
    "    if use_common_channels and len(raw_list) > 1:\n",
    "        common_channels = set(raw_list[0].ch_names)\n",
    "        for raw in raw_list[1:]:\n",
    "            common_channels &= set(raw.ch_names)\n",
    "        common_channels = sorted(list(common_channels))\n",
    "        print(f\"\\nUsing {len(common_channels)} common channels\")\n",
    "        \n",
    "        for i, raw in enumerate(raw_list):\n",
    "            raw_list[i] = raw.copy().pick_channels(common_channels, ordered=True)\n",
    "    \n",
    "    # Concatenate\n",
    "    raw_concat = mne.concatenate_raws(raw_list, preload=preload, verbose=verbose)\n",
    "    print(f\"\\n✓ Neural data: {raw_concat.times[-1]:.2f}s, {len(raw_concat.ch_names)} channels\")\n",
    "    \n",
    "    # Load metadata\n",
    "    print(\"\\n[2/2] Loading metadata...\")\n",
    "    metadata_list = []\n",
    "    \n",
    "    for session_idx, (fif_file, csv_file) in enumerate(paired_files, start=1):\n",
    "        if csv_file is None:\n",
    "            print(f\"  ⚠ Skipping session {session_idx} (no CSV)\")\n",
    "            continue\n",
    "        \n",
    "        full_path = os.path.join(subject_dir, csv_file)\n",
    "        metadata = pd.read_csv(full_path)\n",
    "        metadata['session'] = session_idx\n",
    "        metadata['session_file'] = csv_file\n",
    "        metadata['fif_file'] = fif_file\n",
    "        \n",
    "        # Convert types\n",
    "        numeric_cols = ['trial_number', 'set_size', 'match', 'correct', \n",
    "                       'response', 'response_time']\n",
    "        for col in numeric_cols:\n",
    "            if col in metadata.columns:\n",
    "                metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "        \n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    if len(metadata_list) == 0:\n",
    "        raise ValueError(\"No metadata files could be loaded\")\n",
    "    \n",
    "    metadata_all = pd.concat(metadata_list, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n✓ Metadata: {len(metadata_all)} trials from {len(metadata_list)} sessions\")\n",
    "    \n",
    "    return raw_concat, metadata_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69d823",
   "metadata": {},
   "source": [
    "## 5. ICA summary pdf output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "574754ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICA_summary_pdf(data, ica, pdf_filename):\n",
    "    # Calculate explained variance properly\n",
    "    # Method 1: Use the correct approach\n",
    "    try:\n",
    "        sources = ica.get_sources(data)\n",
    "        # Calculate variance for each component\n",
    "        explained_var_array = np.var(sources.get_data(), axis=1)\n",
    "        # Normalize to get proportion\n",
    "        explained_var_array = explained_var_array / np.sum(explained_var_array)\n",
    "    except:\n",
    "        # Fallback\n",
    "        explained_var_array = np.ones(ica.n_components_) / ica.n_components_\n",
    "\n",
    "    print(f\"Explained variance shape: {explained_var_array.shape}\")\n",
    "    print(f\"Number of components: {ica.n_components_}\")\n",
    "    print(f\"Explained variance values:\\n{explained_var_array}\")\n",
    "    print(f\"Total variance: {np.sum(explained_var_array)*100:.2f}%\")\n",
    "\n",
    "    # Create PDF with all component properties\n",
    "\n",
    "    with PdfPages(pdf_filename) as pdf:\n",
    "        n_components = ica.n_components_\n",
    "        \n",
    "        for comp_idx in range(n_components):\n",
    "            # Create figure with subplots for each component\n",
    "            fig = plt.figure(figsize=(11.69, 8.27))  # A4 landscape\n",
    "            \n",
    "            # Add title with component number and explained variance\n",
    "            var_text = f'{explained_var_array[comp_idx]*100:.2f}%'\n",
    "            fig.suptitle(f'ICA{comp_idx:03d} - Explained Variance: {var_text}', \n",
    "                        fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # 1. Topography\n",
    "            ax1 = plt.subplot(2, 3, 1)\n",
    "            ica.plot_components(picks=comp_idx, axes=ax1, show=False, colorbar=True)\n",
    "            \n",
    "            # Get source data for this component\n",
    "            sources = ica.get_sources(raw)\n",
    "            sfreq = raw.info['sfreq']\n",
    "            \n",
    "            # 2. Time course (using first 10 seconds of data)\n",
    "            ax2 = plt.subplot(2, 3, 2)\n",
    "            n_samples = min(int(10 * sfreq), sources.n_times)\n",
    "            times = sources.times[:n_samples]\n",
    "            data = sources.get_data(picks=comp_idx)[:, :n_samples]\n",
    "            ax2.plot(times, data.T, 'k', linewidth=0.5)\n",
    "            ax2.set_xlabel('Time (s)')\n",
    "            ax2.set_ylabel('AU')\n",
    "            ax2.set_title('Time Course (first 10s)')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # 3. Power Spectrum (fixed)\n",
    "            ax3 = plt.subplot(2, 3, 3)\n",
    "            from mne.time_frequency import psd_array_welch\n",
    "            \n",
    "            # Use all available data for PSD\n",
    "            data_full = sources.get_data(picks=comp_idx)\n",
    "            \n",
    "            # Adjust n_fft based on available data\n",
    "            n_fft = min(2048, data_full.shape[1])\n",
    "            n_per_seg = min(n_fft, data_full.shape[1])\n",
    "            \n",
    "            psds, freqs = psd_array_welch(\n",
    "                data_full, \n",
    "                sfreq=sfreq, \n",
    "                fmin=0.5, \n",
    "                fmax=50, \n",
    "                n_fft=n_fft,\n",
    "                n_per_seg=n_per_seg\n",
    "            )\n",
    "            \n",
    "            ax3.semilogy(freqs, psds.T, 'k', linewidth=1)\n",
    "            ax3.set_xlabel('Frequency (Hz)')\n",
    "            ax3.set_ylabel('Power Spectral Density (µV²/Hz)')\n",
    "            ax3.set_title('Power Spectrum')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            ax3.set_xlim([0, 50])\n",
    "            \n",
    "            # 4. Component properties (alternative visualization)\n",
    "            ax4 = plt.subplot(2, 3, 4)\n",
    "            # Plot first 2 seconds in more detail\n",
    "            n_samples_detail = min(int(2 * sfreq), sources.n_times)\n",
    "            times_detail = sources.times[:n_samples_detail]\n",
    "            data_detail = sources.get_data(picks=comp_idx)[:, :n_samples_detail]\n",
    "            ax4.plot(times_detail, data_detail.T, 'b', linewidth=0.8)\n",
    "            ax4.set_xlabel('Time (s)')\n",
    "            ax4.set_ylabel('AU')\n",
    "            ax4.set_title('Time Course Detail (first 2s)')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            # 5. Variance bar (showing this component in context)\n",
    "            ax5 = plt.subplot(2, 3, 5)\n",
    "            colors = ['red' if i == comp_idx else 'gray' for i in range(n_components)]\n",
    "            ax5.bar(range(n_components), explained_var_array * 100, color=colors, alpha=0.6)\n",
    "            ax5.set_xlabel('Component')\n",
    "            ax5.set_ylabel('Explained Variance (%)')\n",
    "            ax5.set_title('Variance Explained by All Components')\n",
    "            ax5.axhline(y=5, color='r', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "            ax5.legend()\n",
    "            ax5.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # 6. Properties statistics\n",
    "            ax6 = plt.subplot(2, 3, 6)\n",
    "            ax6.axis('off')\n",
    "            \n",
    "            # Calculate some statistics\n",
    "            data_stats = sources.get_data(picks=comp_idx).flatten()\n",
    "            stats_text = f\"\"\"Component Statistics:\n",
    "            \n",
    "    Mean: {np.mean(data_stats):.3f}\n",
    "    Std: {np.std(data_stats):.3f}\n",
    "    Min: {np.min(data_stats):.3f}\n",
    "    Max: {np.max(data_stats):.3f}\n",
    "    Kurtosis: {np.mean((data_stats - np.mean(data_stats))**4) / (np.std(data_stats)**4):.3f}\n",
    "\n",
    "    Explained Variance: {explained_var_array[comp_idx]*100:.2f}%\n",
    "    Rank by Variance: #{np.where(np.argsort(explained_var_array)[::-1] == comp_idx)[0][0] + 1}\n",
    "            \"\"\"\n",
    "            \n",
    "            ax6.text(0.1, 0.9, stats_text, \n",
    "                    transform=ax6.transAxes,\n",
    "                    fontsize=10,\n",
    "                    verticalalignment='top',\n",
    "                    fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            \n",
    "            print(f\"Processed component {comp_idx+1}/{n_components}\")\n",
    "        \n",
    "        # Add summary page at the end\n",
    "        fig_summary = plt.figure(figsize=(11.69, 8.27))\n",
    "        fig_summary.suptitle('ICA Components Summary', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Variance explained table\n",
    "        ax_table = plt.subplot(1, 2, 1)\n",
    "        ax_table.axis('tight')\n",
    "        ax_table.axis('off')\n",
    "        \n",
    "        # Sort by variance for the table\n",
    "        sorted_indices = np.argsort(explained_var_array)[::-1]\n",
    "        table_data = [[f'ICA{i:03d}', f'{explained_var_array[i]*100:.2f}%', f'#{rank+1}'] \n",
    "                    for rank, i in enumerate(sorted_indices)]\n",
    "        \n",
    "        table = ax_table.table(cellText=table_data, \n",
    "                            colLabels=['Component', 'Variance (%)', 'Rank'],\n",
    "                            cellLoc='center',\n",
    "                            loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(8)\n",
    "        table.scale(1, 1.5)\n",
    "        ax_table.set_title('Components Ranked by Variance', fontsize=12, pad=20)\n",
    "        \n",
    "        # Overall variance plot\n",
    "        ax_var = plt.subplot(1, 2, 2)\n",
    "        ax_var.bar(range(n_components), explained_var_array * 100, color='steelblue', alpha=0.7)\n",
    "        ax_var.set_xlabel('Component Index')\n",
    "        ax_var.set_ylabel('Explained Variance (%)')\n",
    "        ax_var.set_title('Variance Explained by Each Component')\n",
    "        ax_var.axhline(y=5, color='r', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "        ax_var.legend()\n",
    "        ax_var.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add text summary\n",
    "        total_var = np.sum(explained_var_array) * 100\n",
    "        ax_var.text(0.02, 0.98, f'Total variance: {total_var:.2f}%',\n",
    "                    transform=ax_var.transAxes, \n",
    "                    verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig_summary, bbox_inches='tight')\n",
    "        plt.close(fig_summary)\n",
    "\n",
    "    print(f'\\nPDF report saved to: {pdf_filename}')\n",
    "\n",
    "    # Print summary to console\n",
    "    print('\\nExplained Variance Summary (sorted):')\n",
    "    print('-' * 50)\n",
    "    sorted_indices = np.argsort(explained_var_array)[::-1]\n",
    "    for rank, i in enumerate(sorted_indices):\n",
    "        print(f'#{rank+1:2d} - ICA{i:03d}: {explained_var_array[i]*100:6.2f}%')\n",
    "    print('-' * 50)\n",
    "    print(f'Total: {np.sum(explained_var_array)*100:6.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa3d36",
   "metadata": {},
   "source": [
    "## 6. Exclusion Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1d88e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ica_rejection_report(ica, raw, exclude_components, output_filename='ica_rejection_report.pdf'):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive PDF report for ICA component rejection.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ica : mne.preprocessing.ICA\n",
    "        Fitted ICA object\n",
    "    raw : mne.io.Raw\n",
    "        Raw data (should be the same data or filtered version used for ICA)\n",
    "    exclude_components : list\n",
    "        List of component indices to exclude (e.g., [0, 1, 2])\n",
    "    output_filename : str\n",
    "        Path for output PDF file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Summary statistics dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add .pdf extension if not present\n",
    "    if not output_filename.endswith('.pdf'):\n",
    "        output_filename += '.pdf'\n",
    "    \n",
    "    # Calculate variance explained by each component\n",
    "    sources = ica.get_sources(raw)\n",
    "    explained_var = np.var(sources.get_data(), axis=1)\n",
    "    explained_var_ratio = explained_var / np.sum(explained_var)\n",
    "    \n",
    "    # Calculate statistics for excluded components\n",
    "    total_var_removed = np.sum(explained_var_ratio[exclude_components]) * 100\n",
    "    n_total_components = ica.n_components_\n",
    "    n_excluded = len(exclude_components)\n",
    "    n_kept = n_total_components - n_excluded\n",
    "    \n",
    "    # Calculate kurtosis for all components\n",
    "    kurtosis_values = []\n",
    "    for i in range(n_total_components):\n",
    "        comp_data = sources.get_data(picks=i).flatten()\n",
    "        kurt = np.mean((comp_data - np.mean(comp_data))**4) / (np.std(comp_data)**4)\n",
    "        kurtosis_values.append(kurt)\n",
    "    \n",
    "    # Create summary dictionary\n",
    "    summary = {\n",
    "        'total_components': n_total_components,\n",
    "        'excluded_components': exclude_components,\n",
    "        'kept_components': [i for i in range(n_total_components) if i not in exclude_components],\n",
    "        'n_excluded': n_excluded,\n",
    "        'n_kept': n_kept,\n",
    "        'total_variance_removed': total_var_removed,\n",
    "        'variance_per_excluded': {f'ICA{i:03d}': explained_var_ratio[i]*100 for i in exclude_components},\n",
    "        'kurtosis_per_excluded': {f'ICA{i:03d}': kurtosis_values[i] for i in exclude_components}\n",
    "    }\n",
    "    \n",
    "    # Create PDF report\n",
    "    with PdfPages(output_filename) as pdf:\n",
    "        # Page 1: Summary Overview\n",
    "        fig = plt.figure(figsize=(11, 8.5))\n",
    "        fig.suptitle('ICA Component Rejection Report', fontsize=18, fontweight='bold')\n",
    "        \n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "REJECTION SUMMARY\n",
    "{'='*80}\n",
    "\n",
    "Total Components: {n_total_components}\n",
    "Excluded: {n_excluded} components\n",
    "Kept: {n_kept} components\n",
    "\n",
    "EXCLUDED COMPONENTS: {', '.join([f'ICA{i:03d}' for i in exclude_components])}\n",
    "\n",
    "VARIANCE ANALYSIS\n",
    "{'-'*80}\n",
    "Total variance removed: {total_var_removed:.2f}%\n",
    "\n",
    "Variance per excluded component:\n",
    "\"\"\"\n",
    "        for comp_idx in exclude_components:\n",
    "            var_pct = explained_var_ratio[comp_idx] * 100\n",
    "            kurt = kurtosis_values[comp_idx]\n",
    "            rank = np.where(np.argsort(explained_var_ratio)[::-1] == comp_idx)[0][0] + 1\n",
    "            summary_text += f\"  ICA{comp_idx:03d}: {var_pct:6.2f}% (Rank #{rank:2d}, Kurtosis: {kurt:7.2f})\\n\"\n",
    "        \n",
    "        summary_text += f\"\"\"\n",
    "ARTIFACT SIGNATURES\n",
    "{'-'*80}\n",
    "\"\"\"\n",
    "        for comp_idx in exclude_components:\n",
    "            kurt = kurtosis_values[comp_idx]\n",
    "            if kurt > 50:\n",
    "                artifact_type = \"Strong artifact (very high kurtosis)\"\n",
    "            elif kurt > 30:\n",
    "                artifact_type = \"Likely artifact (high kurtosis)\"\n",
    "            elif kurt > 10:\n",
    "                artifact_type = \"Possible artifact (moderate kurtosis)\"\n",
    "            else:\n",
    "                artifact_type = \"Review needed (low kurtosis for rejection)\"\n",
    "            summary_text += f\"  ICA{comp_idx:03d}: {artifact_type}\\n\"\n",
    "        \n",
    "        ax.text(0.05, 0.95, summary_text, transform=ax.transAxes,\n",
    "                fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Page 2: Before/After EEG Comparison\n",
    "        fig = plt.figure(figsize=(11, 8.5))\n",
    "        fig.suptitle('EEG Data: Before and After ICA Artifact Rejection', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Prepare data\n",
    "        raw_display = raw.copy().filter(l_freq=None, h_freq=40)\n",
    "        raw_clean = raw.copy().filter(l_freq=None, h_freq=40)\n",
    "        ica.apply(raw_clean, exclude=exclude_components)\n",
    "        \n",
    "        # Select time window (first 10 seconds or less)\n",
    "        duration = min(10, raw_display.times[-1])\n",
    "        n_samples = int(duration * raw_display.info['sfreq'])\n",
    "        times = raw_display.times[:n_samples]\n",
    "        \n",
    "        # Get channel names (limit to first 10 channels for clarity)\n",
    "        n_channels_display = min(10, len(raw_display.ch_names))\n",
    "        picks = range(n_channels_display)\n",
    "        ch_names = [raw_display.ch_names[i] for i in picks]\n",
    "        \n",
    "        # Get data\n",
    "        data_before = raw_display.get_data(picks=picks, start=0, stop=n_samples)\n",
    "        data_after = raw_clean.get_data(picks=picks, start=0, stop=n_samples)\n",
    "        \n",
    "        # Plot before\n",
    "        ax1 = plt.subplot(2, 1, 1)\n",
    "        offset = np.arange(n_channels_display) * np.max(np.abs(data_before)) * 2\n",
    "        for i, ch_data in enumerate(data_before):\n",
    "            ax1.plot(times, ch_data + offset[i], 'k', linewidth=0.5, alpha=0.7)\n",
    "        ax1.set_yticks(offset)\n",
    "        ax1.set_yticklabels(ch_names)\n",
    "        ax1.set_xlabel('Time (s)')\n",
    "        ax1.set_title('Before ICA Rejection', fontsize=12, fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_xlim([0, duration])\n",
    "        \n",
    "        # Plot after\n",
    "        ax2 = plt.subplot(2, 1, 2)\n",
    "        offset = np.arange(n_channels_display) * np.max(np.abs(data_after)) * 2\n",
    "        for i, ch_data in enumerate(data_after):\n",
    "            ax2.plot(times, ch_data + offset[i], 'b', linewidth=0.5, alpha=0.7)\n",
    "        ax2.set_yticks(offset)\n",
    "        ax2.set_yticklabels(ch_names)\n",
    "        ax2.set_xlabel('Time (s)')\n",
    "        ax2.set_title('After ICA Rejection', fontsize=12, fontweight='bold', color='blue')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xlim([0, duration])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Page 3: Combined overlay plot for all excluded components\n",
    "        fig = ica.plot_overlay(raw_display, exclude=exclude_components, picks='eeg', show=False)\n",
    "        excluded_list = ', '.join([f'ICA{i:03d}' for i in exclude_components])\n",
    "        fig.suptitle(f'Overlay: Effect of Removing All Excluded Components\\n({excluded_list})', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Page 4: Overall RMS comparison\n",
    "        fig = plt.figure(figsize=(11, 8.5))\n",
    "        fig.suptitle('Signal Quality Metrics: Before vs After ICA', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Calculate RMS for all channels\n",
    "        rms_before = np.sqrt(np.mean(raw_display.get_data()**2, axis=1))\n",
    "        rms_after = np.sqrt(np.mean(raw_clean.get_data()**2, axis=1))\n",
    "        \n",
    "        # RMS comparison\n",
    "        ax1 = plt.subplot(2, 2, 1)\n",
    "        x = np.arange(len(rms_before))\n",
    "        width = 0.35\n",
    "        ax1.bar(x - width/2, rms_before * 1e6, width, label='Before', alpha=0.7, color='red')\n",
    "        ax1.bar(x + width/2, rms_after * 1e6, width, label='After', alpha=0.7, color='blue')\n",
    "        ax1.set_xlabel('Channel Index')\n",
    "        ax1.set_ylabel('RMS (µV)')\n",
    "        ax1.set_title('RMS Amplitude by Channel')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # RMS reduction percentage\n",
    "        ax2 = plt.subplot(2, 2, 2)\n",
    "        rms_reduction = ((rms_before - rms_after) / rms_before) * 100\n",
    "        colors = ['green' if r > 0 else 'red' for r in rms_reduction]\n",
    "        ax2.bar(x, rms_reduction, color=colors, alpha=0.7)\n",
    "        ax2.set_xlabel('Channel Index')\n",
    "        ax2.set_ylabel('RMS Reduction (%)')\n",
    "        ax2.set_title('Artifact Reduction by Channel')\n",
    "        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Overall variance before/after\n",
    "        ax3 = plt.subplot(2, 2, 3)\n",
    "        var_before = np.var(raw_display.get_data())\n",
    "        var_after = np.var(raw_clean.get_data())\n",
    "        ax3.bar(['Before ICA', 'After ICA'], [var_before * 1e12, var_after * 1e12], \n",
    "               color=['red', 'blue'], alpha=0.7)\n",
    "        ax3.set_ylabel('Variance (µV²)')\n",
    "        ax3.set_title('Overall Signal Variance')\n",
    "        ax3.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Statistics table\n",
    "        ax4 = plt.subplot(2, 2, 4)\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        stats_text = f\"\"\"SIGNAL QUALITY METRICS\n",
    "\n",
    "Before ICA Rejection:\n",
    "  Mean RMS: {np.mean(rms_before)*1e6:.2f} µV\n",
    "  Std RMS:  {np.std(rms_before)*1e6:.2f} µV\n",
    "  Variance: {var_before*1e12:.2f} µV²\n",
    "\n",
    "After ICA Rejection:\n",
    "  Mean RMS: {np.mean(rms_after)*1e6:.2f} µV\n",
    "  Std RMS:  {np.std(rms_after)*1e6:.2f} µV\n",
    "  Variance: {var_after*1e12:.2f} µV²\n",
    "\n",
    "Overall Reduction:\n",
    "  RMS reduction: {((np.mean(rms_before) - np.mean(rms_after))/np.mean(rms_before)*100):.2f}%\n",
    "  Variance reduction: {((var_before - var_after)/var_before*100):.2f}%\n",
    "\"\"\"\n",
    "        \n",
    "        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes,\n",
    "                fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Page 5: Variance Distribution\n",
    "        fig = plt.figure(figsize=(11, 8.5))\n",
    "        fig.suptitle('Variance Distribution Analysis', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Variance bar plot\n",
    "        ax1 = plt.subplot(2, 2, 1)\n",
    "        colors = ['red' if i in exclude_components else 'steelblue' \n",
    "                 for i in range(n_total_components)]\n",
    "        ax1.bar(range(n_total_components), explained_var_ratio * 100, \n",
    "               color=colors, alpha=0.7)\n",
    "        ax1.set_xlabel('Component Index')\n",
    "        ax1.set_ylabel('Explained Variance (%)')\n",
    "        ax1.set_title('Variance by Component')\n",
    "        ax1.axhline(y=5, color='orange', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Cumulative variance\n",
    "        ax2 = plt.subplot(2, 2, 2)\n",
    "        cumvar = np.cumsum(explained_var_ratio) * 100\n",
    "        ax2.plot(range(n_total_components), cumvar, 'b-', linewidth=2)\n",
    "        for idx in exclude_components:\n",
    "            ax2.axvline(x=idx, color='red', linestyle='--', alpha=0.5)\n",
    "        ax2.set_xlabel('Component Index')\n",
    "        ax2.set_ylabel('Cumulative Variance (%)')\n",
    "        ax2.set_title('Cumulative Variance Explained')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Kurtosis distribution\n",
    "        ax3 = plt.subplot(2, 2, 3)\n",
    "        colors_kurt = ['red' if i in exclude_components else 'steelblue' \n",
    "                      for i in range(n_total_components)]\n",
    "        ax3.bar(range(n_total_components), kurtosis_values, \n",
    "               color=colors_kurt, alpha=0.7)\n",
    "        ax3.set_xlabel('Component Index')\n",
    "        ax3.set_ylabel('Kurtosis')\n",
    "        ax3.set_title('Kurtosis by Component')\n",
    "        ax3.axhline(y=30, color='orange', linestyle='--', alpha=0.5, \n",
    "                   label='High kurtosis threshold')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3, axis='y')\n",
    "        ax3.set_yscale('log')\n",
    "        \n",
    "        # Pie chart of variance\n",
    "        ax4 = plt.subplot(2, 2, 4)\n",
    "        kept_var = np.sum(explained_var_ratio[[i for i in range(n_total_components) \n",
    "                                               if i not in exclude_components]]) * 100\n",
    "        removed_var = total_var_removed\n",
    "        \n",
    "        ax4.pie([kept_var, removed_var], \n",
    "               labels=['Kept Components', 'Removed Artifacts'],\n",
    "               colors=['steelblue', 'red'],\n",
    "               autopct='%1.1f%%',\n",
    "               startangle=90)\n",
    "        ax4.set_title('Variance Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Page 6+: Individual component details for excluded components\n",
    "        for comp_idx in exclude_components:\n",
    "            fig = plt.figure(figsize=(11, 8.5))\n",
    "            fig.suptitle(f'ICA{comp_idx:03d} - REJECTED COMPONENT', \n",
    "                        fontsize=14, fontweight='bold', color='red')\n",
    "            \n",
    "            # Topography\n",
    "            ax1 = plt.subplot(2, 3, 1)\n",
    "            ica.plot_components(picks=comp_idx, axes=ax1, show=False, colorbar=True)\n",
    "            \n",
    "            # Time course\n",
    "            ax2 = plt.subplot(2, 3, 2)\n",
    "            comp_data = sources.get_data(picks=comp_idx)\n",
    "            times = sources.times[:min(int(10 * raw.info['sfreq']), len(sources.times))]\n",
    "            data = comp_data[:, :len(times)]\n",
    "            ax2.plot(times, data.T, 'k', linewidth=0.5)\n",
    "            ax2.set_xlabel('Time (s)')\n",
    "            ax2.set_ylabel('AU')\n",
    "            ax2.set_title('Time Course (first 10s)')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Power spectrum\n",
    "            ax3 = plt.subplot(2, 3, 3)\n",
    "            from mne.time_frequency import psd_array_welch\n",
    "            n_fft = min(2048, comp_data.shape[1])\n",
    "            psds, freqs = psd_array_welch(comp_data, sfreq=raw.info['sfreq'],\n",
    "                                         fmin=0.5, fmax=50, n_fft=n_fft,\n",
    "                                         n_per_seg=n_fft)\n",
    "            ax3.semilogy(freqs, psds.T, 'k', linewidth=1)\n",
    "            ax3.set_xlabel('Frequency (Hz)')\n",
    "            ax3.set_ylabel('PSD (µV²/Hz)')\n",
    "            ax3.set_title('Power Spectrum')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            ax3.set_xlim([0, 50])\n",
    "            \n",
    "            # Time course detail (first 2s)\n",
    "            ax4 = plt.subplot(2, 3, 4)\n",
    "            times_detail = sources.times[:min(int(2 * raw.info['sfreq']), len(sources.times))]\n",
    "            data_detail = comp_data[:, :len(times_detail)]\n",
    "            ax4.plot(times_detail, data_detail.T, 'b', linewidth=0.8)\n",
    "            ax4.set_xlabel('Time (s)')\n",
    "            ax4.set_ylabel('AU')\n",
    "            ax4.set_title('Time Course Detail (first 2s)')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Statistics box\n",
    "            ax5 = plt.subplot(2, 3, 5)\n",
    "            ax5.axis('off')\n",
    "            \n",
    "            var_pct = explained_var_ratio[comp_idx] * 100\n",
    "            kurt = kurtosis_values[comp_idx]\n",
    "            rank = np.where(np.argsort(explained_var_ratio)[::-1] == comp_idx)[0][0] + 1\n",
    "            comp_stats = sources.get_data(picks=comp_idx).flatten()\n",
    "            \n",
    "            stats_text = f\"\"\"Component Statistics:\n",
    "\n",
    "Mean: {np.mean(comp_stats):.3f}\n",
    "Std: {np.std(comp_stats):.3f}\n",
    "Min: {np.min(comp_stats):.3f}\n",
    "Max: {np.max(comp_stats):.3f}\n",
    "Kurtosis: {kurt:.3f}\n",
    "\n",
    "Explained Variance: {var_pct:.2f}%\n",
    "Rank by Variance: #{rank}\n",
    "\n",
    "REJECTION RATIONALE:\n",
    "\"\"\"\n",
    "            if kurt > 100:\n",
    "                stats_text += \"• EXTREME artifact signature\\n\"\n",
    "                stats_text += \"• Likely eye blinks/movements\"\n",
    "            elif kurt > 50:\n",
    "                stats_text += \"• Very strong artifact\\n\"\n",
    "                stats_text += \"• Eye or muscle artifact\"\n",
    "            elif kurt > 30:\n",
    "                stats_text += \"• Strong artifact signature\\n\"\n",
    "                stats_text += \"• Probable artifact\"\n",
    "            else:\n",
    "                stats_text += \"• Moderate signature\\n\"\n",
    "                stats_text += \"• Review recommended\"\n",
    "            \n",
    "            ax5.text(0.1, 0.9, stats_text, transform=ax5.transAxes,\n",
    "                    fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle='round', facecolor='salmon', alpha=0.3))\n",
    "            \n",
    "            # Component in context\n",
    "            ax6 = plt.subplot(2, 3, 6)\n",
    "            colors_context = ['red' if i == comp_idx else 'gray' \n",
    "                            for i in range(n_total_components)]\n",
    "            ax6.bar(range(n_total_components), explained_var_ratio * 100, \n",
    "                   color=colors_context, alpha=0.6)\n",
    "            ax6.set_xlabel('Component')\n",
    "            ax6.set_ylabel('Explained Variance (%)')\n",
    "            ax6.set_title('This Component in Context')\n",
    "            ax6.axhline(y=5, color='r', linestyle='--', alpha=0.5)\n",
    "            ax6.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ICA REJECTION REPORT GENERATED\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Report saved to: {output_filename}\")\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Total components: {n_total_components}\")\n",
    "    print(f\"  Rejected: {n_excluded}\")\n",
    "    print(f\"  Kept: {n_kept}\")\n",
    "    print(f\"  Variance removed: {total_var_removed:.2f}%\")\n",
    "    print(f\"\\nRejected components:\")\n",
    "    for comp_idx in exclude_components:\n",
    "        var_pct = explained_var_ratio[comp_idx] * 100\n",
    "        kurt = kurtosis_values[comp_idx]\n",
    "        print(f\"  ICA{comp_idx:03d}: {var_pct:6.2f}% variance, kurtosis={kurt:.2f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa1eb2e",
   "metadata": {},
   "source": [
    "## 7. Usage Example: Load Subject Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73f74659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING SUBJECT DATA\n",
      "================================================================================\n",
      "\n",
      "Directory: Data_converted_MetaData\\Subject_08\n",
      "\n",
      "[1/2] Loading FIF files...\n",
      "Found 5 FIF files:\n",
      "  Data_Subject_08_Session_01.h5_seeg_raw.fif\n",
      "  Data_Subject_08_Session_02.h5_seeg_raw.fif\n",
      "  Data_Subject_08_Session_03.h5_seeg_raw.fif\n",
      "  Data_Subject_08_Session_04.h5_seeg_raw.fif\n",
      "  Data_Subject_08_Session_05.h5_seeg_raw.fif\n",
      "Opening raw data file Data_converted_MetaData\\Subject_08\\Data_Subject_08_Session_01.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 79999 =      0.000 ...   399.995 secs\n",
      "Ready.\n",
      "  Loaded: Data_Subject_08_Session_01.h5_seeg_raw.fif (20 channels)\n",
      "Opening raw data file Data_converted_MetaData\\Subject_08\\Data_Subject_08_Session_02.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 79999 =      0.000 ...   399.995 secs\n",
      "Ready.\n",
      "  Loaded: Data_Subject_08_Session_02.h5_seeg_raw.fif (20 channels)\n",
      "Opening raw data file Data_converted_MetaData\\Subject_08\\Data_Subject_08_Session_03.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 79999 =      0.000 ...   399.995 secs\n",
      "Ready.\n",
      "  Loaded: Data_Subject_08_Session_03.h5_seeg_raw.fif (20 channels)\n",
      "Opening raw data file Data_converted_MetaData\\Subject_08\\Data_Subject_08_Session_04.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 79999 =      0.000 ...   399.995 secs\n",
      "Ready.\n",
      "  Loaded: Data_Subject_08_Session_04.h5_seeg_raw.fif (20 channels)\n",
      "Opening raw data file Data_converted_MetaData\\Subject_08\\Data_Subject_08_Session_05.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 78399 =      0.000 ...   391.995 secs\n",
      "Ready.\n",
      "  Loaded: Data_Subject_08_Session_05.h5_seeg_raw.fif (20 channels)\n",
      "\n",
      "Using 20 common channels\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\RSKALA\\AppData\\Local\\Temp\\ipykernel_9880\\740267351.py:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  subject_dir = 'Data_converted_MetaData\\Subject_08'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concatenating FIF files...\n",
      "✓ Neural data concatenated:\n",
      "  Duration: 1991.99s\n",
      "  Channels: 20\n",
      "  Sampling rate: 200.0 Hz\n",
      "\n",
      "================================================================================\n",
      "[2/2] Loading metadata...\n",
      "Found 5 CSV files:\n",
      "  Data_Subject_08_Session_01.h5_seeg_raw.csv\n",
      "  Data_Subject_08_Session_02.h5_seeg_raw.csv\n",
      "  Data_Subject_08_Session_03.h5_seeg_raw.csv\n",
      "  Data_Subject_08_Session_04.h5_seeg_raw.csv\n",
      "  Data_Subject_08_Session_05.h5_seeg_raw.csv\n",
      "\n",
      "  Loading session 1: Data_Subject_08_Session_01.h5_seeg_raw.csv\n",
      "    Trials: 50\n",
      "    Accuracy: 88.0%\n",
      "\n",
      "  Loading session 2: Data_Subject_08_Session_02.h5_seeg_raw.csv\n",
      "    Trials: 50\n",
      "    Accuracy: 94.0%\n",
      "\n",
      "  Loading session 3: Data_Subject_08_Session_03.h5_seeg_raw.csv\n",
      "    Trials: 50\n",
      "    Accuracy: 86.0%\n",
      "\n",
      "  Loading session 4: Data_Subject_08_Session_04.h5_seeg_raw.csv\n",
      "    Trials: 50\n",
      "    Accuracy: 88.0%\n",
      "\n",
      "  Loading session 5: Data_Subject_08_Session_05.h5_seeg_raw.csv\n",
      "    Trials: 49\n",
      "    Accuracy: 93.9%\n",
      "\n",
      "✓ Metadata concatenated:\n",
      "  Total trials: 249\n",
      "  Sessions: 5\n",
      "  Overall accuracy: 90.0%\n",
      "  Overall mean RT: 1.538s\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION\n",
      "================================================================================\n",
      "✓ Session count matches: 5 sessions\n",
      "\n",
      "✓ Loading complete!\n",
      "\n",
      "================================================================================\n",
      "SUCCESS!\n",
      "================================================================================\n",
      "Loaded 20 channels, 1992.0s\n",
      "Loaded 249 trials\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Example 1: Simple loading (all .fif and .csv files)\n",
    "    subject_dir = 'Data_converted_MetaData\\Subject_08'\n",
    "    \n",
    "    try:\n",
    "        raw, metadata = load_and_concatenate_subject(\n",
    "            subject_dir=subject_dir,\n",
    "            use_common_channels=True,\n",
    "            preload=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUCCESS!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Loaded {len(raw.ch_names)} channels, {raw.times[-1]:.1f}s\")\n",
    "        print(f\"Loaded {len(metadata)} trials\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nDirectory not found: {e}\")\n",
    "        print(\"Please update subject_dir to point to your data\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "raw_cropped = raw.copy().crop(tmax = 1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdff9cc",
   "metadata": {},
   "source": [
    "## 8. Band-pass Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c59b212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 5 contiguous segments\n",
      "Setting up high-pass filter at 1 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal highpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Filter length: 661 samples (3.305 s)\n",
      "\n",
      "Filtering raw data in 5 contiguous segments\n",
      "Setting up high-pass filter at 1 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal highpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Filter length: 661 samples (3.305 s)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_filtered_highpass = raw_cropped.copy().filter(l_freq=1, h_freq = None, verbose=True)\n",
    "raw_filtered_both = raw_cropped.copy().filter(l_freq=1, h_freq = None, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75fc6d",
   "metadata": {},
   "source": [
    "## 9. ICA Decleration & Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1452013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 19 channels (please be patient, this may take a while)\n",
      "Selecting by number: 19 components\n",
      "Fitting ICA took 8.7s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table mne-repr-table\">\n",
       "    <tr>\n",
       "        <th>Method</th>\n",
       "        <td>picard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit parameters</th>\n",
       "        <td>max_iter=500<br /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit</th>\n",
       "        <td>52 iterations on raw data (320001 samples)</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>ICA components</th>\n",
       "        <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Available PCA components</th>\n",
       "        <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Channel types</th>\n",
       "        <td>eeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ICA components marked for exclusion</th>\n",
       "        <td>&mdash;</td>\n",
       "    </tr>\n",
       "    \n",
       "</table>"
      ],
      "text/plain": [
       "<ICA | raw data decomposition, method: picard (fit in 52 iterations on 320001 samples), 19 ICA components (19 PCA components available), channel types: eeg, no sources marked for exclusion>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 19  # Should normally be higher, like 0.999!!\n",
    "method = 'picard'\n",
    "max_iter = 500  # Should normally be higher, like 500 or even 1000!!\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "ica_highpass = mne.preprocessing.ICA(n_components=n_components,\n",
    "                            method=method,\n",
    "                            max_iter=max_iter,\n",
    "                            random_state=random_state)\n",
    "ica_highpass.fit(raw_filtered_highpass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "756f1fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 19 channels (please be patient, this may take a while)\n",
      "Selecting by number: 19 components\n",
      "Fitting ICA took 13.1s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table mne-repr-table\">\n",
       "    <tr>\n",
       "        <th>Method</th>\n",
       "        <td>picard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit parameters</th>\n",
       "        <td>max_iter=500<br /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit</th>\n",
       "        <td>52 iterations on raw data (320001 samples)</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>ICA components</th>\n",
       "        <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Available PCA components</th>\n",
       "        <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Channel types</th>\n",
       "        <td>eeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ICA components marked for exclusion</th>\n",
       "        <td>&mdash;</td>\n",
       "    </tr>\n",
       "    \n",
       "</table>"
      ],
      "text/plain": [
       "<ICA | raw data decomposition, method: picard (fit in 52 iterations on 320001 samples), 19 ICA components (19 PCA components available), channel types: eeg, no sources marked for exclusion>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 19  # Should normally be higher, like 0.999!!\n",
    "method = 'picard'\n",
    "max_iter = 500  # Should normally be higher, like 500 or even 1000!!\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "ica_both = mne.preprocessing.ICA(n_components=n_components,\n",
    "                            method=method,\n",
    "                            max_iter=max_iter,\n",
    "                            random_state=random_state)\n",
    "ica_both.fit(raw_filtered_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b1a86",
   "metadata": {},
   "source": [
    "## 10. Visualisation & Saving as PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a9a1d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance shape: (19,)\n",
      "Number of components: 19\n",
      "Explained variance values:\n",
      "[0.04947421 0.02753686 0.09322267 0.00229592 0.03495261 0.0461842\n",
      " 0.04977761 0.00961155 0.06364997 0.04617864 0.0675893  0.08297366\n",
      " 0.04590059 0.0433387  0.07543416 0.05668208 0.08496475 0.05459133\n",
      " 0.06564119]\n",
      "Total variance: 100.00%\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 1/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 2/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 3/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 4/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 5/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 6/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 7/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 8/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 9/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 10/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 11/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 12/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 13/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 14/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 15/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 16/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 17/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 18/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 19/19\n",
      "\n",
      "PDF report saved to: ICA_report_highpass.pdf\n",
      "\n",
      "Explained Variance Summary (sorted):\n",
      "--------------------------------------------------\n",
      "# 1 - ICA002:   9.32%\n",
      "# 2 - ICA016:   8.50%\n",
      "# 3 - ICA011:   8.30%\n",
      "# 4 - ICA014:   7.54%\n",
      "# 5 - ICA010:   6.76%\n",
      "# 6 - ICA018:   6.56%\n",
      "# 7 - ICA008:   6.36%\n",
      "# 8 - ICA015:   5.67%\n",
      "# 9 - ICA017:   5.46%\n",
      "#10 - ICA006:   4.98%\n",
      "#11 - ICA000:   4.95%\n",
      "#12 - ICA005:   4.62%\n",
      "#13 - ICA009:   4.62%\n",
      "#14 - ICA012:   4.59%\n",
      "#15 - ICA013:   4.33%\n",
      "#16 - ICA004:   3.50%\n",
      "#17 - ICA001:   2.75%\n",
      "#18 - ICA007:   0.96%\n",
      "#19 - ICA003:   0.23%\n",
      "--------------------------------------------------\n",
      "Total: 100.00%\n",
      "Explained variance shape: (19,)\n",
      "Number of components: 19\n",
      "Explained variance values:\n",
      "[0.04947421 0.02753686 0.09322267 0.00229592 0.03495261 0.0461842\n",
      " 0.04977761 0.00961155 0.06364997 0.04617864 0.0675893  0.08297366\n",
      " 0.04590059 0.0433387  0.07543416 0.05668208 0.08496475 0.05459133\n",
      " 0.06564119]\n",
      "Total variance: 100.00%\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 1/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 2/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 3/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 4/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 5/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 6/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 7/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 8/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 9/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 10/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 11/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 12/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 13/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 14/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 15/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 16/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 17/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 18/19\n",
      "Effective window size : 10.240 (s)\n",
      "Processed component 19/19\n",
      "\n",
      "PDF report saved to: ICA_report_both.pdf\n",
      "\n",
      "Explained Variance Summary (sorted):\n",
      "--------------------------------------------------\n",
      "# 1 - ICA002:   9.32%\n",
      "# 2 - ICA016:   8.50%\n",
      "# 3 - ICA011:   8.30%\n",
      "# 4 - ICA014:   7.54%\n",
      "# 5 - ICA010:   6.76%\n",
      "# 6 - ICA018:   6.56%\n",
      "# 7 - ICA008:   6.36%\n",
      "# 8 - ICA015:   5.67%\n",
      "# 9 - ICA017:   5.46%\n",
      "#10 - ICA006:   4.98%\n",
      "#11 - ICA000:   4.95%\n",
      "#12 - ICA005:   4.62%\n",
      "#13 - ICA009:   4.62%\n",
      "#14 - ICA012:   4.59%\n",
      "#15 - ICA013:   4.33%\n",
      "#16 - ICA004:   3.50%\n",
      "#17 - ICA001:   2.75%\n",
      "#18 - ICA007:   0.96%\n",
      "#19 - ICA003:   0.23%\n",
      "--------------------------------------------------\n",
      "Total: 100.00%\n"
     ]
    }
   ],
   "source": [
    "ICA_summary_pdf(raw_filtered_highpass, ica_highpass, \"ICA_report_highpass.pdf\")\n",
    "ICA_summary_pdf(raw_filtered_both, ica_both, \"ICA_report_both.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e35962",
   "metadata": {},
   "source": [
    "## 11. Excluding components & visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8802a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ica_rejection_report(ica_highpass, raw = raw_filtered_highpass, exclude_components=[2], output_filename=\"Rejection_report_highpass_cons\")\n",
    "generate_ica_rejection_report(ica_highpass, raw = raw_filtered_highpass, exclude_components=[2,9,11], output_filename=\"Rejection_report_highpass_free\")\n",
    "generate_ica_rejection_report(ica_both, raw = raw_filtered_both, exclude_components=[2,9,11], output_filename=\"Rejection_report_both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891cb8d6",
   "metadata": {},
   "source": [
    "## 12. Saving the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dfa7e2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 5 contiguous segments\n",
      "Setting up low-pass filter at 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 67 samples (0.335 s)\n",
      "\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (19 components)\n",
      "    Zeroing out 1 ICA component\n",
      "    Projecting back using 19 PCA components\n",
      "Overwriting existing file.\n",
      "Writing d:\\Documents\\Studies\\Neuroscience\\Project\\Mem-Proj\\Data_cleaned\\Cleaned_Highpass.fif\n",
      "Overwriting existing file.\n",
      "Closing d:\\Documents\\Studies\\Neuroscience\\Project\\Mem-Proj\\Data_cleaned\\Cleaned_Highpass.fif\n",
      "[done]\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (19 components)\n",
      "    Zeroing out 3 ICA components\n",
      "    Projecting back using 19 PCA components\n",
      "Writing d:\\Documents\\Studies\\Neuroscience\\Project\\Mem-Proj\\Data_cleaned\\Cleaned_Both.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RSKALA\\AppData\\Local\\Temp\\ipykernel_9880\\1848841807.py:4: RuntimeWarning: This filename (d:\\Documents\\Studies\\Neuroscience\\Project\\Mem-Proj\\Data_cleaned\\Cleaned_Highpass.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  reconst_raw_filtered_highpass.save(fname = \"Data_cleaned/Cleaned_Highpass.fif\", overwrite = True)\n",
      "C:\\Users\\RSKALA\\AppData\\Local\\Temp\\ipykernel_9880\\1848841807.py:9: RuntimeWarning: This filename (d:\\Documents\\Studies\\Neuroscience\\Project\\Mem-Proj\\Data_cleaned\\Cleaned_Both.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  reconst_raw_filtered_both.save(fname = \"Data_cleaned/Cleaned_Both.fif\",overwrite = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing d:\\Documents\\Studies\\Neuroscience\\Project\\Mem-Proj\\Data_cleaned\\Cleaned_Both.fif\n",
      "[done]\n"
     ]
    }
   ],
   "source": [
    "ica_highpass.exclude = [2]\n",
    "reconst_raw_filtered_highpass = raw_filtered_highpass.copy().filter(l_freq = None, h_freq = 40)\n",
    "ica_highpass.apply(reconst_raw_filtered_highpass)\n",
    "reconst_raw_filtered_highpass.save(fname = \"Data_cleaned/Cleaned_Highpass.fif\", overwrite = True)\n",
    "########\n",
    "ica_both.exclude = [2,9,11]\n",
    "reconst_raw_filtered_both = raw_filtered_both.copy()\n",
    "ica_both.apply(reconst_raw_filtered_both)\n",
    "reconst_raw_filtered_both.save(fname = \"Data_cleaned/Cleaned_Both.fif\",overwrite = True)\n",
    "########\n",
    "metadata.to_csv(\"Data_cleaned/Metadata.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
