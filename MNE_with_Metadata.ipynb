{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# MNE Data Analysis Pipeline with Metadata\n",
    "\n",
    "This notebook provides tools for:\n",
    "- Loading FIF files and their corresponding CSV metadata\n",
    "- Concatenating multiple sessions from a subject\n",
    "- Aligning neural data with behavioral metadata\n",
    "- Filtering and preprocessing\n",
    "- Creating epochs with metadata for trial selection\n",
    "- ERP analysis by condition (accuracy, set size, etc.)\n",
    "- Reaction time analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels marked as bad:\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats as scipy_stats\n",
    "import glob\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# For interactive plotting\n",
    "matplotlib.use(\"Qt5Agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata_functions_header",
   "metadata": {},
   "source": [
    "## 2. Metadata Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_metadata_single",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_session_metadata(csv_path):\n",
    "    \"\"\"\n",
    "    Load metadata from a single CSV file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : str\n",
    "        Path to CSV metadata file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    metadata : pd.DataFrame\n",
    "        Trial metadata with columns: trial_number, set_size, match, correct, \n",
    "        response, response_time, probe_letter\n",
    "    \"\"\"\n",
    "    metadata = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convert to appropriate types\n",
    "    numeric_cols = ['trial_number', 'set_size', 'match', 'correct', 'response', 'response_time']\n",
    "    for col in numeric_cols:\n",
    "        if col in metadata.columns:\n",
    "            metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "    \n",
    "    print(f\"Loaded metadata: {len(metadata)} trials\")\n",
    "    print(f\"  Columns: {list(metadata.columns)}\")\n",
    "    \n",
    "    # Show summary\n",
    "    if 'correct' in metadata.columns:\n",
    "        acc = metadata['correct'].mean() * 100\n",
    "        print(f\"  Accuracy: {acc:.1f}%\")\n",
    "    if 'response_time' in metadata.columns:\n",
    "        rt = metadata['response_time'].mean()\n",
    "        print(f\"  Mean RT: {rt:.3f}s\")\n",
    "    if 'set_size' in metadata.columns:\n",
    "        sizes = sorted(metadata['set_size'].dropna().unique())\n",
    "        print(f\"  Set sizes: {sizes}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def load_subject_metadata(subject_dir, subject_id=None, pattern='*metadata*.csv'):\n",
    "    \"\"\"\n",
    "    Load and concatenate metadata from all sessions for a subject.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_dir : str\n",
    "        Directory containing subject's session files\n",
    "    subject_id : str, optional\n",
    "        Subject identifier to add as a column\n",
    "    pattern : str\n",
    "        Glob pattern to match metadata CSV files (default: '*metadata*.csv')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    metadata_all : pd.DataFrame\n",
    "        Concatenated metadata from all sessions with added 'session' column\n",
    "    \"\"\"\n",
    "    csv_files = sorted(glob.glob(os.path.join(subject_dir, pattern)))\n",
    "    \n",
    "    if len(csv_files) == 0:\n",
    "        raise ValueError(f\"No metadata CSV files found in {subject_dir}\")\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} metadata files:\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  {os.path.basename(f)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    metadata_list = []\n",
    "    \n",
    "    for session_idx, csv_file in enumerate(csv_files, start=1):\n",
    "        print(f\"\\nLoading session {session_idx}: {os.path.basename(csv_file)}\")\n",
    "        \n",
    "        metadata = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Add session identifier\n",
    "        metadata['session'] = session_idx\n",
    "        metadata['session_file'] = os.path.basename(csv_file)\n",
    "        \n",
    "        if subject_id is not None:\n",
    "            metadata['subject'] = subject_id\n",
    "        \n",
    "        # Convert types\n",
    "        numeric_cols = ['trial_number', 'set_size', 'match', 'correct', 'response', 'response_time']\n",
    "        for col in numeric_cols:\n",
    "            if col in metadata.columns:\n",
    "                metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "        \n",
    "        print(f\"  Trials: {len(metadata)}\")\n",
    "        if 'correct' in metadata.columns:\n",
    "            acc = metadata['correct'].mean() * 100\n",
    "            print(f\"  Accuracy: {acc:.1f}%\")\n",
    "        \n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    # Concatenate all sessions\n",
    "    metadata_all = pd.concat(metadata_list, ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"\\nCombined metadata:\")\n",
    "    print(f\"  Total trials: {len(metadata_all)}\")\n",
    "    print(f\"  Sessions: {metadata_all['session'].nunique()}\")\n",
    "    \n",
    "    if 'correct' in metadata_all.columns:\n",
    "        overall_acc = metadata_all['correct'].mean() * 100\n",
    "        print(f\"  Overall accuracy: {overall_acc:.1f}%\")\n",
    "        \n",
    "        # Per-session accuracy\n",
    "        print(f\"\\n  Per-session accuracy:\")\n",
    "        for session in sorted(metadata_all['session'].unique()):\n",
    "            session_data = metadata_all[metadata_all['session'] == session]\n",
    "            acc = session_data['correct'].mean() * 100\n",
    "            print(f\"    Session {session}: {acc:.1f}%\")\n",
    "    \n",
    "    if 'response_time' in metadata_all.columns:\n",
    "        overall_rt = metadata_all['response_time'].mean()\n",
    "        print(f\"\\n  Overall mean RT: {overall_rt:.3f}s\")\n",
    "    \n",
    "    return metadata_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concat_functions_header",
   "metadata": {},
   "source": [
    "## 3. Multi-Session Concatenation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "concat_with_metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concatenate_subject(\n",
    "    subject_dir: str,\n",
    "    use_common_channels: bool = True,\n",
    "    preload: bool = True,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[mne.io.Raw, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and concatenate all FIF files and metadata for a subject.\n",
    "    \n",
    "    Automatically finds all .fif and .csv files in the directory.\n",
    "    Matches them by sorting alphabetically.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_dir : str\n",
    "        Directory containing subject's session files\n",
    "    use_common_channels : bool\n",
    "        If True, only keep channels common to all files\n",
    "    preload : bool\n",
    "        Whether to load data into memory\n",
    "    verbose : bool\n",
    "        Verbose output\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    raw_concat : mne.io.Raw\n",
    "        Concatenated Raw object from all sessions\n",
    "    metadata_all : pd.DataFrame\n",
    "        Concatenated metadata from all sessions with 'session' column\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> raw, metadata = load_and_concatenate_subject('data/Subject_01/')\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"LOADING SUBJECT DATA\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDirectory: {subject_dir}\")\n",
    "    \n",
    "    # Get all files in directory\n",
    "    all_files = os.listdir(subject_dir)\n",
    "    \n",
    "    # Find FIF files\n",
    "    fif_files = sorted([f for f in all_files if f.endswith('.fif')])\n",
    "    \n",
    "    # Find CSV files (look for files with .csv extension)\n",
    "    csv_files = sorted([f for f in all_files if f.endswith('.csv')])\n",
    "    \n",
    "    # Verify we found files\n",
    "    if len(fif_files) == 0:\n",
    "        raise ValueError(f\"No .fif files found in {subject_dir}\")\n",
    "    \n",
    "    if len(csv_files) == 0:\n",
    "        raise ValueError(f\"No .csv files found in {subject_dir}\")\n",
    "    \n",
    "    print(f\"\\n[1/2] Loading FIF files...\")\n",
    "    print(f\"Found {len(fif_files)} FIF files:\")\n",
    "    for f in fif_files:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    # Load FIF files\n",
    "    raw_list = []\n",
    "    \n",
    "    for fif_file in fif_files:\n",
    "        full_path = os.path.join(subject_dir, fif_file)\n",
    "        raw = mne.io.read_raw_fif(full_path, preload=False, verbose=verbose)\n",
    "        raw_list.append(raw)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Loaded: {fif_file} ({len(raw.ch_names)} channels)\")\n",
    "    \n",
    "    # Find and use common channels\n",
    "    if use_common_channels and len(raw_list) > 1:\n",
    "        common_channels = set(raw_list[0].ch_names)\n",
    "        for raw in raw_list[1:]:\n",
    "            common_channels &= set(raw.ch_names)\n",
    "        \n",
    "        common_channels = sorted(list(common_channels))\n",
    "        print(f\"\\nUsing {len(common_channels)} common channels\")\n",
    "        \n",
    "        # Pick common channels from all files\n",
    "        for i, raw in enumerate(raw_list):\n",
    "            raw_list[i] = raw.copy().pick_channels(common_channels, ordered=True)\n",
    "    \n",
    "    # Concatenate FIF files\n",
    "    print(\"\\nConcatenating FIF files...\")\n",
    "    raw_concat = mne.concatenate_raws(raw_list, preload=preload, verbose=verbose)\n",
    "    \n",
    "    print(f\"✓ Neural data concatenated:\")\n",
    "    print(f\"  Duration: {raw_concat.times[-1]:.2f}s\")\n",
    "    print(f\"  Channels: {len(raw_concat.ch_names)}\")\n",
    "    print(f\"  Sampling rate: {raw_concat.info['sfreq']} Hz\")\n",
    "    \n",
    "    # Load metadata\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[2/2] Loading metadata...\")\n",
    "    print(f\"Found {len(csv_files)} CSV files:\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    metadata_list = []\n",
    "    \n",
    "    for session_idx, csv_file in enumerate(csv_files, start=1):\n",
    "        full_path = os.path.join(subject_dir, csv_file)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n  Loading session {session_idx}: {csv_file}\")\n",
    "        \n",
    "        metadata = pd.read_csv(full_path)\n",
    "        \n",
    "        # Add session identifier\n",
    "        metadata['session'] = session_idx\n",
    "        metadata['session_file'] = csv_file\n",
    "        \n",
    "        # Convert to numeric types\n",
    "        numeric_cols = ['trial_number', 'set_size', 'match', 'correct', \n",
    "                       'response', 'response_time']\n",
    "        for col in numeric_cols:\n",
    "            if col in metadata.columns:\n",
    "                metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"    Trials: {len(metadata)}\")\n",
    "            if 'correct' in metadata.columns:\n",
    "                acc = metadata['correct'].mean() * 100\n",
    "                print(f\"    Accuracy: {acc:.1f}%\")\n",
    "        \n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    # Concatenate all metadata\n",
    "    metadata_all = pd.concat(metadata_list, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n✓ Metadata concatenated:\")\n",
    "    print(f\"  Total trials: {len(metadata_all)}\")\n",
    "    print(f\"  Sessions: {metadata_all['session'].nunique()}\")\n",
    "    \n",
    "    if 'correct' in metadata_all.columns:\n",
    "        overall_acc = metadata_all['correct'].mean() * 100\n",
    "        print(f\"  Overall accuracy: {overall_acc:.1f}%\")\n",
    "    \n",
    "    if 'response_time' in metadata_all.columns:\n",
    "        overall_rt = metadata_all['response_time'].mean()\n",
    "        print(f\"  Overall mean RT: {overall_rt:.3f}s\")\n",
    "    \n",
    "    # Verify alignment\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    n_sessions_fif = len(fif_files)\n",
    "    n_sessions_meta = metadata_all['session'].nunique()\n",
    "    \n",
    "    if n_sessions_fif == n_sessions_meta:\n",
    "        print(f\"✓ Session count matches: {n_sessions_fif} sessions\")\n",
    "    else:\n",
    "        print(f\"⚠ WARNING: Session count mismatch!\")\n",
    "        print(f\"  FIF files: {n_sessions_fif}\")\n",
    "        print(f\"  CSV files: {n_sessions_meta}\")\n",
    "    \n",
    "    print(\"\\n✓ Loading complete!\")\n",
    "    \n",
    "    return raw_concat, metadata_all\n",
    "\n",
    "\n",
    "def load_and_concatenate_subject_paired(\n",
    "    subject_dir: str,\n",
    "    fif_prefix: Optional[str] = None,\n",
    "    csv_suffix: Optional[str] = None,\n",
    "    use_common_channels: bool = True,\n",
    "    preload: bool = True,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[mne.io.Raw, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and concatenate with smart FIF-CSV pairing.\n",
    "    \n",
    "    Pairs files based on shared naming (e.g., Session_01_raw.fif with Session_01_metadata.csv).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_dir : str\n",
    "        Directory containing subject's session files\n",
    "    fif_prefix : str, optional\n",
    "        Only load FIF files starting with this prefix\n",
    "    csv_suffix : str, optional\n",
    "        Only load CSV files with this suffix (e.g., 'metadata')\n",
    "    use_common_channels : bool\n",
    "        If True, only keep channels common to all files\n",
    "    preload : bool\n",
    "        Whether to load data into memory\n",
    "    verbose : bool\n",
    "        Verbose output\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    raw_concat : mne.io.Raw\n",
    "        Concatenated Raw object\n",
    "    metadata_all : pd.DataFrame\n",
    "        Concatenated metadata\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Load only files with specific naming\n",
    "    >>> raw, meta = load_and_concatenate_subject_paired(\n",
    "    ...     'data/Subject_01/',\n",
    "    ...     csv_suffix='metadata'\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"LOADING SUBJECT DATA (PAIRED MODE)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDirectory: {subject_dir}\")\n",
    "    \n",
    "    # Get all files\n",
    "    all_files = os.listdir(subject_dir)\n",
    "    \n",
    "    # Find FIF files\n",
    "    fif_files = [f for f in all_files if f.endswith('.fif')]\n",
    "    if fif_prefix:\n",
    "        fif_files = [f for f in fif_files if f.startswith(fif_prefix)]\n",
    "    fif_files = sorted(fif_files)\n",
    "    \n",
    "    # Find CSV files\n",
    "    csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "    if csv_suffix:\n",
    "        csv_files = [f for f in csv_files if csv_suffix in f.lower()]\n",
    "    csv_files = sorted(csv_files)\n",
    "    \n",
    "    # Verify\n",
    "    if len(fif_files) == 0:\n",
    "        raise ValueError(f\"No FIF files found in {subject_dir}\")\n",
    "    if len(csv_files) == 0:\n",
    "        raise ValueError(f\"No CSV files found in {subject_dir}\")\n",
    "    \n",
    "    print(f\"\\n[1/2] Loading {len(fif_files)} FIF files...\")\n",
    "    for f in fif_files:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    # Try to pair FIF with CSV files\n",
    "    paired_files = []\n",
    "    \n",
    "    for fif_file in fif_files:\n",
    "        # Extract base name (remove extension and common suffixes)\n",
    "        base_name = fif_file.replace('.fif', '').replace('_raw', '').replace('_eeg', '')\n",
    "        \n",
    "        # Look for matching CSV\n",
    "        matching_csv = None\n",
    "        for csv_file in csv_files:\n",
    "            csv_base = csv_file.replace('.csv', '').replace('_metadata', '')\n",
    "            if base_name in csv_base or csv_base in base_name:\n",
    "                matching_csv = csv_file\n",
    "                break\n",
    "        \n",
    "        if matching_csv:\n",
    "            paired_files.append((fif_file, matching_csv))\n",
    "            if verbose:\n",
    "                print(f\"  Paired: {fif_file} ↔ {matching_csv}\")\n",
    "        else:\n",
    "            # No match found, still use this FIF but warn\n",
    "            paired_files.append((fif_file, None))\n",
    "            print(f\"  ⚠ No matching CSV for: {fif_file}\")\n",
    "    \n",
    "    # Load FIF files\n",
    "    raw_list = []\n",
    "    for fif_file, _ in paired_files:\n",
    "        full_path = os.path.join(subject_dir, fif_file)\n",
    "        raw = mne.io.read_raw_fif(full_path, preload=False, verbose=verbose)\n",
    "        raw_list.append(raw)\n",
    "    \n",
    "    # Common channels\n",
    "    if use_common_channels and len(raw_list) > 1:\n",
    "        common_channels = set(raw_list[0].ch_names)\n",
    "        for raw in raw_list[1:]:\n",
    "            common_channels &= set(raw.ch_names)\n",
    "        common_channels = sorted(list(common_channels))\n",
    "        print(f\"\\nUsing {len(common_channels)} common channels\")\n",
    "        \n",
    "        for i, raw in enumerate(raw_list):\n",
    "            raw_list[i] = raw.copy().pick_channels(common_channels, ordered=True)\n",
    "    \n",
    "    # Concatenate\n",
    "    raw_concat = mne.concatenate_raws(raw_list, preload=preload, verbose=verbose)\n",
    "    print(f\"\\n✓ Neural data: {raw_concat.times[-1]:.2f}s, {len(raw_concat.ch_names)} channels\")\n",
    "    \n",
    "    # Load metadata\n",
    "    print(\"\\n[2/2] Loading metadata...\")\n",
    "    metadata_list = []\n",
    "    \n",
    "    for session_idx, (fif_file, csv_file) in enumerate(paired_files, start=1):\n",
    "        if csv_file is None:\n",
    "            print(f\"  ⚠ Skipping session {session_idx} (no CSV)\")\n",
    "            continue\n",
    "        \n",
    "        full_path = os.path.join(subject_dir, csv_file)\n",
    "        metadata = pd.read_csv(full_path)\n",
    "        metadata['session'] = session_idx\n",
    "        metadata['session_file'] = csv_file\n",
    "        metadata['fif_file'] = fif_file\n",
    "        \n",
    "        # Convert types\n",
    "        numeric_cols = ['trial_number', 'set_size', 'match', 'correct', \n",
    "                       'response', 'response_time']\n",
    "        for col in numeric_cols:\n",
    "            if col in metadata.columns:\n",
    "                metadata[col] = pd.to_numeric(metadata[col], errors='coerce')\n",
    "        \n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    if len(metadata_list) == 0:\n",
    "        raise ValueError(\"No metadata files could be loaded\")\n",
    "    \n",
    "    metadata_all = pd.concat(metadata_list, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n✓ Metadata: {len(metadata_all)} trials from {len(metadata_list)} sessions\")\n",
    "    \n",
    "    return raw_concat, metadata_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "epoch_with_metadata_header",
   "metadata": {},
   "source": [
    "## 4. Epochs with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "create_epochs_with_metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epochs_with_metadata(\n",
    "    raw,\n",
    "    metadata,\n",
    "    event_id,\n",
    "    tmin=-0.2,\n",
    "    tmax=0.8,\n",
    "    baseline=(None, 0),\n",
    "    event_name='retrieval',\n",
    "    reject=None,\n",
    "    preload=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Create epochs with attached metadata for advanced trial selection.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : mne.io.Raw\n",
    "        Raw data object\n",
    "    metadata : pd.DataFrame\n",
    "        Trial metadata DataFrame\n",
    "    event_id : dict\n",
    "        Event ID dictionary\n",
    "    tmin, tmax : float\n",
    "        Epoch time window\n",
    "    baseline : tuple\n",
    "        Baseline correction window\n",
    "    event_name : str\n",
    "        Which event to epoch around (default: 'retrieval')\n",
    "    reject : dict, optional\n",
    "        Rejection criteria\n",
    "    preload : bool\n",
    "        Load data into memory\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    epochs : mne.Epochs\n",
    "        Epochs object with metadata attached\n",
    "    \"\"\"\n",
    "    # Validate metadata\n",
    "    if 'response_time' in metadata.columns:\n",
    "        valid_rt = metadata['response_time'].notna()\n",
    "        if not valid_rt.all():\n",
    "            print(f\"⚠ Warning: {(~valid_rt).sum()} trials have missing RT\")\n",
    "        \n",
    "        rt_values = metadata.loc[valid_rt, 'response_time']\n",
    "        print(f\"RT stats: mean={rt_values.mean():.3f}s, \"\n",
    "              f\"range=[{rt_values.min():.3f}, {rt_values.max():.3f}]s\")\n",
    "    \n",
    "    # Extract ALL events\n",
    "    events, event_dict = mne.events_from_annotations(raw)\n",
    "    \n",
    "    # Filter to only the events we want to epoch around\n",
    "    target_event_code = event_id[event_name]\n",
    "    event_mask = events[:, 2] == target_event_code\n",
    "    filtered_events = events[event_mask]\n",
    "    \n",
    "    # Verify we have the right number of trials\n",
    "    n_events = len(filtered_events)\n",
    "    n_metadata = len(metadata)\n",
    "    \n",
    "    print(f\"\\nFound {n_events} '{event_name}' events (out of {len(events)} total events)\")\n",
    "    print(f\"Have {n_metadata} metadata rows\")\n",
    "    \n",
    "    if n_events != n_metadata:\n",
    "        print(f\"\\n⚠ WARNING: Event count mismatch!\")\n",
    "        min_trials = min(n_events, n_metadata)\n",
    "        print(f\"  Using minimum: {min_trials} trials\")\n",
    "        \n",
    "        # Trim both to match\n",
    "        filtered_events = filtered_events[:min_trials]\n",
    "        metadata = metadata.iloc[:min_trials].copy()\n",
    "        \n",
    "        print(f\"  Trimmed to {len(filtered_events)} events and {len(metadata)} metadata rows\")\n",
    "    \n",
    "    # Create epochs with FILTERED events\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        filtered_events,  # <--- CHANGED: Use filtered_events, not events\n",
    "        event_id={event_name: target_event_code},\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=baseline,\n",
    "        metadata=metadata,\n",
    "        reject=reject,\n",
    "        preload=preload,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Created {len(epochs)} epochs with metadata\")\n",
    "    print(f\"  Time window: {tmin} to {tmax}s\")\n",
    "    print(f\"  Baseline: {baseline}\")\n",
    "    \n",
    "    # Show metadata columns available for selection\n",
    "    if metadata is not None and len(metadata.columns) > 0:\n",
    "        print(f\"  Available metadata: {list(metadata.columns)}\")\n",
    "    \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage_example_header",
   "metadata": {},
   "source": [
    "## 5. Usage Example: Load Subject Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load_subject_example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING SUBJECT DATA\n",
      "================================================================================\n",
      "\n",
      "Directory: Data_converted_MetaData\\Subject_01\n",
      "\n",
      "[1/2] Loading FIF files...\n",
      "Found 4 FIF files:\n",
      "  Data_Subject_01_Session_01.h5_seeg_raw.fif\n",
      "  Data_Subject_01_Session_02.h5_seeg_raw.fif\n",
      "  Data_Subject_01_Session_03.h5_seeg_raw.fif\n",
      "  Data_Subject_01_Session_04.h5_seeg_raw.fif\n",
      "Opening raw data file Data_converted_MetaData\\Subject_01\\Data_Subject_01_Session_01.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 79999 =      0.000 ...   399.995 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\RSKALA\\AppData\\Local\\Temp\\ipykernel_24584\\3742830480.py:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  subject_dir = 'Data_converted_MetaData\\Subject_01'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded: Data_Subject_01_Session_01.h5_seeg_raw.fif (20 channels)\n",
      "Opening raw data file Data_converted_MetaData\\Subject_01\\Data_Subject_01_Session_02.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 79999 =      0.000 ...   399.995 secs\n",
      "Ready.\n",
      "  Loaded: Data_Subject_01_Session_02.h5_seeg_raw.fif (20 channels)\n",
      "Opening raw data file Data_converted_MetaData\\Subject_01\\Data_Subject_01_Session_03.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 79999 =      0.000 ...   399.995 secs\n",
      "Ready.\n",
      "  Loaded: Data_Subject_01_Session_03.h5_seeg_raw.fif (20 channels)\n",
      "Opening raw data file Data_converted_MetaData\\Subject_01\\Data_Subject_01_Session_04.h5_seeg_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 79999 =      0.000 ...   399.995 secs\n",
      "Ready.\n",
      "  Loaded: Data_Subject_01_Session_04.h5_seeg_raw.fif (20 channels)\n",
      "\n",
      "Using 20 common channels\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "\n",
      "Concatenating FIF files...\n",
      "✓ Neural data concatenated:\n",
      "  Duration: 1599.99s\n",
      "  Channels: 20\n",
      "  Sampling rate: 200.0 Hz\n",
      "\n",
      "================================================================================\n",
      "[2/2] Loading metadata...\n",
      "Found 4 CSV files:\n",
      "  Data_Subject_01_Session_01.h5_seeg_raw.csv\n",
      "  Data_Subject_01_Session_02.h5_seeg_raw.csv\n",
      "  Data_Subject_01_Session_03.h5_seeg_raw.csv\n",
      "  Data_Subject_01_Session_04.h5_seeg_raw.csv\n",
      "\n",
      "  Loading session 1: Data_Subject_01_Session_01.h5_seeg_raw.csv\n",
      "    Trials: 50\n",
      "    Accuracy: 88.0%\n",
      "\n",
      "  Loading session 2: Data_Subject_01_Session_02.h5_seeg_raw.csv\n",
      "    Trials: 50\n",
      "    Accuracy: 94.0%\n",
      "\n",
      "  Loading session 3: Data_Subject_01_Session_03.h5_seeg_raw.csv\n",
      "    Trials: 50\n",
      "    Accuracy: 94.0%\n",
      "\n",
      "  Loading session 4: Data_Subject_01_Session_04.h5_seeg_raw.csv\n",
      "    Trials: 50\n",
      "    Accuracy: 94.0%\n",
      "\n",
      "✓ Metadata concatenated:\n",
      "  Total trials: 200\n",
      "  Sessions: 4\n",
      "  Overall accuracy: 92.5%\n",
      "  Overall mean RT: 1.305s\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION\n",
      "================================================================================\n",
      "✓ Session count matches: 4 sessions\n",
      "\n",
      "✓ Loading complete!\n",
      "\n",
      "================================================================================\n",
      "SUCCESS!\n",
      "================================================================================\n",
      "Loaded 20 channels, 1600.0s\n",
      "Loaded 200 trials\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Example 1: Simple loading (all .fif and .csv files)\n",
    "    subject_dir = 'Data_converted_MetaData\\Subject_01'\n",
    "    \n",
    "    try:\n",
    "        raw, metadata = load_and_concatenate_subject(\n",
    "            subject_dir=subject_dir,\n",
    "            use_common_channels=True,\n",
    "            preload=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUCCESS!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Loaded {len(raw.ch_names)} channels, {raw.times[-1]:.1f}s\")\n",
    "        print(f\"Loaded {len(metadata)} trials\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nDirectory not found: {e}\")\n",
    "        print(\"Please update subject_dir to point to your data\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "inspect_metadata",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata columns:\n",
      "['trial_number', 'set_size', 'match', 'correct', 'response', 'response_time', 'probe_letter', 'session', 'session_file']\n",
      "\n",
      "First few trials:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_number</th>\n",
       "      <th>set_size</th>\n",
       "      <th>match</th>\n",
       "      <th>correct</th>\n",
       "      <th>response</th>\n",
       "      <th>response_time</th>\n",
       "      <th>probe_letter</th>\n",
       "      <th>session</th>\n",
       "      <th>session_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2.48400</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>Data_Subject_01_Session_01.h5_seeg_raw.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.66775</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>Data_Subject_01_Session_01.h5_seeg_raw.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.47200</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>Data_Subject_01_Session_01.h5_seeg_raw.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.30875</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>Data_Subject_01_Session_01.h5_seeg_raw.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.51625</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>Data_Subject_01_Session_01.h5_seeg_raw.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.89900</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>Data_Subject_01_Session_01.h5_seeg_raw.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.13200</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>Data_Subject_01_Session_01.h5_seeg_raw.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.58025</td>\n",
       "      <td>Z</td>\n",
       "      <td>1</td>\n",
       "      <td>Data_Subject_01_Session_01.h5_seeg_raw.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.88450</td>\n",
       "      <td>Z</td>\n",
       "      <td>1</td>\n",
       "      <td>Data_Subject_01_Session_01.h5_seeg_raw.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.94300</td>\n",
       "      <td>V</td>\n",
       "      <td>1</td>\n",
       "      <td>Data_Subject_01_Session_01.h5_seeg_raw.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial_number  set_size  match  correct  response  response_time  \\\n",
       "0           1.0       8.0    2.0      1.0      52.0        2.48400   \n",
       "1           2.0       4.0    1.0      1.0      51.0        1.66775   \n",
       "2           3.0       8.0    2.0      1.0      52.0        1.47200   \n",
       "3           4.0       6.0    2.0      1.0      52.0        1.30875   \n",
       "4           5.0       8.0    2.0      1.0      52.0        1.51625   \n",
       "5           6.0       4.0    1.0      1.0      51.0        0.89900   \n",
       "6           7.0       8.0    2.0      1.0      52.0        1.13200   \n",
       "7           8.0       8.0    1.0      1.0      51.0        1.58025   \n",
       "8           9.0       4.0    2.0      1.0      52.0        0.88450   \n",
       "9          10.0       6.0    2.0      1.0      52.0        0.94300   \n",
       "\n",
       "  probe_letter  session                                session_file  \n",
       "0            H        1  Data_Subject_01_Session_01.h5_seeg_raw.csv  \n",
       "1            T        1  Data_Subject_01_Session_01.h5_seeg_raw.csv  \n",
       "2            L        1  Data_Subject_01_Session_01.h5_seeg_raw.csv  \n",
       "3            G        1  Data_Subject_01_Session_01.h5_seeg_raw.csv  \n",
       "4            S        1  Data_Subject_01_Session_01.h5_seeg_raw.csv  \n",
       "5            R        1  Data_Subject_01_Session_01.h5_seeg_raw.csv  \n",
       "6            N        1  Data_Subject_01_Session_01.h5_seeg_raw.csv  \n",
       "7            Z        1  Data_Subject_01_Session_01.h5_seeg_raw.csv  \n",
       "8            Z        1  Data_Subject_01_Session_01.h5_seeg_raw.csv  \n",
       "9            V        1  Data_Subject_01_Session_01.h5_seeg_raw.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_number</th>\n",
       "      <th>set_size</th>\n",
       "      <th>match</th>\n",
       "      <th>correct</th>\n",
       "      <th>response</th>\n",
       "      <th>response_time</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>25.500000</td>\n",
       "      <td>5.970000</td>\n",
       "      <td>1.510000</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>47.775000</td>\n",
       "      <td>1.304856</td>\n",
       "      <td>2.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.467083</td>\n",
       "      <td>1.683231</td>\n",
       "      <td>0.501154</td>\n",
       "      <td>0.264052</td>\n",
       "      <td>13.190692</td>\n",
       "      <td>0.669556</td>\n",
       "      <td>1.12084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591375</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.918281</td>\n",
       "      <td>1.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>1.089875</td>\n",
       "      <td>2.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.401563</td>\n",
       "      <td>3.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>5.585375</td>\n",
       "      <td>4.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       trial_number    set_size       match     correct    response  \\\n",
       "count    200.000000  200.000000  200.000000  200.000000  200.000000   \n",
       "mean      25.500000    5.970000    1.510000    0.925000   47.775000   \n",
       "std       14.467083    1.683231    0.501154    0.264052   13.190692   \n",
       "min        1.000000    4.000000    1.000000    0.000000    1.000000   \n",
       "25%       13.000000    4.000000    1.000000    1.000000   51.000000   \n",
       "50%       25.500000    6.000000    2.000000    1.000000   51.000000   \n",
       "75%       38.000000    8.000000    2.000000    1.000000   52.000000   \n",
       "max       50.000000    8.000000    2.000000    1.000000   52.000000   \n",
       "\n",
       "       response_time    session  \n",
       "count     200.000000  200.00000  \n",
       "mean        1.304856    2.50000  \n",
       "std         0.669556    1.12084  \n",
       "min         0.591375    1.00000  \n",
       "25%         0.918281    1.75000  \n",
       "50%         1.089875    2.50000  \n",
       "75%         1.401563    3.25000  \n",
       "max         5.585375    4.00000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect metadata\n",
    "print(\"\\nMetadata columns:\")\n",
    "print(metadata.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst few trials:\")\n",
    "display(metadata.head(10))\n",
    "\n",
    "print(\"\\nMetadata summary:\")\n",
    "display(metadata.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filtering_header",
   "metadata": {},
   "source": [
    "## 6. Filtering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "filter_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 4 contiguous segments\n",
      "Setting up band-pass filter from 0.1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.10\n",
      "- Lower transition bandwidth: 0.10 Hz (-6 dB cutoff frequency: 0.05 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 6601 samples (33.005 s)\n",
      "\n",
      "Filtering raw data in 4 contiguous segments\n",
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 661 samples (3.305 s)\n",
      "\n",
      "\n",
      "✓ Applied bandpass filter: 0.1-40 Hz\n"
     ]
    }
   ],
   "source": [
    "# Filter the data\n",
    "raw_filtered = raw.copy().filter(l_freq=0.1, h_freq=40.0, verbose=True)\n",
    "raw_filtered_ica = raw.copy().filter(l_freq=1, h_freq = 40, verbose=True)\n",
    "\n",
    "print(\"\\n✓ Applied bandpass filter: 0.1-40 Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "epochs_header",
   "metadata": {},
   "source": [
    "## 7. Create Epochs with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extract_events",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: [np.str_('encoding'), np.str_('fixation'), np.str_('maintenance'), np.str_('response'), np.str_('retrieval')]\n",
      "Event types:\n",
      "  encoding: 200 events (code 1)\n",
      "  fixation: 200 events (code 2)\n",
      "  maintenance: 200 events (code 3)\n",
      "  response: 200 events (code 4)\n",
      "  retrieval: 200 events (code 5)\n"
     ]
    }
   ],
   "source": [
    "# Extract events and event_id from raw\n",
    "events, event_id = mne.events_from_annotations(raw)\n",
    "\n",
    "print(\"Event types:\")\n",
    "for event_name, event_code in event_id.items():\n",
    "    n_events = len(events[events[:, 2] == event_code])\n",
    "    print(f\"  {event_name}: {n_events} events (code {event_code})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e27b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e3a4b1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m evoked = \u001b[43mepochs\u001b[49m[\u001b[32m0\u001b[39m:\u001b[32m100\u001b[39m].average()\n\u001b[32m      2\u001b[39m evoked.plot()\n",
      "\u001b[31mNameError\u001b[39m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "evoked = epochs[0:100].average()\n",
    "evoked.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_epochs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT stats: mean=1.305s, range=[0.591, 5.585]s\n",
      "Used Annotations descriptions: [np.str_('encoding'), np.str_('fixation'), np.str_('maintenance'), np.str_('response'), np.str_('retrieval')]\n",
      "\n",
      "Found 200 'response' events (out of 1000 total events)\n",
      "Have 200 metadata rows\n",
      "\n",
      "✓ Created 199 epochs with metadata\n",
      "  Time window: -1.02 to 0.98s\n",
      "  Baseline: (None, 0)\n",
      "  Available metadata: ['trial_number', 'set_size', 'match', 'correct', 'response', 'response_time', 'probe_letter', 'session', 'session_file']\n",
      "RT stats: mean=1.305s, range=[0.591, 5.585]s\n",
      "Used Annotations descriptions: [np.str_('encoding'), np.str_('fixation'), np.str_('maintenance'), np.str_('response'), np.str_('retrieval')]\n",
      "\n",
      "Found 200 'retrieval' events (out of 1000 total events)\n",
      "Have 200 metadata rows\n",
      "\n",
      "✓ Created 196 epochs with metadata\n",
      "  Time window: -0.2 to 2s\n",
      "  Baseline: (-0.2, 0)\n",
      "  Available metadata: ['trial_number', 'set_size', 'match', 'correct', 'response', 'response_time', 'probe_letter', 'session', 'session_file']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne_qt_browser._pg_figure.MNEQtBrowser at 0x1ceba592330>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create epochs with metadata\n",
    "# Epoch around response event\n",
    "# response_epochs = create_epochs_with_metadata(\n",
    "#     raw=raw_filtered_ica,\n",
    "#     metadata=metadata,\n",
    "#     event_id= event_id,\n",
    "#     tmin=-1.02,\n",
    "#     tmax=0.98,\n",
    "#     baseline=(None, 0),\n",
    "#     event_name='response',  # Change to your event name\n",
    "#     preload=True\n",
    "# )\n",
    "# response_epochs.plot()\n",
    "\n",
    "# Create epochs with metadata\n",
    "# Epoch around retrieval event\n",
    "# retrieval_epochs = create_epochs_with_metadata(\n",
    "#     raw=raw_filtered_ica,\n",
    "#     metadata=metadata,\n",
    "#     event_id= event_id,\n",
    "#     tmin=-0.2,\n",
    "#     tmax=2,\n",
    "#     baseline=(-0.2, 0),\n",
    "#     event_name='retrieval',  # Change to your event name\n",
    "#     preload=True\n",
    "# )\n",
    "# retrieval_epochs.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "747602ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 19 channels (please be patient, this may take a while)\n",
      "Selecting by explained variance: 15 components\n",
      "Fitting ICA took 19.0s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table mne-repr-table\">\n",
       "    <tr>\n",
       "        <th>Method</th>\n",
       "        <td>picard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit parameters</th>\n",
       "        <td>max_iter=500<br /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit</th>\n",
       "        <td>67 iterations on raw data (320000 samples)</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>ICA components</th>\n",
       "        <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Available PCA components</th>\n",
       "        <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Channel types</th>\n",
       "        <td>eeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ICA components marked for exclusion</th>\n",
       "        <td>&mdash;</td>\n",
       "    </tr>\n",
       "    \n",
       "</table>"
      ],
      "text/plain": [
       "<ICA | raw data decomposition, method: picard (fit in 67 iterations on 320000 samples), 15 ICA components (19 PCA components available), channel types: eeg, no sources marked for exclusion>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 0.999  # Should normally be higher, like 0.999!!\n",
    "method = 'picard'\n",
    "max_iter = 500  # Should normally be higher, like 500 or even 1000!!\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "retrieval_ica = mne.preprocessing.ICA(n_components=n_components,\n",
    "                            method=method,\n",
    "                            max_iter=max_iter,\n",
    "                            random_state=random_state)\n",
    "retrieval_ica.fit(raw_filtered_ica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec4545b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MNEFigure size 2880x1760 with 15 Axes>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_ica.plot_components(inst = raw_filtered_ica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c3a684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 10.240 (s)\n",
      "Plotting power spectral density (dB=True).\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1025 is out of bounds for axis 0 with size 1025",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m raw_firt_h = raw.copy().crop(tmin=\u001b[32m0\u001b[39m, tmax=\u001b[32m800\u001b[39m)\n\u001b[32m     22\u001b[39m fig = raw_firt_h.compute_psd(fmax=\u001b[32m100\u001b[39m).plot(average=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43madd_arrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36madd_arrows\u001b[39m\u001b[34m(axes)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# get ymax of a small region around the freq. of interest\u001b[39;00m\n\u001b[32m      9\u001b[39m y = psds[(idx - \u001b[32m4\u001b[39m) : (idx + \u001b[32m5\u001b[39m)].max()\n\u001b[32m     10\u001b[39m ax.arrow(\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     x=\u001b[43mfreqs\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[32m     12\u001b[39m     y=y + \u001b[32m18\u001b[39m,\n\u001b[32m     13\u001b[39m     dx=\u001b[32m0\u001b[39m,\n\u001b[32m     14\u001b[39m     dy=-\u001b[32m12\u001b[39m,\n\u001b[32m     15\u001b[39m     color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     width=\u001b[32m0.1\u001b[39m,\n\u001b[32m     17\u001b[39m     head_width=\u001b[32m3\u001b[39m,\n\u001b[32m     18\u001b[39m     length_includes_head=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     19\u001b[39m )\n",
      "\u001b[31mIndexError\u001b[39m: index 1025 is out of bounds for axis 0 with size 1025"
     ]
    }
   ],
   "source": [
    "def add_arrows(axes):\n",
    "    # add some arrows at 60 Hz and its harmonics\n",
    "    for ax in axes:\n",
    "        freqs = ax.lines[-1].get_xdata()\n",
    "        psds = ax.lines[-1].get_ydata()\n",
    "        for freq in (60, 120, 180, 240):\n",
    "            idx = np.searchsorted(freqs, freq)\n",
    "            # get ymax of a small region around the freq. of interest\n",
    "            y = psds[(idx - 4) : (idx + 5)].max()\n",
    "            ax.arrow(\n",
    "                x=freqs[idx],\n",
    "                y=y + 18,\n",
    "                dx=0,\n",
    "                dy=-12,\n",
    "                color=\"red\",\n",
    "                width=0.1,\n",
    "                head_width=3,\n",
    "                length_includes_head=True,\n",
    "            )\n",
    "\n",
    "raw_firt_h = raw.copy().crop(tmin=0, tmax=800)\n",
    "fig = raw_firt_h.compute_psd(fmax=100).plot(average=True)\n",
    "add_arrows(fig.axes[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb085cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.apply(raw, exclude=blink_idx + heartbeat_idx)\n",
    "ica.plot_overlay(raw, exclude=muscle_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trial_selection_header",
   "metadata": {},
   "source": [
    "## 8. Trial Selection Using Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "trial_selection_examples",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct trials: 184\n",
      "Incorrect trials: 15\n",
      "High load trials (set size ≥ 6): 128\n",
      "Match trials: 97\n",
      "Correct high load trials: 113\n",
      "Fast responses (RT < 1.087s): 99\n"
     ]
    }
   ],
   "source": [
    "# Now you can select trials using pandas-style queries!\n",
    "\n",
    "# Example 1: Select correct trials only\n",
    "epochs_correct = epochs['correct == 1']\n",
    "print(f\"Correct trials: {len(epochs_correct)}\")\n",
    "\n",
    "# Example 2: Select incorrect trials\n",
    "epochs_incorrect = epochs['correct == 0']\n",
    "print(f\"Incorrect trials: {len(epochs_incorrect)}\")\n",
    "\n",
    "# Example 3: Select high memory load trials (set size >= 6)\n",
    "epochs_high_load = epochs['set_size >= 6']\n",
    "print(f\"High load trials (set size ≥ 6): {len(epochs_high_load)}\")\n",
    "\n",
    "# Example 4: Select match trials\n",
    "epochs_match = epochs['match == 1']\n",
    "print(f\"Match trials: {len(epochs_match)}\")\n",
    "\n",
    "# Example 5: Complex query - correct high load trials\n",
    "epochs_correct_high_load = epochs['correct == 1 and set_size >= 6']\n",
    "print(f\"Correct high load trials: {len(epochs_correct_high_load)}\")\n",
    "\n",
    "# Example 6: Fast responses (RT < median)\n",
    "median_rt = epochs.metadata['response_time'].median()\n",
    "epochs_fast = epochs[f'response_time < {median_rt}']\n",
    "print(f\"Fast responses (RT < {median_rt:.3f}s): {len(epochs_fast)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "erp_by_condition_header",
   "metadata": {},
   "source": [
    "## 9. ERP Analysis by Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "erp_correct_vs_incorrect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combining channels using \"mean\"\n",
      "combining channels using \"mean\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Figure size 800x600 with 1 Axes>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare correct vs incorrect trials\n",
    "evoked_correct = epochs['correct == 1'].average()\n",
    "evoked_incorrect = epochs['correct == 0'].average()\n",
    "\n",
    "# Plot comparison\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    {'Correct': evoked_correct, 'Incorrect': evoked_incorrect},\n",
    "    picks='eeg',\n",
    "    combine='mean'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "erp_by_load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare by memory load (set size)\n",
    "evoked_by_load = {}\n",
    "\n",
    "for set_size in sorted(epochs.metadata['set_size'].dropna().unique()):\n",
    "    epochs_size = epochs[f'set_size == {set_size}']\n",
    "    evoked_by_load[f'Set size {int(set_size)}'] = epochs_size.average()\n",
    "\n",
    "# Plot comparison\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    evoked_by_load,\n",
    "    picks='eeg',\n",
    "    combine='mean'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "erp_match_vs_nonmatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare match vs non-match\n",
    "evoked_match = epochs['match == 1'].average()\n",
    "evoked_nonmatch = epochs['match == 0'].average()\n",
    "\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    {'Match': evoked_match, 'Non-match': evoked_nonmatch},\n",
    "    picks='eeg',\n",
    "    combine='mean'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rt_analysis_header",
   "metadata": {},
   "source": [
    "## 10. Reaction Time Analysis with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rt_summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reaction Time Statistics:\n",
      "  Mean: 1.305s\n",
      "  Median: 1.090s\n",
      "  SD: 0.670s\n",
      "  Min: 0.591s\n",
      "  Max: 5.585s\n"
     ]
    }
   ],
   "source": [
    "# RT summary statistics\n",
    "print(\"Reaction Time Statistics:\")\n",
    "print(f\"  Mean: {metadata['response_time'].mean():.3f}s\")\n",
    "print(f\"  Median: {metadata['response_time'].median():.3f}s\")\n",
    "print(f\"  SD: {metadata['response_time'].std():.3f}s\")\n",
    "print(f\"  Min: {metadata['response_time'].min():.3f}s\")\n",
    "print(f\"  Max: {metadata['response_time'].max():.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rt_by_accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RT by accuracy\n",
    "rt_correct = metadata_all[metadata_all['correct'] == 1]['response_time']\n",
    "rt_incorrect = metadata_all[metadata_all['correct'] == 0]['response_time']\n",
    "\n",
    "print(f\"\\nRT by Accuracy:\")\n",
    "print(f\"  Correct: {rt_correct.mean():.3f}s (SD: {rt_correct.std():.3f}s)\")\n",
    "print(f\"  Incorrect: {rt_incorrect.mean():.3f}s (SD: {rt_incorrect.std():.3f}s)\")\n",
    "\n",
    "# Statistical test\n",
    "from scipy.stats import ttest_ind\n",
    "t_stat, p_value = ttest_ind(rt_correct.dropna(), rt_incorrect.dropna())\n",
    "print(f\"  t-test: t = {t_stat:.3f}, p = {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rt_by_load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RT by memory load\n",
    "print(\"\\nRT by Set Size:\")\n",
    "for set_size in sorted(metadata_all['set_size'].dropna().unique()):\n",
    "    rt_size = metadata_all[metadata_all['set_size'] == set_size]['response_time']\n",
    "    print(f\"  Set size {int(set_size)}: {rt_size.mean():.3f}s (SD: {rt_size.std():.3f}s, n={len(rt_size)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_rt_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RT distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Overall distribution\n",
    "axes[0, 0].hist(metadata_all['response_time'].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Reaction Time (s)')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('Overall RT Distribution')\n",
    "axes[0, 0].axvline(metadata_all['response_time'].mean(), color='red', \n",
    "                    linestyle='--', label='Mean')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. By accuracy\n",
    "axes[0, 1].hist([rt_correct.dropna(), rt_incorrect.dropna()], \n",
    "                bins=30, label=['Correct', 'Incorrect'], alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Reaction Time (s)')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('RT by Accuracy')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. By set size\n",
    "rt_by_size = [metadata_all[metadata_all['set_size'] == size]['response_time'].dropna() \n",
    "              for size in sorted(metadata_all['set_size'].dropna().unique())]\n",
    "axes[1, 0].boxplot(rt_by_size, \n",
    "                   labels=[f'{int(s)}' for s in sorted(metadata_all['set_size'].dropna().unique())])\n",
    "axes[1, 0].set_xlabel('Set Size')\n",
    "axes[1, 0].set_ylabel('Reaction Time (s)')\n",
    "axes[1, 0].set_title('RT by Memory Load')\n",
    "\n",
    "# 4. Over trials\n",
    "axes[1, 1].plot(metadata_all['response_time'], 'o-', alpha=0.5, markersize=3)\n",
    "axes[1, 1].set_xlabel('Trial Number')\n",
    "axes[1, 1].set_ylabel('Reaction Time (s)')\n",
    "axes[1, 1].set_title('RT Over Time')\n",
    "\n",
    "# Add session boundaries\n",
    "if 'session' in metadata_all.columns:\n",
    "    session_boundaries = metadata_all.groupby('session').size().cumsum()[:-1]\n",
    "    for boundary in session_boundaries:\n",
    "        axes[1, 1].axvline(boundary, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accuracy_analysis_header",
   "metadata": {},
   "source": [
    "## 11. Accuracy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accuracy_by_condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy\n",
    "overall_acc = metadata_all['correct'].mean() * 100\n",
    "print(f\"Overall Accuracy: {overall_acc:.1f}%\")\n",
    "\n",
    "# Accuracy by session\n",
    "print(\"\\nAccuracy by Session:\")\n",
    "session_acc = metadata_all.groupby('session')['correct'].agg(['mean', 'count'])\n",
    "session_acc['mean'] *= 100\n",
    "session_acc.columns = ['Accuracy (%)', 'N trials']\n",
    "display(session_acc)\n",
    "\n",
    "# Accuracy by set size\n",
    "print(\"\\nAccuracy by Set Size:\")\n",
    "size_acc = metadata_all.groupby('set_size')['correct'].agg(['mean', 'count'])\n",
    "size_acc['mean'] *= 100\n",
    "size_acc.columns = ['Accuracy (%)', 'N trials']\n",
    "display(size_acc)\n",
    "\n",
    "# Accuracy by match condition\n",
    "print(\"\\nAccuracy by Match Condition:\")\n",
    "match_acc = metadata_all.groupby('match')['correct'].agg(['mean', 'count'])\n",
    "match_acc['mean'] *= 100\n",
    "match_acc.columns = ['Accuracy (%)', 'N trials']\n",
    "match_acc.index = ['Non-match', 'Match']\n",
    "display(match_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# By session\n",
    "session_acc = metadata_all.groupby('session')['correct'].mean() * 100\n",
    "axes[0].bar(session_acc.index, session_acc.values)\n",
    "axes[0].set_xlabel('Session')\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].set_title('Accuracy by Session')\n",
    "axes[0].set_ylim([0, 100])\n",
    "\n",
    "# By set size\n",
    "size_acc = metadata_all.groupby('set_size')['correct'].mean() * 100\n",
    "axes[1].bar(size_acc.index, size_acc.values)\n",
    "axes[1].set_xlabel('Set Size')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Accuracy by Memory Load')\n",
    "axes[1].set_ylim([0, 100])\n",
    "\n",
    "# By match\n",
    "match_acc = metadata_all.groupby('match')['correct'].mean() * 100\n",
    "axes[2].bar(['Non-match', 'Match'], match_acc.values)\n",
    "axes[2].set_ylabel('Accuracy (%)')\n",
    "axes[2].set_title('Accuracy by Match Condition')\n",
    "axes[2].set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_analysis_header",
   "metadata": {},
   "source": [
    "## 12. Advanced: Single-Trial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single_trial_correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Correlate single-trial ERP amplitude with RT\n",
    "# Pick a channel and time window of interest\n",
    "channel = 'Fz'  # Change to your channel\n",
    "time_window = (0.3, 0.5)  # Time window in seconds\n",
    "\n",
    "# Get data for correct trials only\n",
    "epochs_correct = epochs['correct == 1']\n",
    "\n",
    "# Extract single-trial amplitudes\n",
    "data = epochs_correct.get_data(picks=channel)\n",
    "times = epochs_correct.times\n",
    "\n",
    "# Find time indices\n",
    "time_mask = (times >= time_window[0]) & (times <= time_window[1])\n",
    "amplitudes = data[:, 0, time_mask].mean(axis=1)  # Mean amplitude in window\n",
    "\n",
    "# Get corresponding RTs\n",
    "rts = epochs_correct.metadata['response_time'].values\n",
    "\n",
    "# Compute correlation\n",
    "from scipy.stats import pearsonr\n",
    "r, p = pearsonr(amplitudes, rts)\n",
    "\n",
    "print(f\"\\nSingle-Trial Correlation:\")\n",
    "print(f\"  Channel: {channel}\")\n",
    "print(f\"  Time window: {time_window[0]}-{time_window[1]}s\")\n",
    "print(f\"  r = {r:.3f}, p = {p:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(amplitudes * 1e6, rts, alpha=0.5)\n",
    "plt.xlabel(f'{channel} Amplitude ({time_window[0]}-{time_window[1]}s) (µV)')\n",
    "plt.ylabel('Response Time (s)')\n",
    "plt.title(f'Single-Trial ERP-RT Correlation\\nr = {r:.3f}, p = {p:.4f}')\n",
    "\n",
    "# Add regression line\n",
    "from scipy.stats import linregress\n",
    "slope, intercept, _, _, _ = linregress(amplitudes, rts)\n",
    "x_line = np.array([amplitudes.min(), amplitudes.max()])\n",
    "y_line = slope * x_line + intercept\n",
    "plt.plot(x_line * 1e6, y_line, 'r-', linewidth=2, label='Regression line')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_results_header",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_epochs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save epochs with metadata\n",
    "# This preserves both the neural data and behavioral metadata\n",
    "epochs.save('subject_epochs-epo.fif', overwrite=True)\n",
    "\n",
    "print(\"✓ Saved epochs with metadata to 'subject_epochs-epo.fif'\")\n",
    "print(\"  You can reload later with: epochs = mne.read_epochs('subject_epochs-epo.fif')\")\n",
    "print(\"  Metadata will be accessible via: epochs.metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86bb3fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHECKING EVENTS IN FIF FILE\n",
      "================================================================================\n",
      "\n",
      "File: Data_converted_MetaData\\Subject_01\\Data_Subject_01_Session_04.h5_seeg_raw.fif\n",
      "Channels: 20\n",
      "Duration: 400.00s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CHECKING STI CHANNEL\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Found 1 STI channel(s): ['STI']\n",
      "\n",
      "  Channel: STI\n",
      "    Min: 0.0\n",
      "    Max: 0.0\n",
      "    Unique values: [0.]\n",
      "    Non-zero samples: 0\n",
      "    ❌ STI channel is all zeros!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "EXTRACTING EVENTS\n",
      "--------------------------------------------------------------------------------\n",
      "❌ No events found!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CHECKING ANNOTATIONS\n",
      "--------------------------------------------------------------------------------\n",
      "❌ No annotations found!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CHECKING EVENT_ID\n",
      "--------------------------------------------------------------------------------\n",
      "❌ No event_id mapping found in raw.info\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "CHECKING EVENTS IN NIX FILE\n",
      "================================================================================\n",
      "\n",
      "Block: Data_Subject_01_Session_01\n",
      "❌ No Events group\n",
      "   Available groups: ['iEEG data', 'Scalp EEG data', 'Trial events single tags iEEG', 'Trial events single tags scalp EEG', 'Spike times', 'Spike waveforms', 'Trial events single tags spike times', 'Spike times multitags', 'iEEG electrode information', 'Scalp EEG electrode information']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:198: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:198: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\RSKALA\\AppData\\Local\\Temp\\ipykernel_18488\\562404097.py:198: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  fif_file = 'Data_converted_MetaData\\Subject_01\\Data_Subject_01_Session_04.h5_seeg_raw.fif'  # Change to your FIF file\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Diagnostic: Check Event Extraction and STI Channel\n",
    "\"\"\"\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "def check_events_in_fif(fif_path):\n",
    "    \"\"\"Check if events are properly encoded in FIF file.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"CHECKING EVENTS IN FIF FILE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load raw\n",
    "    raw = mne.io.read_raw_fif(fif_path, preload=True, verbose=False)\n",
    "    \n",
    "    print(f\"\\nFile: {fif_path}\")\n",
    "    print(f\"Channels: {len(raw.ch_names)}\")\n",
    "    print(f\"Duration: {raw.times[-1]:.2f}s\")\n",
    "    \n",
    "    # Check for STI channel\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"CHECKING STI CHANNEL\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    stim_channels = [ch for ch in raw.ch_names if 'STI' in ch.upper()]\n",
    "    \n",
    "    if len(stim_channels) == 0:\n",
    "        print(\"❌ NO STI CHANNEL FOUND!\")\n",
    "        print(f\"   Available channels: {raw.ch_names}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"✓ Found {len(stim_channels)} STI channel(s): {stim_channels}\")\n",
    "        \n",
    "        # Check STI data\n",
    "        for stim_ch in stim_channels:\n",
    "            stim_data = raw.get_data(picks=stim_ch)[0]\n",
    "            \n",
    "            print(f\"\\n  Channel: {stim_ch}\")\n",
    "            print(f\"    Min: {stim_data.min()}\")\n",
    "            print(f\"    Max: {stim_data.max()}\")\n",
    "            print(f\"    Unique values: {np.unique(stim_data)}\")\n",
    "            print(f\"    Non-zero samples: {np.sum(stim_data != 0)}\")\n",
    "            \n",
    "            if np.all(stim_data == 0):\n",
    "                print(f\"    ❌ STI channel is all zeros!\")\n",
    "            else:\n",
    "                print(f\"    ✓ STI channel has events\")\n",
    "    \n",
    "    # Try to extract events\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"EXTRACTING EVENTS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    try:\n",
    "        events = mne.find_events(raw, stim_channel='STI', verbose=False)\n",
    "        \n",
    "        if len(events) == 0:\n",
    "            print(\"❌ No events found!\")\n",
    "        else:\n",
    "            print(f\"✓ Found {len(events)} events\")\n",
    "            print(f\"\\n  Event codes: {np.unique(events[:, 2])}\")\n",
    "            print(f\"  Event counts:\")\n",
    "            for code in np.unique(events[:, 2]):\n",
    "                n = np.sum(events[:, 2] == code)\n",
    "                print(f\"    Code {code}: {n} events\")\n",
    "            \n",
    "            print(f\"\\n  First 5 events:\")\n",
    "            print(\"  Sample  | Time (s) | Code\")\n",
    "            print(\"  \" + \"-\"*35)\n",
    "            for i, event in enumerate(events[:5]):\n",
    "                sample, _, code = event\n",
    "                time = sample / raw.info['sfreq']\n",
    "                print(f\"  {sample:6d}  | {time:8.2f} | {code:4d}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting events: {e}\")\n",
    "    \n",
    "    # Check annotations\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"CHECKING ANNOTATIONS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    if len(raw.annotations) == 0:\n",
    "        print(\"❌ No annotations found!\")\n",
    "    else:\n",
    "        print(f\"✓ Found {len(raw.annotations)} annotations\")\n",
    "        print(f\"\\n  Annotation types:\")\n",
    "        unique_desc = np.unique(raw.annotations.description)\n",
    "        for desc in unique_desc:\n",
    "            n = np.sum(raw.annotations.description == desc)\n",
    "            print(f\"    {desc}: {n} annotations\")\n",
    "        \n",
    "        print(f\"\\n  First 5 annotations:\")\n",
    "        print(\"  Time (s) | Description\")\n",
    "        print(\"  \" + \"-\"*40)\n",
    "        for i in range(min(5, len(raw.annotations))):\n",
    "            print(f\"  {raw.annotations.onset[i]:8.2f} | {raw.annotations.description[i]}\")\n",
    "    \n",
    "    # Check event_id in info\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"CHECKING EVENT_ID\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    if 'temp' in raw.info and 'event_id' in raw.info['temp']:\n",
    "        event_id = raw.info['temp']['event_id']\n",
    "        print(f\"✓ Found event_id mapping:\")\n",
    "        for name, code in event_id.items():\n",
    "            print(f\"    {name}: {code}\")\n",
    "    else:\n",
    "        print(\"❌ No event_id mapping found in raw.info\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def check_events_in_nix(nix_path):\n",
    "    \"\"\"Check events in NIX file.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CHECKING EVENTS IN NIX FILE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    with h5py.File(nix_path, 'r') as file:\n",
    "        if 'data' not in file:\n",
    "            print(\"❌ No data section\")\n",
    "            return\n",
    "        \n",
    "        block_name = list(file['data'].keys())[0]\n",
    "        block = file['data'][block_name]\n",
    "        \n",
    "        print(f\"\\nBlock: {block_name}\")\n",
    "        \n",
    "        # Check for Events group\n",
    "        if 'groups' not in block:\n",
    "            print(\"❌ No groups section\")\n",
    "            return\n",
    "        \n",
    "        if 'Events' not in block['groups']:\n",
    "            print(\"❌ No Events group\")\n",
    "            print(f\"   Available groups: {list(block['groups'].keys())}\")\n",
    "            return\n",
    "        \n",
    "        events_group = block['groups']['Events']\n",
    "        print(f\"✓ Found Events group\")\n",
    "        \n",
    "        # Check for multi_tags\n",
    "        if 'multi_tags' not in events_group:\n",
    "            print(\"❌ No multi_tags in Events group\")\n",
    "            return\n",
    "        \n",
    "        multi_tags = events_group['multi_tags']\n",
    "        n_tags = len(multi_tags)\n",
    "        print(f\"✓ Found {n_tags} multi_tags (trials)\")\n",
    "        \n",
    "        # Show first trial events\n",
    "        first_tag_key = list(multi_tags.keys())[0]\n",
    "        first_tag = multi_tags[first_tag_key]\n",
    "        \n",
    "        print(f\"\\n  First trial: {first_tag_key}\")\n",
    "        \n",
    "        if 'positions' in first_tag:\n",
    "            positions = first_tag['positions']\n",
    "            \n",
    "            if 'data' in positions:\n",
    "                event_times = positions['data'][()]\n",
    "                print(f\"    Event times: {event_times}\")\n",
    "            \n",
    "            if 'dimensions' in positions:\n",
    "                for dim_key in positions['dimensions'].keys():\n",
    "                    dim = positions['dimensions'][dim_key]\n",
    "                    if 'labels' in dim:\n",
    "                        labels = dim['labels'][()]\n",
    "                        \n",
    "                        def safe_decode(val):\n",
    "                            if isinstance(val, bytes):\n",
    "                                return val.decode('utf-8')\n",
    "                            elif isinstance(val, np.ndarray) and len(val) > 0:\n",
    "                                val = val[0]\n",
    "                                if isinstance(val, bytes):\n",
    "                                    return val.decode('utf-8')\n",
    "                            return val\n",
    "                        \n",
    "                        event_names = [safe_decode(label) for label in labels]\n",
    "                        print(f\"    Event names: {event_names}\")\n",
    "                        \n",
    "                        print(f\"\\n    Event timing:\")\n",
    "                        for name, time in zip(event_names, event_times):\n",
    "                            print(f\"      {name}: {time:.3f}s\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    # Check FIF file\n",
    "    fif_file = 'Data_converted_MetaData\\Subject_01\\Data_Subject_01_Session_04.h5_seeg_raw.fif'  # Change to your FIF file\n",
    "    \n",
    "    if os.path.exists(fif_file):\n",
    "        check_events_in_fif(fif_file)\n",
    "    else:\n",
    "        print(f\"FIF file not found: {fif_file}\")\n",
    "        print(\"Please provide path to your FIF file\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Check NIX file\n",
    "    nix_file = 'data_nix/Data_Subject_01_Session_01.h5'  # Change to your NIX file\n",
    "    \n",
    "    if os.path.exists(nix_file):\n",
    "        check_events_in_nix(nix_file)\n",
    "    else:\n",
    "        print(f\"\\nNIX file not found: {nix_file}\")\n",
    "        print(\"Please provide path to your NIX file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed metadata\n",
    "metadata_all.to_csv('subject_metadata_processed.csv', index=False)\n",
    "\n",
    "print(\"✓ Saved processed metadata to 'subject_metadata_processed.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Loading multi-session data**: Load and concatenate FIF files and CSV metadata from all sessions\n",
    "2. **Metadata integration**: Attach behavioral metadata to epochs for advanced trial selection\n",
    "3. **Flexible trial selection**: Use pandas-style queries to select trials by condition\n",
    "4. **Condition-based ERP analysis**: Compare ERPs across accuracy, memory load, and match conditions\n",
    "5. **Behavioral analysis**: Analyze RT and accuracy patterns\n",
    "6. **Single-trial analysis**: Correlate neural activity with behavior on a trial-by-trial basis\n",
    "\n",
    "### Key advantages of metadata approach:\n",
    "\n",
    "- **Flexible trial selection**: `epochs['correct == 1 and set_size >= 6']`\n",
    "- **Easy subgroup analysis**: Compare conditions without re-epoching\n",
    "- **Single-trial correlations**: Link neural activity to RT, accuracy, etc.\n",
    "- **Preserved alignment**: Neural data and behavior stay perfectly aligned\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "- Apply to multiple subjects\n",
    "- Implement more sophisticated statistical tests\n",
    "- Perform time-frequency analysis by condition\n",
    "- Run source reconstruction\n",
    "- Test early vs. late neural correlates of consciousness hypotheses!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
