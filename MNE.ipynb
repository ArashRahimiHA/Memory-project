{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1582379f-d360-4b5a-9ee6-ca9dde8f09a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels marked as bad:\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use(\"Qt5Agg\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e885c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_concatenate_fif(directory_path, preload=True, \n",
    "                              use_common_channels=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Read all FIF files from a directory and concatenate them.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    directory_path : str\n",
    "        Path to directory containing FIF files\n",
    "    preload : bool\n",
    "        Whether to load data into memory (default: True)\n",
    "    use_common_channels : bool\n",
    "        If True, only keep channels common to all files (default: True)\n",
    "    verbose : bool\n",
    "        Whether to print verbose output (default: False)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    raw_concat : mne.io.Raw\n",
    "        Concatenated Raw object\n",
    "    \"\"\"\n",
    "    raw_list = []\n",
    "    filenames = []\n",
    "    \n",
    "    # First pass: load all files\n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.endswith('.fif'):\n",
    "            full_path = os.path.join(directory_path, filename)\n",
    "            raw = mne.io.read_raw_fif(full_path, preload=False, verbose=verbose)\n",
    "            raw_list.append(raw)\n",
    "            filenames.append(filename)\n",
    "    \n",
    "    if len(raw_list) == 0:\n",
    "        raise ValueError(f\"No FIF files found in {directory_path}\")\n",
    "    \n",
    "    print(f\"Found {len(raw_list)} FIF files\")\n",
    "    \n",
    "    # Find common channels if requested\n",
    "    if use_common_channels and len(raw_list) > 1:\n",
    "        common_channels = set(raw_list[0].ch_names)\n",
    "        for raw in raw_list[1:]:\n",
    "            common_channels &= set(raw.ch_names)\n",
    "        \n",
    "        common_channels = sorted(list(common_channels))\n",
    "        print(f\"Using {len(common_channels)} common channels\")\n",
    "        \n",
    "        # Pick common channels from all files\n",
    "        for i, raw in enumerate(raw_list):\n",
    "            raw_list[i] = raw.copy().pick_channels(common_channels, ordered=True)\n",
    "    \n",
    "    # Concatenate\n",
    "    print(\"Concatenating files...\")\n",
    "    raw_concat = mne.concatenate_raws(raw_list, preload=preload, verbose=verbose)\n",
    "    \n",
    "    print(f\"Concatenation complete: {raw_concat.times[-1]:.2f} seconds total\")\n",
    "    \n",
    "    return raw_concat\n",
    "\n",
    "def read_and_concatenate_fif_all_channels(directory_path, \n",
    "                                          preload=True, \n",
    "                                          fill_value=0.0,\n",
    "                                          verbose=False):\n",
    "    \"\"\"\n",
    "    Read all FIF files from a directory and concatenate them using ALL channels.\n",
    "    Missing channels in individual files are filled with specified value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    directory_path : str\n",
    "        Path to directory containing FIF files\n",
    "    preload : bool\n",
    "        Whether to load data into memory (default: True)\n",
    "    fill_value : float\n",
    "        Value to use for missing channels (default: 0.0)\n",
    "        Common options: 0.0, np.nan, or None (will use 0.0)\n",
    "    verbose : bool\n",
    "        Whether to print verbose output (default: False)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    raw_concat : mne.io.Raw\n",
    "        Concatenated Raw object with all channels\n",
    "    channel_info : dict\n",
    "        Information about which channels were present in which files\n",
    "    \"\"\"\n",
    "    \n",
    "    if fill_value is None:\n",
    "        fill_value = 0.0\n",
    "    \n",
    "    print(f\"Loading FIF files from: {directory_path}\")\n",
    "    print(f\"Missing channels will be filled with: {fill_value}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Load all files and collect channel information\n",
    "    raw_list = []\n",
    "    filenames = []\n",
    "    all_channels_sets = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.endswith('.fif'):\n",
    "            full_path = os.path.join(directory_path, filename)\n",
    "            raw = mne.io.read_raw_fif(full_path, preload=False, verbose=verbose)\n",
    "            raw_list.append(raw)\n",
    "            filenames.append(filename)\n",
    "            all_channels_sets.append(set(raw.ch_names))\n",
    "            \n",
    "            print(f\"Loaded: {filename}\")\n",
    "            print(f\"  Channels: {len(raw.ch_names)}\")\n",
    "    \n",
    "    if len(raw_list) == 0:\n",
    "        raise ValueError(f\"No FIF files found in {directory_path}\")\n",
    "    \n",
    "    print(f\"\\nTotal files loaded: {len(raw_list)}\")\n",
    "    \n",
    "    # Step 2: Find union of all channels\n",
    "    all_channels_union = set()\n",
    "    for ch_set in all_channels_sets:\n",
    "        all_channels_union |= ch_set\n",
    "    \n",
    "    all_channels_sorted = sorted(list(all_channels_union))\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Total unique channels across all files: {len(all_channels_sorted)}\")\n",
    "    \n",
    "    # Step 3: Analyze channel presence across files\n",
    "    channel_presence = {ch: [] for ch in all_channels_sorted}\n",
    "    for i, ch_set in enumerate(all_channels_sets):\n",
    "        for ch in all_channels_sorted:\n",
    "            channel_presence[ch].append(ch in ch_set)\n",
    "    \n",
    "    # Report channels that are missing from some files\n",
    "    missing_in_some = {ch: sum([not present for present in presences]) \n",
    "                       for ch, presences in channel_presence.items()}\n",
    "    channels_missing_somewhere = {ch: count for ch, count in missing_in_some.items() if count > 0}\n",
    "    \n",
    "    if channels_missing_somewhere:\n",
    "        print(f\"\\nChannels missing from some files:\")\n",
    "        for ch, count in sorted(channels_missing_somewhere.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {ch}: missing from {count}/{len(raw_list)} files\")\n",
    "    else:\n",
    "        print(f\"\\nAll channels present in all files!\")\n",
    "    \n",
    "    # Step 4: Process each file to add missing channels\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"Processing files and adding missing channels...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    processed_raws = []\n",
    "    \n",
    "    for i, (raw, filename) in enumerate(zip(raw_list, filenames)):\n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        \n",
    "        # Find missing channels for this file\n",
    "        missing_channels = [ch for ch in all_channels_sorted if ch not in raw.ch_names]\n",
    "        \n",
    "        if missing_channels:\n",
    "            print(f\"  Adding {len(missing_channels)} missing channels filled with {fill_value}\")\n",
    "            \n",
    "            # Load data if not already loaded\n",
    "            if not raw.preload:\n",
    "                raw.load_data()\n",
    "            \n",
    "            # Get channel info from an existing channel to use as template\n",
    "            existing_ch_idx = 0\n",
    "            template_ch_name = raw.ch_names[existing_ch_idx]\n",
    "            \n",
    "            # Create info for the missing channels\n",
    "            info_missing = mne.create_info(\n",
    "                ch_names=missing_channels,\n",
    "                sfreq=raw.info['sfreq'],\n",
    "                ch_types='eeg'  # Default to EEG, will try to match types below\n",
    "            )\n",
    "            \n",
    "            # Try to infer channel type from name patterns\n",
    "            for ch_name in missing_channels:\n",
    "                ch_idx = info_missing.ch_names.index(ch_name)\n",
    "                \n",
    "                # Try to match channel type based on naming conventions\n",
    "                if 'STI' in ch_name or 'TRIG' in ch_name or ch_name.startswith('STI'):\n",
    "                    info_missing['chs'][ch_idx]['kind'] = mne.io.constants.FIFF.FIFFV_STIM_CH\n",
    "                elif 'EOG' in ch_name:\n",
    "                    info_missing['chs'][ch_idx]['kind'] = mne.io.constants.FIFF.FIFFV_EOG_CH\n",
    "                elif 'ECG' in ch_name:\n",
    "                    info_missing['chs'][ch_idx]['kind'] = mne.io.constants.FIFF.FIFFV_ECG_CH\n",
    "                # Otherwise keep as EEG (default)\n",
    "            \n",
    "            # Create data array filled with the specified value\n",
    "            n_samples = raw.n_times\n",
    "            missing_data = np.full((len(missing_channels), n_samples), fill_value)\n",
    "            \n",
    "            # Create Raw object for missing channels\n",
    "            raw_missing = mne.io.RawArray(missing_data, info_missing)\n",
    "            \n",
    "            # Add the missing channels to the original raw\n",
    "            raw = raw.add_channels([raw_missing], force_update_info=True)\n",
    "            \n",
    "            print(f\"  New channel count: {len(raw.ch_names)}\")\n",
    "        else:\n",
    "            print(f\"  No missing channels\")\n",
    "            if not raw.preload:\n",
    "                raw.load_data()\n",
    "        \n",
    "        # Ensure channels are in the same order\n",
    "        raw.reorder_channels(all_channels_sorted)\n",
    "        \n",
    "        processed_raws.append(raw)\n",
    "    \n",
    "    # Step 5: Concatenate all processed files\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"Concatenating all files...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    raw_concat = mne.concatenate_raws(processed_raws, preload=preload, verbose=verbose)\n",
    "    \n",
    "    print(f\"\\nConcatenation complete!\")\n",
    "    print(f\"  Total channels: {len(raw_concat.ch_names)}\")\n",
    "    print(f\"  Total duration: {raw_concat.times[-1]:.2f} seconds\")\n",
    "    print(f\"  Sampling frequency: {raw_concat.info['sfreq']} Hz\")\n",
    "    \n",
    "    # Prepare summary info\n",
    "    channel_info = {\n",
    "        'all_channels': all_channels_sorted,\n",
    "        'n_channels': len(all_channels_sorted),\n",
    "        'channels_missing_somewhere': channels_missing_somewhere,\n",
    "        'files_processed': filenames,\n",
    "        'fill_value_used': fill_value\n",
    "    }\n",
    "    \n",
    "    return raw_concat, channel_info\n",
    "\n",
    "\n",
    "def print_channel_summary(channel_info):\n",
    "    \"\"\"\n",
    "    Print a summary of channel information from concatenation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    channel_info : dict\n",
    "        Dictionary returned by read_and_concatenate_fif_all_channels\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CHANNEL SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nTotal unique channels: {channel_info['n_channels']}\")\n",
    "    print(f\"Files processed: {len(channel_info['files_processed'])}\")\n",
    "    print(f\"Fill value used: {channel_info['fill_value_used']}\")\n",
    "    \n",
    "    if channel_info['channels_missing_somewhere']:\n",
    "        print(f\"\\nChannels with missing data (filled with {channel_info['fill_value_used']}):\")\n",
    "        for ch, count in sorted(channel_info['channels_missing_somewhere'].items(), \n",
    "                               key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {ch}: missing from {count} file(s)\")\n",
    "    else:\n",
    "        print(\"\\nAll channels present in all files - no filling needed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c566522-2df3-4098-a6b9-ff5214cd53d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FIF files from: Data_converted/Subject_06\n",
      "Missing channels will be filled with: 0.0\n",
      "================================================================================\n",
      "Loaded: Data_Subject_06_Session_01.h5_seeg_raw.fif\n",
      "  Channels: 11\n",
      "Loaded: Data_Subject_06_Session_02.h5_seeg_raw.fif\n",
      "  Channels: 11\n",
      "Loaded: Data_Subject_06_Session_03.h5_seeg_raw.fif\n",
      "  Channels: 11\n",
      "Loaded: Data_Subject_06_Session_04.h5_seeg_raw.fif\n",
      "  Channels: 11\n",
      "Loaded: Data_Subject_06_Session_05.h5_seeg_raw.fif\n",
      "  Channels: 11\n",
      "Loaded: Data_Subject_06_Session_06.h5_seeg_raw.fif\n",
      "  Channels: 11\n",
      "Loaded: Data_Subject_06_Session_07.h5_seeg_raw.fif\n",
      "  Channels: 11\n",
      "\n",
      "Total files loaded: 7\n",
      "\n",
      "================================================================================\n",
      "Total unique channels across all files: 11\n",
      "\n",
      "All channels present in all files!\n",
      "\n",
      "================================================================================\n",
      "Processing files and adding missing channels...\n",
      "================================================================================\n",
      "\n",
      "Processing: Data_Subject_06_Session_01.h5_seeg_raw.fif\n",
      "  No missing channels\n",
      "Reading 0 ... 79999  =      0.000 ...   399.995 secs...\n",
      "\n",
      "Processing: Data_Subject_06_Session_02.h5_seeg_raw.fif\n",
      "  No missing channels\n",
      "Reading 0 ... 79999  =      0.000 ...   399.995 secs...\n",
      "\n",
      "Processing: Data_Subject_06_Session_03.h5_seeg_raw.fif\n",
      "  No missing channels\n",
      "Reading 0 ... 78399  =      0.000 ...   391.995 secs...\n",
      "\n",
      "Processing: Data_Subject_06_Session_04.h5_seeg_raw.fif\n",
      "  No missing channels\n",
      "Reading 0 ... 79999  =      0.000 ...   399.995 secs...\n",
      "\n",
      "Processing: Data_Subject_06_Session_05.h5_seeg_raw.fif\n",
      "  No missing channels\n",
      "Reading 0 ... 79999  =      0.000 ...   399.995 secs...\n",
      "\n",
      "Processing: Data_Subject_06_Session_06.h5_seeg_raw.fif\n",
      "  No missing channels\n",
      "Reading 0 ... 79999  =      0.000 ...   399.995 secs...\n",
      "\n",
      "Processing: Data_Subject_06_Session_07.h5_seeg_raw.fif\n",
      "  No missing channels\n",
      "Reading 0 ... 79999  =      0.000 ...   399.995 secs...\n",
      "\n",
      "================================================================================\n",
      "Concatenating all files...\n",
      "================================================================================\n",
      "\n",
      "Concatenation complete!\n",
      "  Total channels: 11\n",
      "  Total duration: 2791.99 seconds\n",
      "  Sampling frequency: 200.0 Hz\n",
      "Finding events on: STI\n",
      "1744 events found on stim channel STI\n",
      "Event IDs: [1 2 3 4 5]\n",
      "Loading FIF files from: Data_converted/Subject_08\n",
      "Missing channels will be filled with: 0.0\n",
      "================================================================================\n",
      "Loaded: Data_Subject_08_Session_01.h5_seeg_raw.fif\n",
      "  Channels: 20\n",
      "Loaded: Data_Subject_08_Session_02.h5_seeg_raw.fif\n",
      "  Channels: 20\n",
      "Loaded: Data_Subject_08_Session_03.h5_seeg_raw.fif\n",
      "  Channels: 20\n",
      "Loaded: Data_Subject_08_Session_04.h5_seeg_raw.fif\n",
      "  Channels: 20\n",
      "Loaded: Data_Subject_08_Session_05.h5_seeg_raw.fif\n",
      "  Channels: 20\n",
      "\n",
      "Total files loaded: 5\n",
      "\n",
      "================================================================================\n",
      "Total unique channels across all files: 20\n",
      "\n",
      "All channels present in all files!\n",
      "\n",
      "================================================================================\n",
      "Processing files and adding missing channels...\n",
      "================================================================================\n",
      "\n",
      "Processing: Data_Subject_08_Session_01.h5_seeg_raw.fif\n",
      "  No missing channels\n",
      "Reading 0 ... 79999  =      0.000 ...   399.995 secs...\n",
      "\n",
      "Processing: Data_Subject_08_Session_02.h5_seeg_raw.fif\n",
      "  No missing channels\n",
      "Reading 0 ... 79999  =      0.000 ...   399.995 secs...\n",
      "\n",
      "Processing: Data_Subject_08_Session_03.h5_seeg_raw.fif\n",
      "  No missing channels\n",
      "Reading 0 ... 79999  =      0.000 ...   399.995 secs...\n",
      "\n",
      "Processing: Data_Subject_08_Session_04.h5_seeg_raw.fif\n",
      "  No missing channels\n",
      "Reading 0 ... 79999  =      0.000 ...   399.995 secs...\n",
      "\n",
      "Processing: Data_Subject_08_Session_05.h5_seeg_raw.fif\n",
      "  No missing channels\n",
      "Reading 0 ... 78399  =      0.000 ...   391.995 secs...\n",
      "\n",
      "================================================================================\n",
      "Concatenating all files...\n",
      "================================================================================\n",
      "\n",
      "Concatenation complete!\n",
      "  Total channels: 20\n",
      "  Total duration: 1991.99 seconds\n",
      "  Sampling frequency: 200.0 Hz\n",
      "Finding events on: STI\n",
      "1245 events found on stim channel STI\n",
      "Event IDs: [1 2 3 4 5]\n",
      "Using qt as 2D backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne_qt_browser._pg_figure.MNEQtBrowser at 0x1cc318545f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading data from subject 6\n",
    "subject_6, s6_channel_info = read_and_concatenate_fif_all_channels(\"Data_converted/Subject_06\")\n",
    "# Then load the data\n",
    "events = mne.find_events(subject_6, \n",
    "                         stim_channel='STI', \n",
    "                         consecutive=True,\n",
    "                         shortest_event=0,   # FIX: Allow 1-sample events\n",
    "                         min_duration=0,      # FIX: Include all durations\n",
    "                         initial_event=True)  # BONUS: Detects initial value\n",
    "event_id = {\n",
    "    'fixation': 1,\n",
    "    'encoding': 2,\n",
    "    'maintenance': 3,\n",
    "    'retrieval': 4,\n",
    "    'response': 5\n",
    "}\n",
    "#loading data from subject 2\n",
    "subject_8, s8_channel_info = read_and_concatenate_fif_all_channels(\"Data_converted/Subject_08\")\n",
    "# Then load the data\n",
    "events_8 = mne.find_events(subject_8, \n",
    "                         stim_channel='STI', \n",
    "                         consecutive=True,\n",
    "                         shortest_event=0,   # FIX: Allow 1-sample events\n",
    "                         min_duration=0,      # FIX: Include all durations\n",
    "                         initial_event=True)  # BONUS: Detects initial value\n",
    "event_id = {\n",
    "    'fixation': 1,\n",
    "    'encoding': 2,\n",
    "    'maintenance': 3,\n",
    "    'retrieval': 4,\n",
    "    'response': 5\n",
    "}\n",
    "#Plotting the raw file\n",
    "subject_8.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41e340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af37daf5-c5aa-4e58-8db0-5b932f212470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 5 contiguous segments\n",
      "Setting up band-pass filter from 0.1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.10\n",
      "- Lower transition bandwidth: 0.10 Hz (-6 dB cutoff frequency: 0.05 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 6601 samples (33.005 s)\n",
      "\n",
      "Filtering raw data in 5 contiguous segments\n",
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 661 samples (3.305 s)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x640 with 1 Axes>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering th data\n",
    "seeg_raw_filtered_8 = subject_8.copy().filter(l_freq=0.1, h_freq=40)\n",
    "seeg_raw_filtered_ica_8 = subject_8.copy().filter(l_freq=1,h_freq=40)\n",
    "\n",
    "#Plotting the filtered data\n",
    "seeg_raw_filtered_ica_8.plot_sensors(kind = \"3d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359790e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data into memory...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_loaded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLoading data into memory...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mall_loaded\u001b[49m.load_data()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Extract events\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExtracting events...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_loaded' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "print(\"\\nLoading data into memory...\")\n",
    "all_loaded.load_data()\n",
    "\n",
    "# Extract events\n",
    "print(\"Extracting events...\")\n",
    "events = mne.find_events(all_loaded, stim_channel='STI', consecutive=True)\n",
    "\n",
    "event_id = {\n",
    "    'fixation': 1,\n",
    "    'encoding': 2,\n",
    "    'maintenance': 3,\n",
    "    'retrieval': 4,\n",
    "    'response': 5\n",
    "}\n",
    "\n",
    "print(f\"\\nFinal concatenated data:\")\n",
    "print(f\"  Channels: {len(all_loaded.ch_names)}\")\n",
    "print(f\"  Duration: {all_loaded.times[-1]:.2f} seconds\")\n",
    "print(f\"  Events found: {len(events)}\")\n",
    "\n",
    "# Show event distribution\n",
    "unique_events, counts = np.unique(events[:, 2], return_counts=True)\n",
    "print(f\"\\nEvent distribution:\")\n",
    "for evt, count in zip(unique_events, counts):\n",
    "    event_name = [k for k, v in event_id.items() if v == evt]\n",
    "    name = event_name[0] if event_name else 'unknown'\n",
    "    print(f\"  {name} ({evt}): {count} events\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74aa7fbc-6a1b-4973-b512-d20fab0b18d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Reaction Time Analysis for FIF File...\n",
      "Processing file: /mnt/user-data/uploads/your_file_raw.fif\n",
      "Loading FIF file: /mnt/user-data/uploads/your_file_raw.fif\n",
      "Loading as Raw file...\n",
      "Error extracting reaction times: fname does not exist: \"d:\\mnt\\user-data\\uploads\\your_file_raw.fif\"\n",
      "\n",
      "No reaction time data found!\n",
      "Tips:\n",
      "- Check that event IDs are correct\n",
      "- Verify that probe and response events exist in the file\n",
      "- Try specifying probe_event_id and response_event_id manually\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\RSKALA\\AppData\\Local\\Temp\\ipykernel_20756\\754550572.py\", line 64, in extract_reaction_times_from_fif\n",
      "    raw = mne.io.read_raw_fif(file_path, preload=False, verbose=False)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\mne\\Lib\\site-packages\\mne\\io\\fiff\\raw.py\", line 543, in read_raw_fif\n",
      "    return Raw(\n",
      "        fname=fname,\n",
      "    ...<3 lines>...\n",
      "        on_split_missing=on_split_missing,\n",
      "    )\n",
      "  File \"<decorator-gen-206>\", line 10, in __init__\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\mne\\Lib\\site-packages\\mne\\io\\fiff\\raw.py\", line 103, in __init__\n",
      "    raw, next_fname, buffer_size_sec = self._read_raw_file(\n",
      "                                       ~~~~~~~~~~~~~~~~~~~^\n",
      "        next_fname, allow_maxshield, preload, do_check_ext\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"<decorator-gen-207>\", line 12, in _read_raw_file\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\mne\\Lib\\site-packages\\mne\\io\\fiff\\raw.py\", line 198, in _read_raw_file\n",
      "    fname = _check_fname(fname, \"read\", True, \"fname\")\n",
      "  File \"<decorator-gen-0>\", line 12, in _check_fname\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\mne\\Lib\\site-packages\\mne\\utils\\check.py\", line 302, in _check_fname\n",
      "    raise FileNotFoundError(f'{name} does not exist: \"{fname}\"')\n",
      "FileNotFoundError: fname does not exist: \"d:\\mnt\\user-data\\uploads\\your_file_raw.fif\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reaction Time Analysis for FIF Format Files (MNE-Python)\n",
    "Extracts, analyzes, and visualizes reaction times from probe onset to response\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import mne\n",
    "from scipy import stats as scipy_stats\n",
    "import pandas as pd\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "\n",
    "def extract_reaction_times_from_fif(fif_file_path, probe_event_id=None, response_event_id=None):\n",
    "    \"\"\"\n",
    "    Extract reaction times from MNE FIF format file.\n",
    "    RT = Response time - Probe onset time\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fif_file_path : str or Path\n",
    "        Path to the FIF format file\n",
    "    probe_event_id : int or str or list, optional\n",
    "        Event ID(s) for probe/retrieval onset\n",
    "        If None, will attempt to detect automatically\n",
    "    response_event_id : int or str or list, optional\n",
    "        Event ID(s) for response/button press\n",
    "        If None, will attempt to detect automatically\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing RT data and metadata\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'reaction_times': [],\n",
    "        'trial_numbers': [],\n",
    "        'correct_trials': [],\n",
    "        'probe_times': [],\n",
    "        'response_times': [],\n",
    "        'probe_event_ids': [],\n",
    "        'response_event_ids': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Loading FIF file: {fif_file_path}\")\n",
    "    \n",
    "    # Load the raw or epochs file\n",
    "    file_path = str(fif_file_path)\n",
    "    \n",
    "    try:\n",
    "        # Try loading as Raw first\n",
    "        if 'epo' in file_path or 'epochs' in file_path.lower():\n",
    "            print(\"Loading as Epochs file...\")\n",
    "            epochs = mne.read_epochs(file_path, preload=False, verbose=False)\n",
    "            events = epochs.events\n",
    "            event_id = epochs.event_id\n",
    "            sfreq = epochs.info['sfreq']\n",
    "        else:\n",
    "            print(\"Loading as Raw file...\")\n",
    "            raw = mne.io.read_raw_fif(file_path, preload=False, verbose=False)\n",
    "            events = mne.find_events(raw, verbose=False)\n",
    "            sfreq = raw.info['sfreq']\n",
    "            \n",
    "            # Try to get event_id from annotations or create generic mapping\n",
    "            if raw.annotations:\n",
    "                event_id = {}\n",
    "                for desc in set(raw.annotations.description):\n",
    "                    if desc.isdigit():\n",
    "                        event_id[desc] = int(desc)\n",
    "                    else:\n",
    "                        # Try to find corresponding event code\n",
    "                        unique_events = np.unique(events[:, 2])\n",
    "                        if len(event_id) < len(unique_events):\n",
    "                            event_id[desc] = unique_events[len(event_id)]\n",
    "            else:\n",
    "                # Create generic event_id mapping\n",
    "                unique_event_codes = np.unique(events[:, 2])\n",
    "                event_id = {f\"Event_{code}\": code for code in unique_event_codes}\n",
    "        \n",
    "        print(f\"Found {len(events)} events\")\n",
    "        print(f\"Sampling frequency: {sfreq} Hz\")\n",
    "        print(f\"Event IDs: {event_id}\")\n",
    "        \n",
    "        # Determine probe and response events\n",
    "        if probe_event_id is None:\n",
    "            # Auto-detect: look for keywords\n",
    "            probe_keys = [k for k in event_id.keys() if any(\n",
    "                word in str(k).lower() for word in ['probe', 'retrieval', 'test', 'cue']\n",
    "            )]\n",
    "            if probe_keys:\n",
    "                probe_event_id = [event_id[k] for k in probe_keys]\n",
    "                print(f\"Auto-detected probe events: {probe_keys} -> {probe_event_id}\")\n",
    "            else:\n",
    "                print(\"WARNING: Could not auto-detect probe events. Please specify probe_event_id.\")\n",
    "                return data\n",
    "        elif isinstance(probe_event_id, str):\n",
    "            probe_event_id = [event_id[probe_event_id]]\n",
    "        elif isinstance(probe_event_id, int):\n",
    "            probe_event_id = [probe_event_id]\n",
    "        \n",
    "        if response_event_id is None:\n",
    "            # Auto-detect: look for keywords\n",
    "            response_keys = [k for k in event_id.keys() if any(\n",
    "                word in str(k).lower() for word in ['response', 'button', 'answer', 'resp']\n",
    "            )]\n",
    "            if response_keys:\n",
    "                response_event_id = [event_id[k] for k in response_keys]\n",
    "                print(f\"Auto-detected response events: {response_keys} -> {response_event_id}\")\n",
    "            else:\n",
    "                print(\"WARNING: Could not auto-detect response events. Please specify response_event_id.\")\n",
    "                return data\n",
    "        elif isinstance(response_event_id, str):\n",
    "            response_event_id = [event_id[response_event_id]]\n",
    "        elif isinstance(response_event_id, int):\n",
    "            response_event_id = [response_event_id]\n",
    "        \n",
    "        # Extract probe and response times\n",
    "        probe_mask = np.isin(events[:, 2], probe_event_id)\n",
    "        response_mask = np.isin(events[:, 2], response_event_id)\n",
    "        \n",
    "        probe_events = events[probe_mask]\n",
    "        response_events = events[response_mask]\n",
    "        \n",
    "        print(f\"Found {len(probe_events)} probe events\")\n",
    "        print(f\"Found {len(response_events)} response events\")\n",
    "        \n",
    "        # Convert sample indices to time (in seconds)\n",
    "        probe_times = probe_events[:, 0] / sfreq\n",
    "        response_times = response_events[:, 0] / sfreq\n",
    "        \n",
    "        # Match each probe to the next response\n",
    "        for i, (probe_sample, probe_time, probe_code) in enumerate(zip(\n",
    "            probe_events[:, 0], probe_times, probe_events[:, 2]\n",
    "        )):\n",
    "            # Find the next response after this probe\n",
    "            future_response_mask = response_events[:, 0] > probe_sample\n",
    "            \n",
    "            if np.any(future_response_mask):\n",
    "                next_response_idx = np.where(future_response_mask)[0][0]\n",
    "                response_sample = response_events[next_response_idx, 0]\n",
    "                response_time = response_times[next_response_idx]\n",
    "                response_code = response_events[next_response_idx, 2]\n",
    "                \n",
    "                # Calculate RT in milliseconds\n",
    "                rt = (response_time - probe_time) * 1000\n",
    "                \n",
    "                # Only include reasonable RTs (100ms - 3000ms)\n",
    "                if 100 <= rt <= 3000:\n",
    "                    data['reaction_times'].append(rt)\n",
    "                    data['trial_numbers'].append(i + 1)\n",
    "                    data['probe_times'].append(probe_time)\n",
    "                    data['response_times'].append(response_time)\n",
    "                    data['probe_event_ids'].append(probe_code)\n",
    "                    data['response_event_ids'].append(response_code)\n",
    "                    \n",
    "                    # Placeholder for correctness (would need task info)\n",
    "                    data['correct_trials'].append(True)\n",
    "        \n",
    "        print(f\"\\nExtracted {len(data['reaction_times'])} valid trials (100-3000ms)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting reaction times: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def compute_rt_statistics(reaction_times):\n",
    "    \"\"\"\n",
    "    Compute comprehensive statistics on reaction times.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    reaction_times : array-like\n",
    "        Reaction times in milliseconds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary of statistics\n",
    "    \"\"\"\n",
    "    rts = np.array(reaction_times)\n",
    "    \n",
    "    if len(rts) == 0:\n",
    "        print(\"No reaction times to analyze!\")\n",
    "        return None\n",
    "    \n",
    "    stats_dict = {\n",
    "        'mean': np.mean(rts),\n",
    "        'median': np.median(rts),\n",
    "        'std': np.std(rts),\n",
    "        'sem': scipy_stats.sem(rts),\n",
    "        'min': np.min(rts),\n",
    "        'max': np.max(rts),\n",
    "        'q25': np.percentile(rts, 25),\n",
    "        'q75': np.percentile(rts, 75),\n",
    "        'iqr': np.percentile(rts, 75) - np.percentile(rts, 25),\n",
    "        'n_trials': len(rts),\n",
    "        'skewness': scipy_stats.skew(rts),\n",
    "        'kurtosis': scipy_stats.kurtosis(rts),\n",
    "        'cv': (np.std(rts) / np.mean(rts)) * 100  # Coefficient of variation\n",
    "    }\n",
    "    \n",
    "    return stats_dict\n",
    "\n",
    "\n",
    "def detect_outliers(reaction_times, method='iqr', threshold=3):\n",
    "    \"\"\"\n",
    "    Detect outlier trials based on reaction time.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    reaction_times : array-like\n",
    "        Reaction times in milliseconds\n",
    "    method : str\n",
    "        'iqr' for interquartile range or 'zscore' for z-score method\n",
    "    threshold : float\n",
    "        For 'iqr': multiplier for IQR (default 3)\n",
    "        For 'zscore': z-score threshold (default 3)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (outlier_mask, outlier_indices)\n",
    "    \"\"\"\n",
    "    rts = np.array(reaction_times)\n",
    "    \n",
    "    if method == 'iqr':\n",
    "        q25 = np.percentile(rts, 25)\n",
    "        q75 = np.percentile(rts, 75)\n",
    "        iqr = q75 - q25\n",
    "        \n",
    "        lower_bound = q25 - threshold * iqr\n",
    "        upper_bound = q75 + threshold * iqr\n",
    "        \n",
    "        outlier_mask = (rts < lower_bound) | (rts > upper_bound)\n",
    "        \n",
    "    elif method == 'zscore':\n",
    "        z_scores = np.abs(scipy_stats.zscore(rts))\n",
    "        outlier_mask = z_scores > threshold\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"method must be 'iqr' or 'zscore'\")\n",
    "    \n",
    "    outlier_indices = np.where(outlier_mask)[0]\n",
    "    \n",
    "    return outlier_mask, outlier_indices\n",
    "\n",
    "\n",
    "def plot_rt_analysis(data, stats_dict, output_path=None):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of reaction time data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : dict\n",
    "        Dictionary containing RT data from extract_reaction_times_from_fif\n",
    "    stats_dict : dict\n",
    "        Dictionary of statistics from compute_rt_statistics\n",
    "    output_path : str, optional\n",
    "        Path to save the figure. If None, saves to current directory.\n",
    "    \"\"\"\n",
    "    # Set default output path based on platform\n",
    "    if output_path is None:\n",
    "        output_dir = Path('/mnt/user-data/outputs')\n",
    "        if not output_dir.exists():\n",
    "            output_dir = Path('.')  # Use current directory\n",
    "        output_path = output_dir / 'rt_analysis_fif.png'\n",
    "    \n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    rts = np.array(data['reaction_times'])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Reaction Time Analysis (FIF Format)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Histogram with KDE\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(rts, bins=30, alpha=0.7, color='skyblue', edgecolor='black', density=True)\n",
    "    \n",
    "    # Add KDE\n",
    "    from scipy.stats import gaussian_kde\n",
    "    kde = gaussian_kde(rts)\n",
    "    x_range = np.linspace(rts.min(), rts.max(), 200)\n",
    "    ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
    "    \n",
    "    # Add mean and median lines\n",
    "    ax.axvline(stats_dict['mean'], color='green', linestyle='--', \n",
    "               linewidth=2, label=f\"Mean: {stats_dict['mean']:.1f} ms\")\n",
    "    ax.axvline(stats_dict['median'], color='orange', linestyle='--', \n",
    "               linewidth=2, label=f\"Median: {stats_dict['median']:.1f} ms\")\n",
    "    \n",
    "    ax.set_xlabel('Reaction Time (ms)', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Distribution of Reaction Times', fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Box plot with outliers\n",
    "    ax = axes[0, 1]\n",
    "    bp = ax.boxplot(rts, vert=True, patch_artist=True, \n",
    "                     showmeans=True, meanline=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    bp['medians'][0].set_color('red')\n",
    "    bp['medians'][0].set_linewidth(2)\n",
    "    bp['means'][0].set_color('green')\n",
    "    bp['means'][0].set_linewidth(2)\n",
    "    \n",
    "    ax.set_ylabel('Reaction Time (ms)', fontsize=12)\n",
    "    ax.set_title('Box Plot with Outliers', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add text with quartiles\n",
    "    ax.text(1.35, stats_dict['q75'], f\"Q3: {stats_dict['q75']:.1f}\", \n",
    "            fontsize=10, va='center')\n",
    "    ax.text(1.35, stats_dict['median'], f\"Median: {stats_dict['median']:.1f}\", \n",
    "            fontsize=10, va='center')\n",
    "    ax.text(1.35, stats_dict['q25'], f\"Q1: {stats_dict['q25']:.1f}\", \n",
    "            fontsize=10, va='center')\n",
    "    \n",
    "    # 3. Reaction time across trials\n",
    "    ax = axes[0, 2]\n",
    "    trial_nums = data['trial_numbers']\n",
    "    ax.plot(trial_nums, rts, 'o-', alpha=0.6, markersize=4, linewidth=1)\n",
    "    ax.axhline(stats_dict['mean'], color='red', linestyle='--', \n",
    "               linewidth=2, label='Mean', alpha=0.7)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(trial_nums, rts, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(trial_nums, p(trial_nums), \"r--\", alpha=0.5, linewidth=2, \n",
    "            label=f'Trend (slope: {z[0]:.2f})')\n",
    "    \n",
    "    ax.set_xlabel('Trial Number', fontsize=12)\n",
    "    ax.set_ylabel('Reaction Time (ms)', fontsize=12)\n",
    "    ax.set_title('RT Across Trials', fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Q-Q plot for normality\n",
    "    ax = axes[1, 0]\n",
    "    scipy_stats.probplot(rts, dist=\"norm\", plot=ax)\n",
    "    ax.set_title('Q-Q Plot (Normality Check)', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Cumulative distribution\n",
    "    ax = axes[1, 1]\n",
    "    sorted_rts = np.sort(rts)\n",
    "    cumulative = np.arange(1, len(sorted_rts) + 1) / len(sorted_rts)\n",
    "    ax.plot(sorted_rts, cumulative, linewidth=2)\n",
    "    ax.axhline(0.5, color='red', linestyle='--', alpha=0.5, label='50th percentile')\n",
    "    ax.axvline(stats_dict['median'], color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add percentile markers\n",
    "    percentiles = [10, 25, 75, 90]\n",
    "    for p in percentiles:\n",
    "        val = np.percentile(rts, p)\n",
    "        ax.axvline(val, color='gray', linestyle=':', alpha=0.3)\n",
    "        ax.text(val, 0.02, f'{p}th', fontsize=8, rotation=90)\n",
    "    \n",
    "    ax.set_xlabel('Reaction Time (ms)', fontsize=12)\n",
    "    ax.set_ylabel('Cumulative Probability', fontsize=12)\n",
    "    ax.set_title('Cumulative Distribution Function', fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Statistics table\n",
    "    ax = axes[1, 2]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    stats_text = f\"\"\"\n",
    "    REACTION TIME STATISTICS\n",
    "    ═══════════════════════════\n",
    "    \n",
    "    Central Tendency:\n",
    "      Mean:         {stats_dict['mean']:.2f} ms\n",
    "      Median:       {stats_dict['median']:.2f} ms\n",
    "      \n",
    "    Variability:\n",
    "      SD:           {stats_dict['std']:.2f} ms\n",
    "      SEM:          {stats_dict['sem']:.2f} ms\n",
    "      CV:           {stats_dict['cv']:.2f}%\n",
    "      IQR:          {stats_dict['iqr']:.2f} ms\n",
    "      \n",
    "    Range:\n",
    "      Min:          {stats_dict['min']:.2f} ms\n",
    "      Max:          {stats_dict['max']:.2f} ms\n",
    "      Q25:          {stats_dict['q25']:.2f} ms\n",
    "      Q75:          {stats_dict['q75']:.2f} ms\n",
    "      \n",
    "    Distribution:\n",
    "      Skewness:     {stats_dict['skewness']:.3f}\n",
    "      Kurtosis:     {stats_dict['kurtosis']:.3f}\n",
    "      \n",
    "    Sample:\n",
    "      N trials:     {stats_dict['n_trials']}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.text(0.1, 0.95, stats_text, transform=ax.transAxes,\n",
    "            fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nPlot saved to: {output_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def generate_report(data, stats_dict, outlier_info=None):\n",
    "    \"\"\"\n",
    "    Generate a text report of the analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : dict\n",
    "        Dictionary containing RT data\n",
    "    stats_dict : dict\n",
    "        Dictionary of statistics\n",
    "    outlier_info : tuple, optional\n",
    "        (outlier_mask, outlier_indices) from detect_outliers\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"REACTION TIME ANALYSIS REPORT (FIF FORMAT)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nSample Size: {stats_dict['n_trials']} trials\")\n",
    "    \n",
    "    print(\"\\n--- Central Tendency ---\")\n",
    "    print(f\"Mean RT:              {stats_dict['mean']:.2f} ms\")\n",
    "    print(f\"Median RT:            {stats_dict['median']:.2f} ms\")\n",
    "    print(f\"Difference:           {abs(stats_dict['mean'] - stats_dict['median']):.2f} ms\")\n",
    "    \n",
    "    print(\"\\n--- Variability ---\")\n",
    "    print(f\"Standard Deviation:   {stats_dict['std']:.2f} ms\")\n",
    "    print(f\"Standard Error:       {stats_dict['sem']:.2f} ms\")\n",
    "    print(f\"Coefficient of Var:   {stats_dict['cv']:.2f}%\")\n",
    "    print(f\"Interquartile Range:  {stats_dict['iqr']:.2f} ms\")\n",
    "    \n",
    "    print(\"\\n--- Range ---\")\n",
    "    print(f\"Minimum RT:           {stats_dict['min']:.2f} ms\")\n",
    "    print(f\"25th Percentile:      {stats_dict['q25']:.2f} ms\")\n",
    "    print(f\"75th Percentile:      {stats_dict['q75']:.2f} ms\")\n",
    "    print(f\"Maximum RT:           {stats_dict['max']:.2f} ms\")\n",
    "    print(f\"Total Range:          {stats_dict['max'] - stats_dict['min']:.2f} ms\")\n",
    "    \n",
    "    print(\"\\n--- Distribution Shape ---\")\n",
    "    print(f\"Skewness:             {stats_dict['skewness']:.3f}\", end=\"\")\n",
    "    if stats_dict['skewness'] > 0.5:\n",
    "        print(\" (right-skewed)\")\n",
    "    elif stats_dict['skewness'] < -0.5:\n",
    "        print(\" (left-skewed)\")\n",
    "    else:\n",
    "        print(\" (approximately symmetric)\")\n",
    "    \n",
    "    print(f\"Kurtosis:             {stats_dict['kurtosis']:.3f}\", end=\"\")\n",
    "    if stats_dict['kurtosis'] > 1:\n",
    "        print(\" (heavy-tailed)\")\n",
    "    elif stats_dict['kurtosis'] < -1:\n",
    "        print(\" (light-tailed)\")\n",
    "    else:\n",
    "        print(\" (normal-like tails)\")\n",
    "    \n",
    "    # Normality test\n",
    "    _, p_value = scipy_stats.shapiro(data['reaction_times'])\n",
    "    print(f\"\\nShapiro-Wilk Test:    p = {p_value:.4f}\", end=\"\")\n",
    "    if p_value < 0.05:\n",
    "        print(\" (significantly non-normal)\")\n",
    "    else:\n",
    "        print(\" (not significantly different from normal)\")\n",
    "    \n",
    "    # Outlier information\n",
    "    if outlier_info is not None:\n",
    "        outlier_mask, outlier_indices = outlier_info\n",
    "        n_outliers = np.sum(outlier_mask)\n",
    "        percent_outliers = (n_outliers / len(data['reaction_times'])) * 100\n",
    "        \n",
    "        print(\"\\n--- Outlier Detection (IQR method, threshold=3) ---\")\n",
    "        print(f\"Number of outliers:   {n_outliers} ({percent_outliers:.1f}%)\")\n",
    "        if n_outliers > 0:\n",
    "            outlier_rts = np.array(data['reaction_times'])[outlier_mask]\n",
    "            print(f\"Outlier RT range:     {outlier_rts.min():.2f} - {outlier_rts.max():.2f} ms\")\n",
    "            print(f\"Outlier trial IDs:    {np.array(data['trial_numbers'])[outlier_mask].tolist()}\")\n",
    "    \n",
    "    # Event ID information\n",
    "    if len(data['probe_event_ids']) > 0:\n",
    "        unique_probe_ids = np.unique(data['probe_event_ids'])\n",
    "        unique_response_ids = np.unique(data['response_event_ids'])\n",
    "        print(\"\\n--- Event Information ---\")\n",
    "        print(f\"Probe event IDs:      {unique_probe_ids.tolist()}\")\n",
    "        print(f\"Response event IDs:   {unique_response_ids.tolist()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main analysis pipeline for FIF files.\"\"\"\n",
    "    \n",
    "    # Example file path - UPDATE THIS to your actual file\n",
    "    fif_file = \"/mnt/user-data/uploads/your_file_raw.fif\"\n",
    "    \n",
    "    # Optional: Specify event IDs if auto-detection fails\n",
    "    # probe_event_id = 3  # or [3, 4] for multiple\n",
    "    # response_event_id = 5  # or [5, 6] for multiple\n",
    "    probe_event_id = None  # Auto-detect\n",
    "    response_event_id = None  # Auto-detect\n",
    "    \n",
    "    print(\"Starting Reaction Time Analysis for FIF File...\")\n",
    "    print(f\"Processing file: {fif_file}\")\n",
    "    \n",
    "    # Extract data\n",
    "    data = extract_reaction_times_from_fif(fif_file, \n",
    "                                          probe_event_id=probe_event_id,\n",
    "                                          response_event_id=response_event_id)\n",
    "    \n",
    "    if len(data['reaction_times']) == 0:\n",
    "        print(\"\\nNo reaction time data found!\")\n",
    "        print(\"Tips:\")\n",
    "        print(\"- Check that event IDs are correct\")\n",
    "        print(\"- Verify that probe and response events exist in the file\")\n",
    "        print(\"- Try specifying probe_event_id and response_event_id manually\")\n",
    "        return\n",
    "    \n",
    "    # Compute statistics\n",
    "    stats_dict = compute_rt_statistics(data['reaction_times'])\n",
    "    \n",
    "    if stats_dict is None:\n",
    "        return\n",
    "    \n",
    "    # Detect outliers\n",
    "    outlier_mask, outlier_indices = detect_outliers(data['reaction_times'], \n",
    "                                                     method='iqr', threshold=3)\n",
    "    \n",
    "    # Generate report\n",
    "    generate_report(data, stats_dict, (outlier_mask, outlier_indices))\n",
    "    \n",
    "    # Create visualizations\n",
    "    plot_rt_analysis(data, stats_dict)\n",
    "    \n",
    "    # Save data to CSV\n",
    "    output_dir = Path('/mnt/user-data/outputs')\n",
    "    if not output_dir.exists():\n",
    "        output_dir = Path('.')  # Use current directory\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'trial': data['trial_numbers'],\n",
    "        'reaction_time_ms': data['reaction_times'],\n",
    "        'probe_time_sec': data['probe_times'],\n",
    "        'response_time_sec': data['response_times'],\n",
    "        'probe_event_id': data['probe_event_ids'],\n",
    "        'response_event_id': data['response_event_ids'],\n",
    "        'is_outlier': outlier_mask\n",
    "    })\n",
    "    \n",
    "    csv_path = output_dir / 'reaction_times_fif.csv'\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nData saved to: {csv_path}\")\n",
    "    \n",
    "    # Optional: Save cleaned data (without outliers)\n",
    "    df_clean = df[~df['is_outlier']]\n",
    "    csv_clean_path = output_dir / 'reaction_times_fif_clean.csv'\n",
    "    df_clean.to_csv(csv_clean_path, index=False)\n",
    "    print(f\"Clean data (outliers removed) saved to: {csv_clean_path}\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a8006e4-6294-4326-bc6c-8f9bb6e6d113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FIF file: subject01_session01_seeg_raw.fif\n",
      "Loading as Raw file...\n",
      "Found 249 events\n",
      "Sampling frequency: 200.0 Hz\n",
      "Event IDs: {np.str_('Response'): np.int64(1), np.str_('Probe'): np.int64(2), np.str_('Maintenance'): np.int64(3), np.str_('Stimulus'): np.int64(4), np.str_('Fixation'): np.int64(5)}\n",
      "Found 50 probe events\n",
      "Found 50 response events\n",
      "\n",
      "Extracted 47 valid trials (100-3000ms)\n",
      "\n",
      "Plot saved to: rt_analysis_fif.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1200 with 6 Axes>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = extract_reaction_times_from_fif(\n",
    "    'subject01_session01_seeg_raw.fif',\n",
    "    probe_event_id=4,      # or [3, 4] for multiple\n",
    "    response_event_id=5    # or [5, 6] for multiple\n",
    ")\n",
    "\n",
    "stats = compute_rt_statistics(data['reaction_times'])\n",
    "plot_rt_analysis(data, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d916249-36fd-413a-8d2f-f3076bae67ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5ba521-65c9-42b4-b44d-8393f99523f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding events on: STI\n",
      "Trigger channel STI has a non-zero initial value of 1 (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "8888 events found on stim channel STI\n",
      "Event IDs: [1 2 3 4 5]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have 6 events shorter than the shortest_event. These are very unusual and you may want to set min_duration to a larger value e.g. x / raw.info['sfreq']. Where x = 1 sample shorter than the shortest event length.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m events = \u001b[43mmne\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_loaded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstim_channel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSTI\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsecutive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m event_id = {\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfixation\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m2\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m5\u001b[39m\n\u001b[32m      9\u001b[39m }\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal concatenated data:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<decorator-gen-141>:12\u001b[39m, in \u001b[36mfind_events\u001b[39m\u001b[34m(raw, stim_channel, output, consecutive, min_duration, shortest_event, mask, uint_cast, mask_type, initial_event, verbose)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\mne\\Lib\\site-packages\\mne\\event.py:792\u001b[39m, in \u001b[36mfind_events\u001b[39m\u001b[34m(raw, stim_channel, output, consecutive, min_duration, shortest_event, mask, uint_cast, mask_type, initial_event, verbose)\u001b[39m\n\u001b[32m    790\u001b[39m     n_short_events = np.sum(np.diff(events[:, \u001b[32m0\u001b[39m]) < shortest_event)\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_short_events > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    793\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_short_events\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m events shorter than the shortest_event. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    794\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThese are very unusual and you may want to set min_duration to a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    795\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlarger value e.g. x / raw.info[\u001b[39m\u001b[33m'\u001b[39m\u001b[33msfreq\u001b[39m\u001b[33m'\u001b[39m\u001b[33m]. Where x = 1 sample shorter \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    796\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthan the shortest event length.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    797\u001b[39m         )\n\u001b[32m    799\u001b[39m     events_list.append(events)\n\u001b[32m    801\u001b[39m events = np.concatenate(events_list, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: You have 6 events shorter than the shortest_event. These are very unusual and you may want to set min_duration to a larger value e.g. x / raw.info['sfreq']. Where x = 1 sample shorter than the shortest event length."
     ]
    }
   ],
   "source": [
    "events = mne.find_events(all_loaded, stim_channel='STI', consecutive=True)\n",
    "\n",
    "event_id = {\n",
    "    'fixation': 1,\n",
    "    'encoding': 2,\n",
    "    'maintenance': 3,\n",
    "    'retrieval': 4,\n",
    "    'response': 5\n",
    "}\n",
    "\n",
    "print(f\"\\nFinal concatenated data:\")\n",
    "print(f\"  Channels: {len(all_loaded.ch_names)}\")\n",
    "print(f\"  Duration: {all_loaded.times[-1]:.2f} seconds\")\n",
    "print(f\"  Events found: {len(events)}\")\n",
    "\n",
    "# Show event distribution\n",
    "unique_events, counts = np.unique(events[:, 2], return_counts=True)\n",
    "print(f\"\\nEvent distribution:\")\n",
    "for evt, count in zip(unique_events, counts):\n",
    "    event_name = [k for k, v in event_id.items() if v == evt]\n",
    "    name = event_name[0] if event_name else 'unknown'\n",
    "    print(f\"  {name} ({evt}): {count} events\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb4b9abb-6659-4c4e-8df7-f4f548c664cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "349 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 349 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "349 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 349 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "349 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 349 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "349 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 349 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "349 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 349 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "349 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 349 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "349 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 349 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "349 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 349 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "348 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 348 events and 633 original time points ...\n",
      "104 bad epochs dropped\n",
      "Not setting metadata\n",
      "348 matching events found\n",
      "Setting baseline interval to [-1.16, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 348 events and 633 original time points ...\n",
      "104 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "\n",
    "baseline = (0, 0)\n",
    "\n",
    "\n",
    "# Now create epochs with the valid event_id\n",
    "fixation_epoch = mne.Epochs(seeg_raw_filtered,\n",
    "                    events=events,\n",
    "                    event_id={'fixation': 1},  # This is now populated!\n",
    "                    tmin=0,\n",
    "                    tmax=1,\n",
    "                    baseline=baseline,\n",
    "                    preload=True)\n",
    "\n",
    "encoding_epoch = mne.Epochs(seeg_raw_filtered,\n",
    "                    events=events,\n",
    "                    event_id={'encoding': 2},  # This is now populated!\n",
    "                    tmin=0,\n",
    "                    tmax=1,\n",
    "                    baseline=baseline,\n",
    "                    preload=True)\n",
    "\n",
    "maintenance_epoch = mne.Epochs(seeg_raw_filtered,\n",
    "                    events=events,\n",
    "                    event_id={'maintenance': 3},  # This is now populated!\n",
    "                    tmin=0,\n",
    "                    tmax=1,\n",
    "                    baseline=baseline,\n",
    "                    preload=True)\n",
    "\n",
    "probe_epoch = mne.Epochs(seeg_raw_filtered,\n",
    "                    events=events,\n",
    "                    event_id={'retreival': 4},  # This is now populated!\n",
    "                    tmin=0,\n",
    "                    tmax=1,\n",
    "                    baseline=baseline,\n",
    "                    preload=True)\n",
    "###############################33333\n",
    "fixation_epoch_ica = mne.Epochs(seeg_raw_filtered_ica,\n",
    "                    events=events,\n",
    "                    event_id={'fixation': 1},  # This is now populated!\n",
    "                    tmin=0,\n",
    "                    tmax=1,\n",
    "                    baseline=None,\n",
    "                    preload=True)\n",
    "\n",
    "encoding_epoch_ica = mne.Epochs(seeg_raw_filtered_ica,\n",
    "                    events=events,\n",
    "                    event_id={'encoding': 2},  # This is now populated!\n",
    "                    tmin=0,\n",
    "                    tmax=1,\n",
    "                    baseline=None,\n",
    "                    preload=True)\n",
    "\n",
    "maintenance_epoch_ica = mne.Epochs(seeg_raw_filtered_ica,\n",
    "                    events=events,\n",
    "                    event_id={'maintenance': 3},  # This is now populated!\n",
    "                    tmin=0,\n",
    "                    tmax=1,\n",
    "                    baseline=None,\n",
    "                    preload=True)\n",
    "\n",
    "probe_epoch_ica = mne.Epochs(seeg_raw_filtered_ica,\n",
    "                    events=events,\n",
    "                    event_id={'retreival': 4},  # This is now populated!\n",
    "                    tmin=0,\n",
    "                    tmax=1,\n",
    "                    baseline=None,\n",
    "                    preload=True)\n",
    "\n",
    "\n",
    "response_epoch_ica = mne.Epochs(seeg_raw_filtered_ica_8,\n",
    "                    events=events,\n",
    "                    event_id={'response': 5},  # This is now populated!\n",
    "                    tmin=-1.160,\n",
    "                    tmax=2,\n",
    "                    baseline=None,\n",
    "                    preload=True)\n",
    "\n",
    "response_epoch = mne.Epochs(seeg_raw_filtered_8,\n",
    "                    events=events,\n",
    "                    event_id={'response': 5},  # This is now populated!\n",
    "                    tmin=-1.160,\n",
    "                    tmax=2,\n",
    "                    baseline=(None,0),\n",
    "                    preload=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c98fc2-d2b7-4953-b808-bffaddec3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs.plot()\n",
    "evoked_fixation = fixation_epoch.average()\n",
    "evoked_encoding = encoding_epoch.average()\n",
    "evoked_maintenance = maintenance_epoch.average()\n",
    "evoked_response = response_epoch.average()\n",
    "#evoked_encoding = encoding_epoch.average()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdb74ca0-1a64-431b-b3a5-055f1325d06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mne_qt_browser._pg_figure.MNEQtBrowser at 0x1e2562595b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_epoch.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70447a7d-f6d6-4768-9926-5222852bfac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.plot_compare_evokeds([evoked_fixation, evoked_encoding, evoked_maintenance,probe_evoked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23692902-5806-490f-9202-48b668e633f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 19 channels (please be patient, this may take a while)\n",
      "Selecting by explained variance: 16 components\n",
      "Fitting ICA took 7.3s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table mne-repr-table\">\n",
       "    <tr>\n",
       "        <th>Method</th>\n",
       "        <td>picard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit parameters</th>\n",
       "        <td>max_iter=500<br /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit</th>\n",
       "        <td>93 iterations on epochs (154452 samples)</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>ICA components</th>\n",
       "        <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Available PCA components</th>\n",
       "        <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Channel types</th>\n",
       "        <td>eeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ICA components marked for exclusion</th>\n",
       "        <td>&mdash;</td>\n",
       "    </tr>\n",
       "    \n",
       "</table>"
      ],
      "text/plain": [
       "<ICA | epochs decomposition, method: picard (fit in 93 iterations on 154452 samples), 16 ICA components (19 PCA components available), channel types: eeg, no sources marked for exclusion>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 0.999  # Should normally be higher, like 0.999!!\n",
    "method = 'picard'\n",
    "max_iter = 500  # Should normally be higher, like 500 or even 1000!!\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "ica = mne.preprocessing.ICA(n_components=n_components,\n",
    "                            method=method,\n",
    "                            max_iter=max_iter,\n",
    "                            random_state=random_state)\n",
    "ica.fit(response_epoch_ica )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "923b7f72-6c61-465e-8058-53d5eb0ccc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "244 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MNEFigure size 975x967 with 16 Axes>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ica.plot_components(inst=response_epoch_ica)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4271c9e1-3d14-4cfd-89cf-f87eafb44682",
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_response = response_epoch.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a64dfed-b8b6-4617-93ff-33721ebbaa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2880x1708 with 2 Axes>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evoked_response.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33f1d787-1b93-4e63-a724-181c73522718",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evoked_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevoked_response\u001b[49m.plot_topomap()\n",
      "\u001b[31mNameError\u001b[39m: name 'evoked_response' is not defined"
     ]
    }
   ],
   "source": [
    "evoked_response.plot_topomap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33a537d4-85a7-44b2-8a26-420df2bab4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No projector specified for this dataset. Please consider the method self.add_proj.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2880x1708 with 6 Axes>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evoked_response.plot_joint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57a2511b-aa91-485d-9daf-867c96e8511e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=18, n_times=160000\n",
      "    Range : 0 ... 159999 =      0.000 ...   799.995 secs\n",
      "Ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne_qt_browser._pg_figure.MNEQtBrowser at 0x1134e0e0ef0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ica.plot_sources(seeg_raw_filtered_ica, show_scrollbars=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8d28052-054e-4d33-9657-36bbd2c389d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>,\n",
       " <Figure size 700x600 with 6 Axes>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shows: topography, time series, PSD, and variance over epochs\n",
    "ica.plot_properties(seeg_raw_filtered_ica, picks=range(ica.n_components_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e083441-5d89-4b8c-9a78-9ae648db088d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
