{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# MNE Data Analysis Pipeline\n",
    "\n",
    "This notebook contains tools for:\n",
    "- Concatenating multiple FIF files from multiple sessions\n",
    "- Loading and preprocessing EEG/SEEG data\n",
    "- Filtering and artifact removal (ICA)\n",
    "- Event extraction and analysis\n",
    "- Reaction time analysis\n",
    "- Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# For interactive plotting\n",
    "matplotlib.use(\"Qt5Agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions_header",
   "metadata": {},
   "source": [
    "## 2. Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concat_header",
   "metadata": {},
   "source": [
    "### 2.1 File Concatenation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concat_common",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_concatenate_fif(directory_path, preload=True, \n",
    "                              use_common_channels=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Read all FIF files from a directory and concatenate them using COMMON channels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    directory_path : str\n",
    "        Path to directory containing FIF files\n",
    "    preload : bool\n",
    "        Whether to load data into memory (default: True)\n",
    "    use_common_channels : bool\n",
    "        If True, only keep channels common to all files (default: True)\n",
    "    verbose : bool\n",
    "        Whether to print verbose output (default: False)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    raw_concat : mne.io.Raw\n",
    "        Concatenated Raw object\n",
    "    \"\"\"\n",
    "    raw_list = []\n",
    "    filenames = []\n",
    "    \n",
    "    # Load all files\n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.endswith('.fif'):\n",
    "            full_path = os.path.join(directory_path, filename)\n",
    "            raw = mne.io.read_raw_fif(full_path, preload=False, verbose=verbose)\n",
    "            raw_list.append(raw)\n",
    "            filenames.append(filename)\n",
    "    \n",
    "    if len(raw_list) == 0:\n",
    "        raise ValueError(f\"No FIF files found in {directory_path}\")\n",
    "    \n",
    "    print(f\"Found {len(raw_list)} FIF files\")\n",
    "    \n",
    "    # Find common channels if requested\n",
    "    if use_common_channels and len(raw_list) > 1:\n",
    "        common_channels = set(raw_list[0].ch_names)\n",
    "        for raw in raw_list[1:]:\n",
    "            common_channels &= set(raw.ch_names)\n",
    "        \n",
    "        common_channels = sorted(list(common_channels))\n",
    "        print(f\"Using {len(common_channels)} common channels\")\n",
    "        \n",
    "        # Pick common channels from all files\n",
    "        for i, raw in enumerate(raw_list):\n",
    "            raw_list[i] = raw.copy().pick_channels(common_channels, ordered=True)\n",
    "    \n",
    "    # Concatenate\n",
    "    print(\"Concatenating files...\")\n",
    "    raw_concat = mne.concatenate_raws(raw_list, preload=preload, verbose=verbose)\n",
    "    \n",
    "    print(f\"Concatenation complete: {raw_concat.times[-1]:.2f} seconds total\")\n",
    "    \n",
    "    return raw_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concat_all",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_concatenate_fif_all_channels(directory_path, \n",
    "                                          preload=True, \n",
    "                                          fill_value=0.0,\n",
    "                                          verbose=False):\n",
    "    \"\"\"\n",
    "    Read all FIF files from a directory and concatenate them using ALL channels.\n",
    "    Missing channels in individual files are filled with specified value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    directory_path : str\n",
    "        Path to directory containing FIF files\n",
    "    preload : bool\n",
    "        Whether to load data into memory (default: True)\n",
    "    fill_value : float\n",
    "        Value to use for missing channels (default: 0.0)\n",
    "    verbose : bool\n",
    "        Whether to print verbose output (default: False)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    raw_concat : mne.io.Raw\n",
    "        Concatenated Raw object with all channels\n",
    "    channel_info : dict\n",
    "        Information about which channels were present in which files\n",
    "    \"\"\"\n",
    "    \n",
    "    if fill_value is None:\n",
    "        fill_value = 0.0\n",
    "    \n",
    "    print(f\"Loading FIF files from: {directory_path}\")\n",
    "    print(f\"Missing channels will be filled with: {fill_value}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Load all files and collect channel information\n",
    "    raw_list = []\n",
    "    filenames = []\n",
    "    all_channels_sets = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.endswith('.fif'):\n",
    "            full_path = os.path.join(directory_path, filename)\n",
    "            raw = mne.io.read_raw_fif(full_path, preload=False, verbose=verbose)\n",
    "            raw_list.append(raw)\n",
    "            filenames.append(filename)\n",
    "            all_channels_sets.append(set(raw.ch_names))\n",
    "            \n",
    "            print(f\"Loaded: {filename}\")\n",
    "            print(f\"  Channels: {len(raw.ch_names)}\")\n",
    "    \n",
    "    if len(raw_list) == 0:\n",
    "        raise ValueError(f\"No FIF files found in {directory_path}\")\n",
    "    \n",
    "    print(f\"\\nTotal files loaded: {len(raw_list)}\")\n",
    "    \n",
    "    # Step 2: Find union of all channels\n",
    "    all_channels_union = set()\n",
    "    for ch_set in all_channels_sets:\n",
    "        all_channels_union |= ch_set\n",
    "    \n",
    "    all_channels_sorted = sorted(list(all_channels_union))\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Total unique channels across all files: {len(all_channels_sorted)}\")\n",
    "    \n",
    "    # Step 3: Analyze channel presence across files\n",
    "    channel_presence = {ch: [] for ch in all_channels_sorted}\n",
    "    for i, ch_set in enumerate(all_channels_sets):\n",
    "        for ch in all_channels_sorted:\n",
    "            channel_presence[ch].append(ch in ch_set)\n",
    "    \n",
    "    # Report channels that are missing from some files\n",
    "    missing_in_some = {ch: sum([not present for present in presences]) \n",
    "                       for ch, presences in channel_presence.items()}\n",
    "    channels_missing_somewhere = {ch: count for ch, count in missing_in_some.items() if count > 0}\n",
    "    \n",
    "    if channels_missing_somewhere:\n",
    "        print(f\"\\nChannels missing from some files:\")\n",
    "        for ch, count in sorted(channels_missing_somewhere.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {ch}: missing from {count}/{len(raw_list)} files\")\n",
    "    else:\n",
    "        print(f\"\\nAll channels present in all files!\")\n",
    "    \n",
    "    # Step 4: Process each file to add missing channels\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"Processing files and adding missing channels...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    processed_raws = []\n",
    "    \n",
    "    for i, (raw, filename) in enumerate(zip(raw_list, filenames)):\n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        \n",
    "        # Find missing channels for this file\n",
    "        missing_channels = [ch for ch in all_channels_sorted if ch not in raw.ch_names]\n",
    "        \n",
    "        if missing_channels:\n",
    "            print(f\"  Adding {len(missing_channels)} missing channels filled with {fill_value}\")\n",
    "            \n",
    "            # Load data if not already loaded\n",
    "            if not raw.preload:\n",
    "                raw.load_data()\n",
    "            \n",
    "            # Create info for the missing channels\n",
    "            info_missing = mne.create_info(\n",
    "                ch_names=missing_channels,\n",
    "                sfreq=raw.info['sfreq'],\n",
    "                ch_types='eeg'\n",
    "            )\n",
    "            \n",
    "            # Try to infer channel type from name patterns\n",
    "            for ch_name in missing_channels:\n",
    "                ch_idx = info_missing.ch_names.index(ch_name)\n",
    "                \n",
    "                if 'STI' in ch_name or 'TRIG' in ch_name or ch_name.startswith('STI'):\n",
    "                    info_missing['chs'][ch_idx]['kind'] = mne.io.constants.FIFF.FIFFV_STIM_CH\n",
    "                elif any(seeg_prefix in ch_name for seeg_prefix in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']):\n",
    "                    info_missing['chs'][ch_idx]['kind'] = mne.io.constants.FIFF.FIFFV_SEEG_CH\n",
    "            \n",
    "            # Create data for missing channels\n",
    "            n_samples = raw.n_times\n",
    "            missing_data = np.full((len(missing_channels), n_samples), fill_value, dtype=np.float64)\n",
    "            \n",
    "            # Create Raw object for missing channels\n",
    "            raw_missing = mne.io.RawArray(missing_data, info_missing)\n",
    "            \n",
    "            # Combine with original raw\n",
    "            raw = raw.add_channels([raw_missing], force_update_info=True)\n",
    "            \n",
    "            # Reorder channels to match all_channels_sorted\n",
    "            raw = raw.reorder_channels(all_channels_sorted)\n",
    "        else:\n",
    "            print(\"  No missing channels\")\n",
    "            if not raw.preload:\n",
    "                raw.load_data()\n",
    "        \n",
    "        processed_raws.append(raw)\n",
    "    \n",
    "    # Step 5: Concatenate all processed files\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"Concatenating all files...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    raw_concat = mne.concatenate_raws(processed_raws, preload=preload, verbose=verbose)\n",
    "    \n",
    "    print(f\"\\nConcatenation complete!\")\n",
    "    print(f\"  Total channels: {len(raw_concat.ch_names)}\")\n",
    "    print(f\"  Total duration: {raw_concat.times[-1]:.2f} seconds\")\n",
    "    print(f\"  Sampling frequency: {raw_concat.info['sfreq']} Hz\")\n",
    "    \n",
    "    return raw_concat, channel_presence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rt_header",
   "metadata": {},
   "source": [
    "### 2.2 Reaction Time Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rt_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reaction_times_from_fif(fif_file_path, probe_event_id=None, response_event_id=None):\n",
    "    \"\"\"\n",
    "    Extract reaction times from MNE FIF format file.\n",
    "    RT = Response time - Probe onset time\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fif_file_path : str or Path\n",
    "        Path to the FIF format file\n",
    "    probe_event_id : int or str or list, optional\n",
    "        Event ID(s) for probe/retrieval onset\n",
    "    response_event_id : int or str or list, optional\n",
    "        Event ID(s) for response/button press\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict : Dictionary containing RT data and metadata\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'reaction_times': [],\n",
    "        'trial_numbers': [],\n",
    "        'probe_times': [],\n",
    "        'response_times': [],\n",
    "        'probe_event_ids': [],\n",
    "        'response_event_ids': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Loading FIF file: {fif_file_path}\")\n",
    "    \n",
    "    # Load the raw file\n",
    "    file_path = str(fif_file_path)\n",
    "    \n",
    "    try:\n",
    "        raw = mne.io.read_raw_fif(file_path, preload=False, verbose=False)\n",
    "        events = mne.find_events(raw, verbose=False)\n",
    "        sfreq = raw.info['sfreq']\n",
    "        \n",
    "        # Try to get event_id from annotations\n",
    "        if raw.annotations:\n",
    "            event_id = {}\n",
    "            for desc in set(raw.annotations.description):\n",
    "                if desc.isdigit():\n",
    "                    event_id[desc] = int(desc)\n",
    "                else:\n",
    "                    unique_events = np.unique(events[:, 2])\n",
    "                    if len(event_id) < len(unique_events):\n",
    "                        event_id[desc] = unique_events[len(event_id)]\n",
    "        else:\n",
    "            unique_event_codes = np.unique(events[:, 2])\n",
    "            event_id = {f\"Event_{code}\": code for code in unique_event_codes}\n",
    "        \n",
    "        print(f\"Found {len(events)} events\")\n",
    "        print(f\"Sampling frequency: {sfreq} Hz\")\n",
    "        print(f\"Event IDs: {event_id}\")\n",
    "        \n",
    "        # Determine probe and response events\n",
    "        if probe_event_id is None:\n",
    "            probe_keys = [k for k in event_id.keys() if any(\n",
    "                word in str(k).lower() for word in ['probe', 'retrieval', 'test', 'cue']\n",
    "            )]\n",
    "            if probe_keys:\n",
    "                probe_event_id = [event_id[k] for k in probe_keys]\n",
    "                print(f\"Auto-detected probe events: {probe_keys} -> {probe_event_id}\")\n",
    "            else:\n",
    "                print(\"WARNING: Could not auto-detect probe events.\")\n",
    "                return data\n",
    "        elif isinstance(probe_event_id, str):\n",
    "            probe_event_id = [event_id[probe_event_id]]\n",
    "        elif isinstance(probe_event_id, int):\n",
    "            probe_event_id = [probe_event_id]\n",
    "        \n",
    "        if response_event_id is None:\n",
    "            response_keys = [k for k in event_id.keys() if any(\n",
    "                word in str(k).lower() for word in ['response', 'button', 'answer', 'resp']\n",
    "            )]\n",
    "            if response_keys:\n",
    "                response_event_id = [event_id[k] for k in response_keys]\n",
    "                print(f\"Auto-detected response events: {response_keys} -> {response_event_id}\")\n",
    "            else:\n",
    "                print(\"WARNING: Could not auto-detect response events.\")\n",
    "                return data\n",
    "        elif isinstance(response_event_id, str):\n",
    "            response_event_id = [event_id[response_event_id]]\n",
    "        elif isinstance(response_event_id, int):\n",
    "            response_event_id = [response_event_id]\n",
    "        \n",
    "        # Extract probe and response times\n",
    "        probe_mask = np.isin(events[:, 2], probe_event_id)\n",
    "        response_mask = np.isin(events[:, 2], response_event_id)\n",
    "        \n",
    "        probe_events = events[probe_mask]\n",
    "        response_events = events[response_mask]\n",
    "        \n",
    "        print(f\"Found {len(probe_events)} probe events\")\n",
    "        print(f\"Found {len(response_events)} response events\")\n",
    "        \n",
    "        # Convert sample indices to time (in seconds)\n",
    "        probe_times = probe_events[:, 0] / sfreq\n",
    "        response_times = response_events[:, 0] / sfreq\n",
    "        \n",
    "        # Match each probe to the next response\n",
    "        for i, (probe_sample, probe_time, probe_code) in enumerate(zip(\n",
    "            probe_events[:, 0], probe_times, probe_events[:, 2]\n",
    "        )):\n",
    "            # Find the next response after this probe\n",
    "            future_response_mask = response_events[:, 0] > probe_sample\n",
    "            \n",
    "            if np.any(future_response_mask):\n",
    "                next_response_idx = np.where(future_response_mask)[0][0]\n",
    "                response_time = response_times[next_response_idx]\n",
    "                response_code = response_events[next_response_idx, 2]\n",
    "                \n",
    "                rt = response_time - probe_time\n",
    "                \n",
    "                data['reaction_times'].append(rt)\n",
    "                data['trial_numbers'].append(i + 1)\n",
    "                data['probe_times'].append(probe_time)\n",
    "                data['response_times'].append(response_time)\n",
    "                data['probe_event_ids'].append(probe_code)\n",
    "                data['response_event_ids'].append(response_code)\n",
    "        \n",
    "        print(f\"\\nExtracted {len(data['reaction_times'])} reaction times\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return data\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_header",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_subject_header",
   "metadata": {},
   "source": [
    "### 3.1 Load Subject Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_subject_6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from subject 6\n",
    "subject_6, s6_channel_info = read_and_concatenate_fif_all_channels(\"Data_converted/Subject_06\")\n",
    "\n",
    "# Extract events\n",
    "events_6 = mne.find_events(\n",
    "    subject_6, \n",
    "    stim_channel='STI', \n",
    "    consecutive=True,\n",
    "    shortest_event=0,\n",
    "    min_duration=0,\n",
    "    initial_event=True\n",
    ")\n",
    "\n",
    "# Define event IDs\n",
    "event_id = {\n",
    "    'fixation': 1,\n",
    "    'encoding': 2,\n",
    "    'maintenance': 3,\n",
    "    'retrieval': 4,\n",
    "    'response': 5\n",
    "}\n",
    "\n",
    "print(f\"\\nLoaded Subject 6: {len(subject_6.ch_names)} channels, {subject_6.times[-1]:.1f}s duration\")\n",
    "print(f\"Found {len(events_6)} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_subject_8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from subject 8\n",
    "subject_8, s8_channel_info = read_and_concatenate_fif_all_channels(\"Data_converted/Subject_08\")\n",
    "\n",
    "# Extract events\n",
    "events_8 = mne.find_events(\n",
    "    subject_8, \n",
    "    stim_channel='STI', \n",
    "    consecutive=True,\n",
    "    shortest_event=0,\n",
    "    min_duration=0,\n",
    "    initial_event=True\n",
    ")\n",
    "\n",
    "print(f\"\\nLoaded Subject 8: {len(subject_8.ch_names)} channels, {subject_8.times[-1]:.1f}s duration\")\n",
    "print(f\"Found {len(events_8)} events\")\n",
    "\n",
    "# Plot raw data\n",
    "subject_8.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filter_header",
   "metadata": {},
   "source": [
    "### 3.2 Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filtering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for analysis\n",
    "# 0.1-40 Hz for general analysis\n",
    "seeg_raw_filtered_8 = subject_8.copy().filter(l_freq=0.1, h_freq=40)\n",
    "\n",
    "# 1-40 Hz for ICA (higher low-freq cutoff removes slow drifts)\n",
    "seeg_raw_filtered_ica_8 = subject_8.copy().filter(l_freq=1, h_freq=40)\n",
    "\n",
    "# Plot sensor positions\n",
    "seeg_raw_filtered_ica_8.plot_sensors(kind=\"3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ica_header",
   "metadata": {},
   "source": [
    "### 3.3 ICA for Artifact Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ica_fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ICA\n",
    "n_components = 18  # Adjust based on number of channels\n",
    "method = 'fastica'\n",
    "random_state = 42\n",
    "\n",
    "ica = mne.preprocessing.ICA(\n",
    "    n_components=n_components,\n",
    "    method=method,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Fit ICA on filtered data\n",
    "ica.fit(seeg_raw_filtered_ica_8)\n",
    "\n",
    "print(f\"\\nICA fitted with {ica.n_components_} components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ica_plot_sources",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ICA sources for inspection\n",
    "ica.plot_sources(seeg_raw_filtered_ica_8, show_scrollbars=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ica_plot_properties",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot component properties: topography, time series, PSD, and variance\n",
    "ica.plot_properties(seeg_raw_filtered_ica_8, picks=range(ica.n_components_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ica_exclude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude bad components (manual selection after inspection)\n",
    "# Example: ica.exclude = [0, 1, 5]  # Component indices to exclude\n",
    "\n",
    "# Apply ICA to remove artifacts\n",
    "# seeg_raw_cleaned = ica.apply(seeg_raw_filtered_8.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "epoch_header",
   "metadata": {},
   "source": [
    "## 4. Epoching and ERP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_epochs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create epochs\n",
    "tmin, tmax = -0.2, 0.8  # Time window around events\n",
    "\n",
    "epochs = mne.Epochs(\n",
    "    seeg_raw_filtered_8,\n",
    "    events_8,\n",
    "    event_id=event_id,\n",
    "    tmin=tmin,\n",
    "    tmax=tmax,\n",
    "    baseline=(None, 0),\n",
    "    preload=True\n",
    ")\n",
    "\n",
    "print(f\"Created {len(epochs)} epochs\")\n",
    "print(f\"Epoch duration: {tmax - tmin}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute_evoked",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evoked responses (ERPs)\n",
    "evoked_retrieval = epochs['retrieval'].average()\n",
    "evoked_response = epochs['response'].average()\n",
    "\n",
    "# Plot evoked responses\n",
    "evoked_retrieval.plot()\n",
    "evoked_response.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_topomap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot topographic maps at different time points\n",
    "times = np.arange(0.1, 0.5, 0.1)\n",
    "evoked_retrieval.plot_topomap(times=times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot joint ERP and topography\n",
    "evoked_response.plot_joint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rt_analysis_header",
   "metadata": {},
   "source": [
    "## 5. Reaction Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract_rt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract reaction times\n",
    "# Using the raw object and specifying probe and response event IDs\n",
    "rt_data = extract_reaction_times_from_fif(\n",
    "    subject_8,\n",
    "    probe_event_id=4,  # retrieval\n",
    "    response_event_id=5  # response\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "rt_df = pd.DataFrame(rt_data)\n",
    "\n",
    "if len(rt_df) > 0:\n",
    "    print(\"\\nReaction Time Statistics:\")\n",
    "    print(f\"Mean RT: {rt_df['reaction_times'].mean():.3f}s\")\n",
    "    print(f\"Median RT: {rt_df['reaction_times'].median():.3f}s\")\n",
    "    print(f\"SD RT: {rt_df['reaction_times'].std():.3f}s\")\n",
    "    print(f\"Min RT: {rt_df['reaction_times'].min():.3f}s\")\n",
    "    print(f\"Max RT: {rt_df['reaction_times'].max():.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_rt_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RT distribution\n",
    "if len(rt_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0, 0].hist(rt_df['reaction_times'], bins=30, edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Reaction Time (s)')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].set_title('RT Distribution')\n",
    "    \n",
    "    # Box plot\n",
    "    axes[0, 1].boxplot(rt_df['reaction_times'])\n",
    "    axes[0, 1].set_ylabel('Reaction Time (s)')\n",
    "    axes[0, 1].set_title('RT Box Plot')\n",
    "    \n",
    "    # Time series\n",
    "    axes[1, 0].plot(rt_df['trial_numbers'], rt_df['reaction_times'], 'o-')\n",
    "    axes[1, 0].set_xlabel('Trial Number')\n",
    "    axes[1, 0].set_ylabel('Reaction Time (s)')\n",
    "    axes[1, 0].set_title('RT Over Trials')\n",
    "    \n",
    "    # Q-Q plot\n",
    "    scipy_stats.probplot(rt_df['reaction_times'], dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title('Q-Q Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization_header",
   "metadata": {},
   "source": [
    "## 6. Additional Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_raw_detailed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raw data with events\n",
    "subject_8.plot(\n",
    "    events=events_8,\n",
    "    event_id=event_id,\n",
    "    scalings='auto',\n",
    "    duration=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_psd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot power spectral density\n",
    "seeg_raw_filtered_8.compute_psd(fmax=50).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_epochs_image",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot epochs as image\n",
    "epochs['retrieval'].plot_image(picks='eeg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
